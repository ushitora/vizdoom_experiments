{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from __future__ import print_function\n",
    "import time,random,threading\n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "CONFIG_FILE_PATH = \"./config/simpler_basic.cfg\"\n",
    "MODEL_PATH = \"./model_v00/model_v00.ckpt\"\n",
    "RESOLUTION = (40,60,1)\n",
    "\n",
    "N_ADV = 5\n",
    "\n",
    "N_WORKERS = 10\n",
    "\n",
    "WORKER_STEPS =100000\n",
    "\n",
    "UPDATE_FREQ = 10\n",
    "\n",
    "N_ACTION = 2\n",
    "MAX_AIM = 100.0\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "FREQ_UPDATE = 10\n",
    "FREQ_TEST = 50\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.0\n",
    "EPS_STEPS = WORKER_STEPS*N_WORKERS\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "RMSProbDecaly = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --スレッドになるクラスです　-------\n",
    "class Worker_thread:\n",
    "    # スレッドは学習環境environmentを持ちます\n",
    "    def __init__(self, thread_name, parameter_server):\n",
    "        self.environment = Environment(thread_name, parameter_server)\n",
    "        print(thread_name,\" Initialized\")\n",
    "\n",
    "    def run(self):            \n",
    "        while True:\n",
    "            if not self.environment.finished:\n",
    "                self.environment.run()\n",
    "            else:\n",
    "                sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,name, parameter_server):\n",
    "        self.game = DoomGame()\n",
    "        self.game.add_available_button(Button.TURN_LEFT_RIGHT_DELTA,100)\n",
    "        self.game.load_config(CONFIG_FILE_PATH)\n",
    "        self.game.set_window_visible(False)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "        self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        #game.set_screen_format(ScreenFormat.CRCGCB)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "        \n",
    "        self.network = Network_local(name, parameter_server)\n",
    "        self.agent = Agent(name,self.network)\n",
    "        \n",
    "        self.local_step = 0\n",
    "        \n",
    "        self.finished = False\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.record = False\n",
    "        self.parameter_server = parameter_server\n",
    "        \n",
    "        self.loss_fire = []\n",
    "        self.loss_aim = []\n",
    "        self.loss_value = []\n",
    "        self.frame = []\n",
    "        \n",
    "        self.record_reward = []\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.game.new_episode()\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION,mode='constant')\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def run(self):\n",
    "        global frames\n",
    "        \n",
    "        self.game.new_episode()\n",
    "        \n",
    "        train_episode = 0\n",
    "        for step in range(WORKER_STEPS):\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                buff = self.name + \":\" + str(step) + \"step is passed\"\n",
    "                print(buff)\n",
    "            #Copy params from global\n",
    "            self.agent.network.pull_parameter_server()\n",
    "\n",
    "            if not self.game.is_episode_finished():\n",
    "\n",
    "                s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "                action = self.agent.act(s1)\n",
    "                reward = self.game.make_action(action,1)\n",
    "                isterminal = self.game.is_episode_finished()\n",
    "                s2 = self.preprocess(self.game.get_state().screen_buffer) if not isterminal else None\n",
    "                \n",
    "                if self.record==True:\n",
    "                    self.parameter_server.write_summary(frames,np.array([s1]),np.array([action]),np.array([[reward]]))\n",
    "                    l_a,l_f,l_v = self.parameter_server.calc_loss(frames,np.array([s1]),np.array([action]),np.array([[reward]]))\n",
    "                    self.loss_fire.append(l_a[0][0])\n",
    "                    self.loss_aim.append(l_f[0][0])\n",
    "                    self.loss_value.append(l_v[0][0])\n",
    "                    self.frame.append(frames)\n",
    "\n",
    "                self.agent.advantage_push_network(s1,action,reward,s2,isterminal)\n",
    "\n",
    "                frames += 1\n",
    "                self.local_step += 1\n",
    "\n",
    "            else:\n",
    "                train_episode += 1\n",
    "                self.start_episode()\n",
    "                frames += 1\n",
    "                \n",
    "        print(self.name,\" finished |\",train_episode,\" episodes was trained\")\n",
    "        \n",
    "        self.finished = True\n",
    "    \n",
    "    def test_score(self):\n",
    "        \n",
    "        global frames\n",
    "        current_step = frames\n",
    "        self.agent.network.pull_parameter_server()\n",
    "\n",
    "        self.game.new_episode()\n",
    "        total_reward = 0\n",
    "        while not self.game.is_episode_finished():\n",
    "\n",
    "            s1 = self.preprocess(self.game.get_state().screen_buffer)                \n",
    "            action = self.agent.act_test(s1)\n",
    "            reward = self.game.make_action(action,1)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        buff = \"-------TEST of %s in %d step--------\\n\"%(self.name,current_step)\n",
    "        buff += \"\\t REWARD: %.2f\\n\"%(total_reward) \n",
    "        print(buff)\n",
    "        self.record_reward.append(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def state():\n",
    "        name = \"STATE\"\n",
    "        shape = [None,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]]\n",
    "        return tf.placeholder(tf.float32,shape=shape,name=name)\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 8\n",
    "        kernel_size = [6,6]\n",
    "        stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs, \\\n",
    "                                            stride=stride,padding=padding,activation_fn=activation, \\\n",
    "                                           weights_initializer=weights_init, \\\n",
    "                                            biases_initializer=bias_init)\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 16\n",
    "        kernel_size = [3,3]\n",
    "        stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs, \\\n",
    "                                            stride=stride,padding=padding,activation_fn=activation, \\\n",
    "                                           weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        return tf.contrib.layers.flatten(pre_layer)\n",
    "        \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs = 512\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def policy_mu(pre_layer):\n",
    "        num_outputs = 2\n",
    "        activation_fn = tf.nn.sigmoid\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init)*200 - 100\n",
    "    \n",
    "    def policy_gamma(pre_layer):\n",
    "        num_outputs = 2\n",
    "        activation_fn = tf.nn.softplus\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.sqrt(tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init))\n",
    "    def value(pre_layer):\n",
    "        num_outputs = 1\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                weights_initializer=weights_init, biases_initializer=bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --グローバルなTensorFlowのDeep Neural Networkのクラスです　-------\n",
    "class ParameterServer:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope(\"parameter_server\"):      # スレッド名で重み変数に名前を与え、識別します（Name Space）\n",
    "            self._build_model()            # ニューラルネットワークの形を決定\n",
    "            \n",
    "        with tf.variable_scope(\"summary\"):\n",
    "            self._summary()\n",
    "            \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)    # loss関数を最小化していくoptimizerの定義です\n",
    "        \n",
    "        print(\"-------GLOBAL-------\")\n",
    "        for w in self.weights_params:\n",
    "            print(w)\n",
    "\n",
    "    def _build_model(self):\n",
    "            self.state = NetworkSetting.state()\n",
    "            self.conv1 = NetworkSetting.conv1(self.state)\n",
    "            self.conv2 = NetworkSetting.conv2(self.conv1)\n",
    "            reshape = NetworkSetting.reshape(self.conv2)\n",
    "            fc1 = NetworkSetting.fc1(reshape)\n",
    "\n",
    "            with tf.variable_scope(\"policy\"):\n",
    "                self.mu = NetworkSetting.policy_mu(fc1)\n",
    "                self.gamma = NetworkSetting.policy_gamma(fc1)\n",
    "            \n",
    "            with tf.variable_scope(\"value\"):\n",
    "                self.value = NetworkSetting.value(fc1)\n",
    "    \n",
    "    # Recording node in tensorboard\n",
    "    def _summary(self):\n",
    "        \n",
    "        self.a_t = tf.placeholder(tf.float32, shape=(None, N_ACTION))\n",
    "        self.r_t = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "        # Normal Distributions as Policy\n",
    "        p_aim = tf.distributions.Normal(loc=self.mu[:,0],scale=self.gamma[:,0])\n",
    "        p_fire = tf.distributions.Normal(loc=self.mu[:,1],scale=self.gamma[:,1])\n",
    "\n",
    "        # Probability for Action_Aim\n",
    "        prob_aim = tf.reshape(p_aim.prob(self.a_t[:,0]),[-1,1],name=\"prob_aim\")\n",
    "\n",
    "        # Probability for Action_Fire\n",
    "        cdf_fire = p_fire.cdf(0.5)\n",
    "        prob_fire = cdf_fire * (tf.ones_like(self.a_t[:,1])-self.a_t[:,1]) + (tf.ones_like(self.a_t[:,1])-cdf_fire)*self.a_t[:,1]\n",
    "        prob_fire = tf.reshape(prob_fire,[-1,1],name=\"prob_fire\")\n",
    "\n",
    "        log_policy_aim = tf.log(prob_aim + 1e-10, name=\"log_policy_aim\")\n",
    "        log_policy_fire = tf.log(prob_fire + 1e-10, name=\"log_policy_fire\")\n",
    "\n",
    "        advantage = self.r_t - self.value\n",
    "\n",
    "        self.loss_policy_aim = -log_policy_aim * tf.stop_gradient(advantage)\n",
    "        self.loss_policy_fire = -log_policy_fire * tf.stop_gradient(advantage)\n",
    "\n",
    "        self.loss_value = tf.square(advantage)\n",
    "        \n",
    "        tf.summary.scalar('loss_aim',self.loss_policy_aim[0][0])\n",
    "        tf.summary.scalar('loss_fire', self.loss_policy_fire[0][0])\n",
    "        tf.summary.scalar('loss_value', self.loss_value[0][0])\n",
    "        \n",
    "        state_shape = self.state.get_shape()\n",
    "        conv1_shape = self.conv1.get_shape()\n",
    "        conv2_shape = self.conv2.get_shape()\n",
    "        tf.summary.image('state',tf.reshape(self.state,[-1, state_shape[1], state_shape[2], state_shape[3]]),1)\n",
    "        tf.summary.image('conv1',tf.reshape(self.conv1,[-1, conv1_shape[1], conv1_shape[2], 1]),1)\n",
    "        tf.summary.image('conv2',tf.reshape(self.conv2,[-1, conv2_shape[1], conv2_shape[2], 1]),1)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\",SESS.graph)\n",
    "        \n",
    "    def calc_loss(self,step,s1,a,r):\n",
    "        loss_aim,loss_fire,loss_v = SESS.run([self.loss_policy_aim, \\\n",
    "                                          self.loss_policy_fire, \\\n",
    "                                          self.loss_value], \\\n",
    "                                         feed_dict={self.state:s1,self.a_t:a,self.r_t:r})\n",
    "        return loss_aim,loss_fire,loss_v\n",
    "    \n",
    "    def write_summary(self,step,s1,a,r):\n",
    "        m = SESS.run(self.merged,feed_dict={self.state:s1,self.a_t:a,self.r_t:r})\n",
    "        self.writer.add_summary(m,step)\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.saver.save(SESS, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,name,network):\n",
    "        self.name = name\n",
    "        self.network = network\n",
    "        self.memory = []\n",
    "    \n",
    "    def act(self,s1):\n",
    "        \n",
    "        global frames\n",
    "        \n",
    "        if frames>=EPS_STEPS:\n",
    "            eps = EPS_END\n",
    "        else:\n",
    "            eps = EPS_START + frames*(EPS_END - EPS_START) / EPS_STEPS\n",
    "        \n",
    "        if random.random() < eps:\n",
    "            aim = random.random() * MAX_AIM\n",
    "            attack = random.randint(0,1)\n",
    "            return [aim,attack]\n",
    "        else:\n",
    "            s1 = np.array([s1])\n",
    "            action_aim, action_fire = self.network.predict_actions(s1)\n",
    "            return [action_aim[0],action_fire[0]]\n",
    "        \n",
    "    def act_test(self,s1):\n",
    "        s1 = np.array([s1])\n",
    "        action_aim, action_fire = self.network.predict_actions(s1)\n",
    "        return [action_aim[0],action_fire[0]]\n",
    "    \n",
    "    def advantage_push_network(self,s1,action,reward,s2,isterminal):\n",
    "        \n",
    "        self.memory.append((s1,action,reward,s2))\n",
    "        \n",
    "        if isterminal:\n",
    "            for i in range(len(self.memory)-1,-1,-1):\n",
    "                s1,a,r,s2 = self.memory[i]\n",
    "                if i==N_ADV-1:\n",
    "                    self.R = 0\n",
    "                else:\n",
    "                    self.R = r + GAMMA*self.R\n",
    "                \n",
    "                self.network.train_push(s1,a,self.R,s2,isterminal)\n",
    "            \n",
    "            self.memory = []\n",
    "            self.R = 0\n",
    "            self.network.update_parameter_server()\n",
    "\n",
    "        if len(self.memory)>=N_ADV:\n",
    "            \n",
    "            for i in range(N_ADV-1,-1,-1):\n",
    "                s1,a,r,s2 = self.memory[i]\n",
    "                if i==N_ADV-1:\n",
    "                    self.R = self.network.predict_value(np.array([s1]))[0][0]\n",
    "                else:\n",
    "                    self.R = r + GAMMA*self.R\n",
    "                \n",
    "                self.network.train_push(s1,a,self.R,s2,isterminal)\n",
    "            \n",
    "            self.memory = []\n",
    "            self.R = 0\n",
    "            self.network.update_parameter_server()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_local(object):\n",
    "    def __init__(self,name,parameter_server):\n",
    "        self.name = name\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._model()\n",
    "            self._build_graph(parameter_server)\n",
    "            \n",
    "        self.s1 = np.empty(shape=(100,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.s2 = np.empty(shape=(100,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.reward = np.empty(shape=(100,1),dtype=np.float32)\n",
    "        self.action = np.empty(shape=(100,2),dtype=np.float32)\n",
    "        self.isterminal = np.empty(shape=(100,1),dtype=np.int8)\n",
    "        self.queue_pointer = 0\n",
    "        \n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self):\n",
    "        \n",
    "        self.state = NetworkSetting.state()\n",
    "        conv1 = NetworkSetting.conv1(self.state)\n",
    "        conv2 = NetworkSetting.conv2(conv1)\n",
    "        reshape = NetworkSetting.reshape(conv2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "\n",
    "        with tf.variable_scope(\"policy\"):\n",
    "            self.mu = NetworkSetting.policy_mu(fc1)\n",
    "            self.gamma = NetworkSetting.policy_gamma(fc1)\n",
    "\n",
    "        with tf.variable_scope(\"value\"):\n",
    "            self.value = NetworkSetting.value(fc1)\n",
    "            \n",
    "    def _build_graph(self,parameter_server):\n",
    "#         with tf.variable_scope(self.name+\"_graph\"):\n",
    "        self.a_t = tf.placeholder(tf.float32, shape=(None, N_ACTION))\n",
    "        self.r_t = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "        # Normal Distributions as Policy\n",
    "        self.p_aim = tf.distributions.Normal(loc=self.mu[:,0],scale=self.gamma[:,0] + 0.5)\n",
    "        self.p_fire = tf.distributions.Normal(loc=self.mu[:,1],scale=self.gamma[:,1] + 0.5)\n",
    "        \n",
    "        self.sample_aim = self.p_aim.sample([1])\n",
    "        self.sample_fire = self.p_fire.sample([1])\n",
    "\n",
    "        # Probability for Action_Aim\n",
    "        prob_aim = tf.reshape(self.p_aim.prob(self.a_t[:,0]),[-1,1],name=\"prob_aim\")\n",
    "\n",
    "        # Probability for Action_Fire\n",
    "        self.cdf_fire = self.p_fire.cdf(0.5)\n",
    "        self.prob_fire = self.cdf_fire * (tf.ones_like(self.a_t[:,1])-self.a_t[:,1]) + (tf.ones_like(self.a_t[:,1])-self.cdf_fire)*self.a_t[:,1]\n",
    "        self.prob_fire = tf.reshape(self.prob_fire,[-1,1],name=\"prob_fire\")\n",
    "\n",
    "        self.prob = tf.concat([prob_aim,self.prob_fire],1)\n",
    "\n",
    "        log_policy_aim = tf.log(prob_aim + 1e-10, name=\"log_policy_aim\")\n",
    "        log_policy_fire = tf.log(self.prob_fire + 1e-10, name=\"log_policy_fire\")\n",
    "\n",
    "        advantage = self.r_t - self.value\n",
    "\n",
    "        loss_policy_aim = -log_policy_aim * tf.stop_gradient(advantage)\n",
    "        loss_policy_fire = -log_policy_fire * tf.stop_gradient(advantage)\n",
    "\n",
    "        loss_value = tf.square(advantage)\n",
    "\n",
    "        self.loss_total = tf.reduce_mean(loss_policy_aim + loss_policy_fire + loss_value)\n",
    "\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "        self.grads = tf.gradients(self.loss_total, self.weights_params)\n",
    "\n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))\n",
    "\n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params,parameter_server.weights_params)]\n",
    "\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params)]\n",
    "        \n",
    "    def pull_parameter_server(self):\n",
    "        SESS.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self):\n",
    "        SESS.run(self.push_local_weight_params)\n",
    "        \n",
    "    def show_weights(self):\n",
    "        hoge = SESS.run(self.weights_params)\n",
    "        for i in range(len(hoge)):\n",
    "            print(hoge[i])\n",
    "            \n",
    "    def update_parameter_server(self):\n",
    "        if self.queue_pointer > 0:\n",
    "            s1 = self.s1[0:self.queue_pointer]\n",
    "            s2 = self.s2[0:self.queue_pointer]\n",
    "            r = self.reward[0:self.queue_pointer]\n",
    "            a = self.action[0:self.queue_pointer]\n",
    "            feed_dict = {self.state: s1,self.a_t:a, self.r_t:r}\n",
    "            SESS.run(self.update_global_weight_params,feed_dict)\n",
    "            self.queue_pointer = 0\n",
    "    \n",
    "    def predict_value(self,s):\n",
    "        v = SESS.run(self.value,feed_dict={self.state:s})\n",
    "        return v\n",
    "    \n",
    "    def predict_actions(self,s):\n",
    "        feed_dict = {self.state:s}\n",
    "        [action_aim, action_fire] = SESS.run([self.sample_aim, self.sample_fire],feed_dict)\n",
    "        # Encode action_fire to 0 or 1 \n",
    "        action_fire[action_fire>=0] = 1\n",
    "        action_fire[action_fire<0] = 0\n",
    "        return [action_aim[0],action_fire[0]]\n",
    "    \n",
    "    def predict_probability(self,s,a):\n",
    "        feed_dict = {self.state:s, self.a_t:a}\n",
    "        prob = SESS.run(self.prob, feed_dict)\n",
    "        return prob\n",
    "    \n",
    "    def train_push(self,s,a,r,s_,isterminal):\n",
    "        self.s1[self.queue_pointer] = s\n",
    "        self.s2[self.queue_pointer] = s_\n",
    "        self.action[self.queue_pointer] = a\n",
    "        self.reward[self.queue_pointer] = r\n",
    "        self.isterminal[self.queue_pointer] = isterminal\n",
    "        self.queue_pointer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------GLOBAL-------\n",
      "<tf.Variable 'parameter_server/Conv/weights:0' shape=(6, 6, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/Conv/biases:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/Conv_1/weights:0' shape=(3, 3, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/Conv_1/biases:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/fully_connected/weights:0' shape=(1120, 512) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected/weights:0' shape=(512, 2) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected/biases:0' shape=(2,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected_1/weights:0' shape=(512, 2) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected_1/biases:0' shape=(2,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/value/fully_connected/weights:0' shape=(512, 1) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/value/fully_connected/biases:0' shape=(1,) dtype=float32_ref>\n",
      "local_thread1  Initialized\n",
      "local_thread2  Initialized\n",
      "local_thread3  Initialized\n",
      "local_thread4  Initialized\n",
      "local_thread5  Initialized\n",
      "local_thread6  Initialized\n",
      "local_thread7  Initialized\n",
      "local_thread8  Initialized\n",
      "local_thread9  Initialized\n",
      "local_thread10  Initialized\n",
      "local_thread1:0step is passed\n",
      "local_thread2:0step is passed\n",
      "local_thread3:0step is passed\n",
      "local_thread4:0step is passed\n",
      "local_thread5:0step is passed\n",
      "local_thread6:0step is passed\n",
      "local_thread7:0step is passed\n",
      "local_thread8:0step is passed\n",
      "local_thread9:0step is passed\n",
      "local_thread10:0step is passed\n",
      "-------TEST of test_env in 0 step--------\n",
      "\t REWARD: -410.00\n",
      "\n",
      "-------TEST of test_env in 1690 step--------\n",
      "\t REWARD: -410.00\n",
      "\n",
      "TEST at 2000~2999 step cant be finished\n",
      "local_thread8:500step is passed\n",
      "local_thread2:500step is passed\n",
      "local_thread4:500step is passed\n",
      "local_thread9:500step is passed\n",
      "local_thread7:500step is passed\n",
      "local_thread3:500step is passed\n",
      "local_thread5:500step is passed\n",
      "local_thread6:500step is passed\n",
      "local_thread10:500step is passed\n",
      "-------TEST of test_env in 3387 step--------\n",
      "\t REWARD: -410.00\n",
      "\n",
      "TEST at 4000~4999 step cant be finished\n",
      "local_thread1:500step is passed\n",
      "-------TEST of test_env in 5080 step--------\n",
      "\t REWARD: -410.00\n",
      "\n",
      "-------TEST of test_env in 6757 step--------\n",
      "\t REWARD: -410.00\n",
      "\n",
      "TEST at 7000~7999 step cant be finished\n",
      "local_thread4  finished | 3  episodes was trained\n",
      "local_thread8  finished | 5  episodes was trained\n",
      "local_thread2  finished | 6  episodes was trained\n",
      "local_thread6  finished | 5  episodes was trained\n",
      "local_thread10  finished | 6  episodes was trained\n",
      "local_thread9  finished | 3  episodes was trained\n",
      "local_thread3  finished | 4  episodes was trained\n",
      "local_thread7  finished | 4  episodes was trained\n",
      "local_thread5  finished | 4  episodes was trained\n",
      "-------TEST of test_env in 8412 step--------\n",
      "\t REWARD: -410.00\n",
      "\n",
      "local_thread1  finished | 3  episodes was trained\n",
      "-------TEST of test_env in 9923 step--------\n",
      "\t REWARD: -410.00\n",
      "\n",
      "*****************************\n",
      "TIME to LEARNING:44.206 [sec]\n",
      "*****************************\n",
      "Learning phase is finished\n",
      "-------TEST of test_env in 10000 step--------\n",
      "\t REWARD: 95.00\n",
      "\n",
      "-------TEST of test_env in 10000 step--------\n",
      "\t REWARD: 95.00\n",
      "\n",
      "-------TEST of test_env in 10000 step--------\n",
      "\t REWARD: -410.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- main ここからメイン関数です------------------------------\n",
    "# global変数の定義と、セッションの開始です\n",
    "frames = 0              # 全スレッドで共有して使用する総ステップ数\n",
    "SESS = tf.Session()     # TensorFlowのセッション開始\n",
    "isLearned = False\n",
    "\n",
    "# M1.スレッドを作成します\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    parameter_server = ParameterServer()    # 全スレッドで共有するパラメータを持つエンティティです\n",
    "    threads = []     # 並列して走るスレッド\n",
    "    # 学習するスレッドを用意\n",
    "    for i in range(N_WORKERS):\n",
    "        thread_name = \"local_thread\"+str(i+1)\n",
    "        threads.append(Worker_thread(thread_name=thread_name, parameter_server=parameter_server))\n",
    "        \n",
    "test_env = Environment(\"test_env\", parameter_server)\n",
    "\n",
    "# TensorFlowでマルチスレッドを実行します\n",
    "SESS.run(tf.global_variables_initializer())     # TensorFlowを使う場合、最初に変数初期化をして、実行します\n",
    "\n",
    "sleep(3.0)\n",
    "\n",
    "threads[0].environment.record = True\n",
    "\n",
    "start_time = time.time()\n",
    "for worker in threads:\n",
    "    job = lambda: worker.run()      # この辺は、マルチスレッドを走らせる作法だと思って良い\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "\n",
    "test_frame = 0\n",
    "while True:\n",
    "    \n",
    "    if frames >= test_frame and frames<test_frame+1000:\n",
    "#         print(frames)\n",
    "        test_env.test_score()\n",
    "        test_frame += 1000\n",
    "    elif frames >= test_frame+1000:\n",
    "        print(\"TEST at %d~%d step cant be finished\"%(test_frame, test_frame+1000-1))\n",
    "        test_frame += 1000\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    isLearned = True\n",
    "    for worker in threads:\n",
    "        if not worker.environment.finished:\n",
    "            isLearned = False\n",
    "    \n",
    "    if isLearned:\n",
    "        break\n",
    "\n",
    "print(\"*****************************\\nTIME to LEARNING:%.3f [sec]\\n*****************************\"%(time.time()-start_time))\n",
    "\n",
    "np.save(\"./records/reward.npy\",np.array(test_env.record_reward))\n",
    "np.save(\"./records/loss_policy_fire.npy\", threads[0].environment.loss_fire)\n",
    "np.save(\"./records/loss_policy_aim.npy\", threads[0].environment.loss_aim)\n",
    "np.save(\"./records/loss_policy_value.npy\", threads[0].environment.loss_value)\n",
    "np.save(\"./records/frame.npy\", threads[0].environment.frame)\n",
    "\n",
    "parameter_server.save_model()\n",
    "print(\"Learning phase is finished\")\n",
    "for i in range(3):\n",
    "    test_env.test_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
