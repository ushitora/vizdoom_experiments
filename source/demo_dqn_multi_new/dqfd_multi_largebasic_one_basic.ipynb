{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import skimage.color, skimage.transform\n",
    "from vizdoom import *\n",
    "import os, time, random, threading, h5py, math,pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from game_instance_basic import GameInstanceSimpleDeathmatch, GameInstanceBasic\n",
    "from global_constants import *\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "from replay_memory import ReplayMemory\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JST = timezone(timedelta(hours=+9),'JST')\n",
    "\n",
    "DATETIME = datetime.now(JST)\n",
    "LOGDIR = \"../data/demo_dqn/logs/log_\"+DATETIME.strftime(\"%Y-%m-%d-%H-%M-%S\")+\"/\"\n",
    "MODEL_PATH =  \"../data/demo_dqn/models/model_\"+DATETIME.strftime(\"%Y-%m-%d-%H-%M-%S\")+\"/model.ckpt\"\n",
    "CONFIG_FILE_PATH = \"./config/basic.cfg\"\n",
    "PLAY_LOGDIR = \"../data/demo_dqn/playlogs/playlog_\"+DATETIME.strftime(\"%Y-%m-%d-%H-%M-%S\")+\"/\"\n",
    "DEMO_PATH = [\"../demonstration/basic/demodata_basic_render_weapon_01.hdf5\",\"../demonstration/basic/demo_basic_render_weapon02.hdf5\",\"../demonstration/basic/demo_basic_render_weapon03.hdf5\"]\n",
    "# __name__ = \"learning_imitation\"\n",
    "__name__ = \"learning_async\"\n",
    "# __name__ = \"test\"\n",
    "N_ACTION = 3\n",
    "N_AGENT_ACTION = 3\n",
    "BOTS_NUM = 1\n",
    "N_WORKERS = 5\n",
    "REWARDS = {'living':-1.0, 'kill':100.0}\n",
    "# REWARDS = {'living':-0.01, 'kill':1.0}\n",
    "LSTM_SIZE = 1024\n",
    "N_ADV = 5\n",
    "N_SEQ = 5\n",
    "LAMBDA_ONE = 1.0\n",
    "LAMBDA1 = 1.0\n",
    "LAMBDA2 = 1.0\n",
    "LAMBDA3 = 0.0001\n",
    "RESOLUTION = (120,120,3)\n",
    "MERGIN_VALUE = 0.02\n",
    "INTERVAL_BATCH_LEARNING = 10\n",
    "INTERVAL_UPDATE_NETWORK = 10\n",
    "INTERVAL_PULL_PARAMS = 2\n",
    "N_BATCH = 64\n",
    "INTERVAL_UPDATE_ORIGIN = 10\n",
    "USED_GPU = \"0\"\n",
    "BETA_MIN = 0.0\n",
    "BETA_MAX = 0.4\n",
    "EPS_MAX = 0.9\n",
    "EPS_MIN = 0.85\n",
    "N_STEPS = int(10000 / N_WORKERS)\n",
    "IMIT_MODEL_PATH = \"../data/demo_dqn/models/model_2019-02-03-17-32-02/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LOGDIR):\n",
    "    os.mkdir(LOGDIR)\n",
    "if not os.path.exists(os.path.dirname(MODEL_PATH)):\n",
    "    os.mkdir(os.path.dirname(MODEL_PATH))\n",
    "if not os.path.exists(PLAY_LOGDIR):\n",
    "    os.mkdir(PLAY_LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,sess,  name, game_instance, network, agent, start_time=None, end_time=None, n_step=None,  random_seed=0):\n",
    "#     def __init__(self,sess,  name, start_time, end_time, parameter_server):\n",
    "        self.name = name\n",
    "        self.sess = sess\n",
    "        self.game = game_instance\n",
    "        self.game.game.set_seed(random_seed)\n",
    "        self.game.game.set_render_weapon(True)\n",
    "        self.game.game.set_render_crosshair(True)\n",
    "        self.game.game.set_episode_timeout(500)\n",
    "        self.game.game.init()\n",
    "        self.network = network\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.clear_obs()\n",
    "        self.clear_batch()\n",
    "        \n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.n_step = n_step\n",
    "        self.progress = 0.0\n",
    "        self.log_server = None\n",
    "        \n",
    "        self.replay_memory = None\n",
    "        \n",
    "        self.step = 0\n",
    "        self.model_gen_count = 0\n",
    "        \n",
    "        self.times_act = None\n",
    "        self.times_update = None\n",
    "        \n",
    "        self.count_update = 0\n",
    "        self.rewards_detail = None\n",
    "        \n",
    "        self.count_idx = np.zeros_like(replaymemory.tree.tree, dtype=np.int32)\n",
    "        print(self.name,\" initialized...\")\n",
    "        \n",
    "    def run_learning(self, coordinator):\n",
    "        print(self.name + \" start learning\")\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        self.game.new_episode()\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                self.learning_step()\n",
    "                if self.n_step is not None:\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "#                 if self.progress >= 1.0:\n",
    "#                     break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(self.name,\" ended\")\n",
    "            \n",
    "#         if self.log_server is not None:\n",
    "#             coordinator.request_stop()\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def run_prelearning(self, coordinator):\n",
    "        assert self.replay_memory is not None\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                loss_values = self.prelearning_step()\n",
    "                if self.n_step is not None:\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "        except Exception as e:\n",
    "            coordinator.request_stop(e)\n",
    "            \n",
    "        coordinator.request_stop()\n",
    "        return 0\n",
    "    \n",
    "    def run_exploring(self, coordinator):\n",
    "        print(self.name + \" start exploring\")\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        self.game.new_episode()\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                self.exploring_step()\n",
    "                if self.n_step is not None:\n",
    "                    if self.step % 1000 == 0:\n",
    "                        print(self.name,\":\", self.step)\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            coordinator.request_stop(e)\n",
    "            \n",
    "        if self.log_server is not None:\n",
    "            coordinator.request_stop()\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def run_test(self, coordinator):\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "#             while True:\n",
    "                play_log = []\n",
    "                reward,frag, death,kill,total_detail,step = self.test_agent(reward_buff =play_log)\n",
    "                with open(os.path.join(PLAY_LOGDIR, \"playlog_step%02d.txt\"%int(self.progress*100)), 'wb') as f:\n",
    "                    pickle.dump(play_log, f)\n",
    "                if self.rewards_detail is not None:\n",
    "                    self.rewards_detail.append(total_detail)\n",
    "#                 print(\"----------TEST at %.1f ---------\"%(self.progress*100))\n",
    "#                 print(\"FRAG:\",frag,\"KILL:\",kill, \"DEATH:\",death,\"STEP:\",step)\n",
    "#                 print(\"REWARD:\",reward)\n",
    "#                 print(\"REWARD_DETAIL\", total_detail)\n",
    "\n",
    "                if self.log_server is not None:\n",
    "                    self.log_server.write_score(self.sess,self.step,  reward, frag, death ,kill, step)\n",
    "                    if self.progress >= self.model_gen_count/12:\n",
    "                        self.model_gen_count += 1\n",
    "                        self.log_server.save_model(sess=self.sess, model_path=MODEL_PATH, step=self.model_gen_count+1)\n",
    "                        \n",
    "                    \n",
    "                self.step += 1\n",
    "                if self.n_step is not None:\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(self.name, \"killed \")\n",
    "#             coordinator.request_stop(e)\n",
    "\n",
    "    def learning_step(self):\n",
    "        if self.step % INTERVAL_PULL_PARAMS == 0:\n",
    "            self.network.pull_parameter_server(self.sess)\n",
    "#         self.network.push_parameter_server(self.sess)\n",
    "        loss_values = []\n",
    "        if not self.game.is_episode_finished() and self.game.get_screen_buff() is not None:\n",
    "            \n",
    "            if self.times_act is not None:\n",
    "                start_time = datetime.now().timestamp()\n",
    "\n",
    "            s1_ = self.preprocess(self.game.get_screen_buff())\n",
    "            self.push_obs(s1_)\n",
    "            agent_action_idx = self.agent.act_eps_greedy(self.sess, self.obs['s1'], self.progress)\n",
    "            engin_action = self.convert_action_agent2engine_simple(agent_action_idx)\n",
    "#             engin_action = self.convert_action_agent2engine(agent_action_idx)\n",
    "            r,r_detail = self.game.make_action(self.step,engin_action , FRAME_REPEAT)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            if isterminal:\n",
    "                s2_ = np.zeros(RESOLUTION)\n",
    "            else:\n",
    "                s2_ = self.preprocess(self.game.get_screen_buff())\n",
    "            \n",
    "            self.push_batch( self.obs['s1'], agent_action_idx, s2_, r , isterminal, False)\n",
    "            \n",
    "            if self.times_act is not None:\n",
    "                self.times_act.append(datetime.now().timestamp() - start_time)\n",
    "            \n",
    "            if len(self.memory) >= N_ADV or isterminal:\n",
    "                batch = self.make_advantage_data()\n",
    "                self.clear_batch()\n",
    "                for i,b in enumerate(batch):\n",
    "                    if len(b) == 8:\n",
    "                        self.replay_memory.store(b)\n",
    "            \n",
    "            self.step += 1\n",
    "            \n",
    "            if self.step % INTERVAL_UPDATE_NETWORK == 0:\n",
    "                self.network.copy_network_learning2target(self.sess)\n",
    "                \n",
    "            if self.times_update is not None:\n",
    "                start_time = datetime.now().timestamp()\n",
    "            \n",
    "            if self.step % INTERVAL_BATCH_LEARNING == 0 and len(self.replay_memory) >= N_BATCH:\n",
    "                s1, actions, r_one, r_adv, isdemo, is_weight, tree_idx = self.make_batch()\n",
    "                if self.log_server is not None:\n",
    "                    self.count_idx[tree_idx] += 1\n",
    "                loss_values = self.network.update_parameter_server(self.sess, s1, actions, r_one, r_adv, isdemo, is_weight)\n",
    "                self.count_update += 1\n",
    "                tderror = loss_values[4]\n",
    "                l_one, l_n, l_m, l_l = loss_values[:-1]\n",
    "#                 self.replay_memory.batch_update(tree_idx, tderror)\n",
    "#                 self.replay_memory.batch_update_new(tree_idx, tderror,np.array(r_adv)>0)\n",
    "                if self.log_server is not None:\n",
    "                    self.log_server.write_loss(self.sess,self.step ,np.mean(l_one), np.mean(l_n), np.mean(l_m), l_l)\n",
    "#                     self.log_server.write_img(self.sess, self.step, s1[0:1])\n",
    "                    self.log_server.write_weights(self.sess, self.step)\n",
    "                self.replay_memory.batch_update_new(tree_idx, np.copy(l_one),np.array(r_adv)>0)\n",
    "#                 self.replay_memory.batch_update_new(tree_idx, np.copy(l_one),np.array(np.max(r_adv, axis=1))>0)\n",
    "                    \n",
    "            if self.times_update is not None:\n",
    "                self.times_update.append(datetime.now().timestamp() - start_time)\n",
    "        else:\n",
    "            self.game.new_episode()\n",
    "            self.clear_batch()\n",
    "            self.clear_obs()\n",
    "\n",
    "        return loss_values\n",
    "    \n",
    "    def prelearning_step(self):\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "#         self.network.push_parameter_server(self.sess)\n",
    "\n",
    "        s1, actions, r_one, r_adv, isdemo, is_weight, tree_idx = self.make_batch()\n",
    "        loss_values = self.network.update_parameter_server(self.sess, s1, actions, r_one, r_adv, isdemo, is_weight)\n",
    "        tderror = loss_values[4]\n",
    "        l_one, l_n, l_m, l_l = loss_values[:-1]\n",
    "        self.replay_memory.batch_update(tree_idx, tderror) \n",
    "        \n",
    "        if self.step % INTERVAL_UPDATE_NETWORK == 0:\n",
    "            self.network.copy_network_learning2target(self.sess)\n",
    "        \n",
    "        if self.log_server is not None:\n",
    "            if self.step % 10 == 0:\n",
    "                self.log_server.write_loss(self.sess, self.step, np.mean(l_one), np.mean(l_n), np.mean(l_m), l_l)\n",
    "#                 self.log_server.write_img(self.sess, self.step, s1[0:1])\n",
    "                self.log_server.write_weights(self.sess, self.step)\n",
    "        self.step += 1\n",
    "        return loss_values\n",
    "\n",
    "    def test_agent(self, gif_buff=None, reward_buff=None, sample_imgs=None):\n",
    "        \n",
    "        self.game.new_episode()\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "\n",
    "        step = 0\n",
    "        gif_img = []\n",
    "        total_reward = 0\n",
    "        total_detail = {}\n",
    "        self.clear_obs()\n",
    "        while not self.game.is_episode_finished():\n",
    "            s1_row = self.game.get_screen_buff()\n",
    "            s1 = self.preprocess(s1_row)\n",
    "            if sample_imgs is not None:\n",
    "                sample_imgs.append(s1)\n",
    "            if gif_buff is not None:\n",
    "                gif_img.append(s1_row.transpose(1,2,0))\n",
    "            self.push_obs(s1)\n",
    "            action = self.agent.act_greedy(self.sess,self.obs['s1'])\n",
    "            engine_action = self.convert_action_agent2engine_simple(action)\n",
    "#             engine_action = self.convert_action_agent2engine(action)\n",
    "            reward,reward_detail = self.game.make_action(step,engine_action,FRAME_REPEAT)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            total_reward += reward\n",
    "            for k in reward_detail.keys():\n",
    "                if not k in total_detail.keys():\n",
    "                    total_detail[k] = reward_detail[k]\n",
    "                else:\n",
    "                    total_detail[k] += reward_detail[k]\n",
    "            step += 1\n",
    "            if reward_buff is not None:\n",
    "                reward_buff.append((engine_action, reward_detail))\n",
    "        \n",
    "        save_img = []\n",
    "        if gif_buff is not None:\n",
    "            for i in range(len(gif_img)):\n",
    "                save_img.append(Image.fromarray(np.uint8(gif_img[i])))\n",
    "            gif_buff += save_img\n",
    "            \n",
    "        return total_reward, self.game.get_frag_count(), self.game.get_death_count(), self.game.get_kill_count(), total_detail, step\n",
    "        \n",
    "    def convert_action_engine2agent(self,engine_action):\n",
    "#         return engine_action.index(1)\n",
    "        assert type(engine_action) == type(list()), print(\"type: \", type(engine_action))\n",
    "        ans = 0\n",
    "        for i, e_a in enumerate(engine_action):\n",
    "            ans += e_a * 2**i\n",
    "        return ans\n",
    "    \n",
    "    def convert_action_agent2engine(self,agent_action):\n",
    "        assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print(\"type(agent_action)=\",type(agent_action))\n",
    "        ans = []\n",
    "        for i in range(N_ACTION):\n",
    "            ans.append(agent_action%2)\n",
    "            agent_action = int(agent_action / 2)\n",
    "        return ans\n",
    "    \n",
    "    def convert_action_agent2engine_simple(self, agent_action):\n",
    "        assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print(\"type(agent_action)=\",type(agent_action))\n",
    "        ans = np.zeros((N_AGENT_ACTION,))\n",
    "        ans[agent_action] = 1\n",
    "        return ans.tolist()\n",
    "    \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3 and img.shape[0]==3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION, mode=\"constant\")\n",
    "        img = img.astype(np.float32)\n",
    "#         img = (img)/255.0\n",
    "        return img\n",
    "\n",
    "    def push_obs(self, s1):\n",
    "        self.obs['s1'] = s1\n",
    "        \n",
    "    def clear_obs(self):\n",
    "        self.obs = {}\n",
    "        self.obs['s1'] = np.zeros(RESOLUTION, dtype=np.float32)\n",
    "        \n",
    "    def push_batch(self, s1, action,s2,  reward, isterminal, isdemo):\n",
    "        self.memory.append([np.copy(s1), action, np.copy(s2) , reward, isterminal, isdemo])\n",
    "    \n",
    "    def clear_batch(self):\n",
    "        self.memory = []\n",
    "    \n",
    "    def make_advantage_data(self):\n",
    "        len_memory = len(self.memory)\n",
    "        ret_batch = []\n",
    "        R_adv = 0\n",
    "        _,_,s2_adv,_,_,_ = self.memory[-1]\n",
    "        for i in range(len_memory-1, -1, -1):\n",
    "            s1,a,s2,r,isterminal,isdemo = self.memory[i]\n",
    "            R_adv = r + GAMMA*R_adv\n",
    "            ret_batch.append(np.array([s1, a,s2,s2_adv,r ,R_adv ,isterminal, isdemo]))\n",
    "        \n",
    "        self.memory = []\n",
    "        return ret_batch\n",
    "    \n",
    "    def make_batch(self):\n",
    "        while True:\n",
    "            tree_idx, batch_row, is_weight = self.replay_memory.sample(N_BATCH, self.calc_beta(self.progress))\n",
    "#             tree_idx, batch_row, is_weight = self.replay_memory.sample(N_BATCH, 0.1)\n",
    "            s2_input = [ batch_row[i,2] for i in range(N_BATCH)]\n",
    "            s2_adv = [ batch_row[i,3] for i in range(N_BATCH)]\n",
    "            if (np.shape(s2_input) == ((N_BATCH,)+RESOLUTION) and np.shape(s2_adv) == ((N_BATCH,)+RESOLUTION)):\n",
    "                break\n",
    "        \n",
    "        s1, actions, s2, r_one, r_adv, isdemo = [],[],[],[],[],[]\n",
    "        \n",
    "        predicted_q_adv  = self.network.get_qvalue_max_learningaction(self.sess,s2_adv)\n",
    "        predicted_q = self.network.get_qvalue_max_learningaction(self.sess,s2_input)\n",
    "        \n",
    "#         predicted_q_adv  = self.network.get_qvalue_target(self.sess,s2_adv)\n",
    "#         predicted_q = self.network.get_qvalue_target(self.sess,s2_input)\n",
    "        \n",
    "        for i in range(N_BATCH):\n",
    "            s1.append(batch_row[i][0])\n",
    "            actions.append(batch_row[i][1])\n",
    "            R_one = batch_row[i][4] + GAMMA * predicted_q[i] if batch_row[i][6] == False else batch_row[i][4]\n",
    "            R_adv = batch_row[i][5] + GAMMA**N_ADV * predicted_q_adv[i] if batch_row[i][6] == False else batch_row[i][5]\n",
    "#             R_one = batch_row[i][4] + GAMMA * predicted_q[i] if batch_row[i][6] == False else batch_row[i][4] + np.zeros_like(predicted_q[i])\n",
    "#             R_adv = batch_row[i][5] + GAMMA**N_ADV * predicted_q_adv[i] if batch_row[i][6] == False else batch_row[i][5] + np.zeros_like(predicted_q_adv[i])\n",
    "            r_one.append(R_one)\n",
    "            r_adv.append(R_adv)\n",
    "            isdemo.append(batch_row[i][7])\n",
    "\n",
    "        actions = np.array(actions)\n",
    "        return s1, actions.astype(np.int32), r_one, r_adv, isdemo, is_weight, tree_idx\n",
    "    \n",
    "    def make_batch_uniform(self):\n",
    "        while True:\n",
    "            tree_idx, batch_row, is_weight = self.replay_memory.sample_uniform(N_BATCH)\n",
    "            \n",
    "            s2_input = [ batch_row[i,2] for i in range(N_BATCH)]\n",
    "            s2_adv = [ batch_row[i,3] for i in range(N_BATCH)]\n",
    "            if (np.shape(s2_input) == (N_BATCH,5, 120,120,3) and np.shape(s2_adv) == (N_BATCH,5, 120,120,3)):\n",
    "                break\n",
    "        \n",
    "        s1, actions, s2, r_one, r_adv, isdemo = [],[],[],[],[],[]\n",
    "        \n",
    "        predicted_q_adv  = self.network.get_qvalue_max_learningaction(self.sess,s2_adv)\n",
    "        \n",
    "        predicted_q = self.network.get_qvalue_max_learningaction(self.sess,s2_input)\n",
    "        \n",
    "        for i in range(N_BATCH):\n",
    "            s1.append(batch_row[i][0])\n",
    "            actions.append(batch_row[i][1])\n",
    "            R_one = batch_row[i][4] + GAMMA * predicted_q[i] if batch_row[i][6] == False else batch_row[i][4]\n",
    "            R_adv = batch_row[i][5] + GAMMA**N_ADV * predicted_q_adv[i] if batch_row[i][6] == False else batch_row[i][5]\n",
    "            r_one.append(R_one)\n",
    "            r_adv.append(R_adv)\n",
    "            isdemo.append(batch_row[i][7])\n",
    "\n",
    "        actions = np.array(actions)\n",
    "        return s1, actions.astype(np.int32), r_one, r_adv, isdemo, is_weight, tree_idx\n",
    "    \n",
    "    def calc_beta(self, progress):\n",
    "#         return BETA_MIN\n",
    "        return (BETA_MAX - BETA_MIN) * progress + BETA_MIN\n",
    "    \n",
    "    def exploring_step(self):\n",
    "        if self.step % INTERVAL_PULL_PARAMS == 0:\n",
    "            self.network.pull_parameter_server(self.sess)\n",
    "        loss_values = []\n",
    "        if not self.game.is_episode_finished() and self.game.get_screen_buff() is not None:\n",
    "\n",
    "            s1_ = self.preprocess(self.game.get_screen_buff())\n",
    "            self.push_obs(s1_)\n",
    "            agent_action_idx = self.agent.act_eps_greedy(self.sess, self.obs['s1'], self.progress)\n",
    "            engin_action = self.convert_action_agent2engine_simple(agent_action_idx)\n",
    "#             engin_action = self.convert_action_agent2engine(agent_action_idx)\n",
    "            r,r_detail = self.game.make_action(self.step,engin_action , FRAME_REPEAT)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            if isterminal:\n",
    "                s2_ = np.zeros(RESOLUTION)\n",
    "            else:\n",
    "                s2_ = self.preprocess(self.game.get_screen_buff())\n",
    "            \n",
    "            self.push_batch( self.obs['s1'], agent_action_idx, s2_, r , isterminal, False)\n",
    "            \n",
    "            if len(self.memory) >= N_ADV or isterminal:\n",
    "                batch = self.make_advantage_data()\n",
    "                self.clear_batch()\n",
    "                for i,b in enumerate(batch):\n",
    "                    if len(b) == 8:\n",
    "                        self.replay_memory.store(b)\n",
    "            \n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.game.new_episode()\n",
    "            self.clear_batch()\n",
    "            self.clear_obs()\n",
    "\n",
    "        return loss_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterServer:\n",
    "    def __init__(self, sess, log_dir):\n",
    "        self.sess = sess\n",
    "        with tf.variable_scope(\"parameter_server\", reuse=tf.AUTO_REUSE):\n",
    "            self.state1_ = tf.placeholder(tf.float32, shape=(None,) + RESOLUTION)\n",
    "            self.q_value, self.conv1, self.conv2, self.q_prob = self._build_model(self.state1_)\n",
    "\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "#         self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "            \n",
    "        with tf.variable_scope(\"summary\", reuse=tf.AUTO_REUSE):\n",
    "            self._build_summary(sess,log_dir)\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep = 20)\n",
    "        \n",
    "#         print(\"-------GLOBAL-------\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "\n",
    "    def _build_model(self,state):\n",
    "            conv1 = NetworkSetting.conv1(state)\n",
    "#             maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "            conv2 = NetworkSetting.conv2(conv1)\n",
    "#             maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "            reshape = NetworkSetting.reshape(conv2)\n",
    "            fc1 = NetworkSetting.fc1(reshape)\n",
    "            q = NetworkSetting.q_value(fc1)\n",
    "            \n",
    "            q_prob = tf.nn.softmax(q)\n",
    "                \n",
    "            print(\"---------MODEL SHAPE-------------\")\n",
    "            print(state.get_shape())\n",
    "            print(conv1.get_shape())\n",
    "#             print(maxpool1.get_shape())\n",
    "            print(conv2.get_shape())\n",
    "#             print(maxpool2.get_shape())\n",
    "            print(reshape.get_shape())\n",
    "            print(fc1.get_shape())\n",
    "            print(q.get_shape())\n",
    "            \n",
    "            return q, conv1, conv2, q_prob\n",
    "                \n",
    "    def _build_summary(self,sess, log_dir):\n",
    "        \n",
    "        self.reward_ = tf.placeholder(tf.float32,shape=(), name=\"reward\")\n",
    "        self.frag_ = tf.placeholder(tf.float32, shape=(), name=\"frag\")\n",
    "        self.death_ = tf.placeholder(tf.float32, shape=(), name=\"death\")\n",
    "        self.kill_ = tf.placeholder(tf.float32, shape=(), name=\"kill\")\n",
    "        self.score_step_ = tf.placeholder(tf.float32, shape=(), name=\"step\")\n",
    "        self.loss_one_ = tf.placeholder(tf.float32, shape=(), name=\"loss_one\")\n",
    "        self.loss_adv_ = tf.placeholder(tf.float32, shape=(), name=\"loss_adv\")\n",
    "        self.loss_cls_ = tf.placeholder(tf.float32, shape=(), name=\"loss_class\")\n",
    "        self.loss_l2_ = tf.placeholder(tf.float32, shape=(), name=\"loss_l2\")\n",
    "        \n",
    "        with tf.variable_scope(\"Summary_Score\"):\n",
    "            s = [tf.summary.scalar('reward', self.reward_, family=\"score\"), tf.summary.scalar('frag', self.frag_, family=\"score\"), \\\n",
    "                 tf.summary.scalar(\"death\", self.death_, family=\"score\"), tf.summary.scalar(\"kill\", self.kill_, family=\"score\"), \\\n",
    "                 tf.summary.scalar(\"step\",self.score_step_, family=\"score\")]\n",
    "            self.summary_reward = tf.summary.merge(s)\n",
    "        \n",
    "        with tf.variable_scope(\"Summary_Loss\"):\n",
    "            list_summary = [tf.summary.scalar('loss_onestep', self.loss_one_, family=\"loss\"), tf.summary.scalar('loss_advantage', self.loss_adv_, family=\"loss\"), tf.summary.scalar('loss_class', self.loss_cls_, family=\"loss\"), tf.summary.scalar('loss_l2', self.loss_l2_, family='loss')]\n",
    "            self.summary_loss = tf.summary.merge(list_summary)\n",
    "        \n",
    "#         with tf.variable_scope(\"Summary_Images\"):\n",
    "#             conv1_display = tf.reshape(tf.transpose(self.conv1, [0,1,4,2,3]), (-1, self.conv1.get_shape()[1],self.conv1.get_shape()[2]))\n",
    "#             conv2_display = tf.reshape(tf.transpose(self.conv2, [0,1,4,2,3]), (-1, self.conv2.get_shape()[1],self.conv2.get_shape()[2]))\n",
    "#             conv1_display = tf.expand_dims(conv1_display, -1)\n",
    "#             conv2_display = tf.expand_dims(conv2_display, -1)\n",
    "\n",
    "#             state_shape = self.state1_.get_shape()\n",
    "#             conv1_shape = conv1_display.get_shape()\n",
    "#             conv2_shape = conv2_display.get_shape()\n",
    "\n",
    "#             s_img = []\n",
    "#             s_img.append(tf.summary.image('state',tf.reshape(self.state1_,[-1, state_shape[2], state_shape[3], state_shape[4]]), 1, family=\"state1\"))\n",
    "#             s_img.append(tf.summary.image('conv1',tf.reshape(self.conv1,[-1, conv1_shape[1], conv1_shape[2], 1]), family=\"conv1\"))\n",
    "#             s_img.append(tf.summary.image('conv2',tf.reshape(self.conv2,[-1, conv2_shape[1], conv2_shape[2], 1]), family=\"conv2\"))\n",
    "\n",
    "#             self.summary_image = tf.summary.merge(s_img)\n",
    "            \n",
    "        with tf.variable_scope(\"Summary_Weights\"):\n",
    "            s = [tf.summary.histogram(values=w, name=w.name, family=\"weights\") for w in self.weights_params]\n",
    "            self.summary_weights = tf.summary.merge(s)\n",
    "\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "        \n",
    "    def write_graph(self, sess):\n",
    "        self.writer.add_graph(sess.graph)\n",
    "        \n",
    "    def write_score(self,sess, step ,reward, frag, death, kill, score_step):\n",
    "        m = sess.run(self.summary_reward, feed_dict={self.reward_:reward, self.frag_:frag, self.death_:death, self.kill_:kill, self.score_step_:score_step})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_loss(self,sess, step, l_o, l_n,l_c, l_l):\n",
    "        m = sess.run(self.summary_loss, feed_dict={self.loss_one_: l_o, self.loss_adv_:l_n, self.loss_cls_:l_c, self.loss_l2_:l_l})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "#     def write_img(self,sess, step, state):\n",
    "#         m = sess.run(self.summary_image, feed_dict={self.state1_: state})\n",
    "#         return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_weights(self, sess, step):\n",
    "        m = sess.run(self.summary_weights)\n",
    "        return self.writer.add_summary(m, step)\n",
    "        \n",
    "    def load_model(self, sess, model_path, step):\n",
    "        self.saver.restore(sess, model_path+'-'+str(step))\n",
    "    \n",
    "    def save_model(self, sess,  model_path, step):\n",
    "        self.saver.save(sess, model_path, global_step = step)\n",
    "        \n",
    "    def load_cnnweights(self, sess, weights_path):\n",
    "        assert len(weights_path) == 4\n",
    "        cnn_weights = self.weights_params[:4]\n",
    "        w_demo = [np.load(w_p) for w_p in weights_path]\n",
    "        plh = [tf.placeholder(tf.float32, shape=w.shape) for w in w_demo]\n",
    "        assign_op = [w.assign(p) for w, p in zip(cnn_weights, plh)]\n",
    "        feed_dict = {p:w for w,p in zip(w_demo, plh)}\n",
    "        sess.run(assign_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, network,random_seed):\n",
    "        self.network = network\n",
    "        self.randomstate = np.random.RandomState(random_seed)\n",
    "        \n",
    "    def calc_eps(self, progress):\n",
    "        if progress < 0.2:\n",
    "            return EPS_MIN\n",
    "        elif progress >= 0.2 and progress < 0.8:\n",
    "            return ((EPS_MAX - EPS_MIN)/ 0.6) * progress + ( EPS_MIN -  (EPS_MAX - EPS_MIN)/ 0.6 * 0.2)\n",
    "        else :\n",
    "            return EPS_MAX\n",
    "\n",
    "    def act_eps_greedy(self, sess, s1, progress):\n",
    "        assert progress >= 0.0 and progress <=1.0\n",
    "        \n",
    "        eps = self.calc_eps(progress)\n",
    "        if self.randomstate.rand() <= eps:\n",
    "#             a_idx = self.randomstate.choice(range(N_AGENT_ACTION), p=self.network.get_policy(sess,[s1])[0])\n",
    "            a_idx = self.network.get_best_action(sess, [s1])[0]\n",
    "        else:\n",
    "            a_idx = self.randomstate.randint(N_AGENT_ACTION)\n",
    "            \n",
    "        return a_idx\n",
    "    \n",
    "    def act_greedy(self, sess, s1):\n",
    "#         a_idx = self.randomstate.choice(range(N_AGENT_ACTION), p=self.network.get_policy(sess,[s1])[0])\n",
    "        a_idx = self.network.get_best_action(sess, [s1])[0]\n",
    "        return a_idx\n",
    "    \n",
    "    def get_sum_prob(self,sess, s1):\n",
    "        q_value = self.network.get_qvalue_learning(sess, [s1])[0]\n",
    "        q_value = np.maximum(q_value,0) + 0.01\n",
    "        q_prob = (q_value)/sum(q_value)\n",
    "        a_idx = np.random.choice(N_AGENT_ACTION, p=q_prob)\n",
    "        return a_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name, parameter_server):\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope(\"learning_network\"):\n",
    "                self.state1_ = tf.placeholder(tf.float32,shape=(None,)+RESOLUTION, name=\"state_1\")\n",
    "                self.q_value, self.conv1, self.conv2,self.reshape,self.fc1 = self._build_model(self.state1_)\n",
    "            with tf.variable_scope(\"target_network\"):\n",
    "                self.state1_target_ = tf.placeholder(tf.float32,shape=(None,)+RESOLUTION, name=\"state_1\")\n",
    "                self.q_value_target,_,_,_,_ = self._build_model(self.state1_target_)\n",
    "            \n",
    "            self.a_ = tf.placeholder(tf.int32, shape=(None,), name=\"action\")\n",
    "            self.target_one_ = tf.placeholder(tf.float32, shape=(None,), name=\"target_one_\")\n",
    "            self.target_n_ = tf.placeholder(tf.float32, shape=(None,), name=\"target_n_\")\n",
    "#             self.target_one_ = tf.placeholder(tf.float32, shape=(None,N_AGENT_ACTION), name=\"target_one_\")\n",
    "#             self.target_n_ = tf.placeholder(tf.float32, shape=(None,N_AGENT_ACTION), name=\"target_n_\")\n",
    "            self.isdemo_ = tf.placeholder(tf.float32,shape=(None,), name=\"isdemo_\")\n",
    "            self.mergin_ = tf.placeholder(tf.float32,shape=(None,N_AGENT_ACTION), name=\"mergin_\")\n",
    "            self.is_weight_ = tf.placeholder(tf.float32, shape=(None,), name=\"is_weight\")\n",
    "                \n",
    "            self._build_graph()\n",
    "            \n",
    "#             self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)\n",
    "#             self.optimizer = tf.train.AdamOptimizer()\n",
    "#             self.update = self.optimizer.minimize(self.loss_total, var_list = self.weights_params_learning)\n",
    "        \n",
    "#         self.grads = parameter_server.optimizer.compute_gradients(self.loss_total, var_list=self.weights_params_learning)\n",
    "        \n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients([(g,w) for g, w in zip(self.grads, parameter_server.weights_params)])\n",
    "        \n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params_learning,parameter_server.weights_params)]\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params_learning)]\n",
    "\n",
    "    def _build_model(self,state):\n",
    "        conv1 = NetworkSetting.conv1(state)\n",
    "#         maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        conv2 = NetworkSetting.conv2(conv1)\n",
    "#         maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(conv2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "        \n",
    "        return q_value, conv1, conv2,reshape,fc1\n",
    "\n",
    "    def _build_graph(self):\n",
    "\n",
    "        self.q_prob = tf.nn.softmax(self.q_value)\n",
    "        self.q_argmax = tf.argmax(self.q_value, axis=1)\n",
    "        self.q_learning_max = tf.reduce_max(self.q_value, axis=1)\n",
    "        self.q_target_max = tf.reduce_max(self.q_value_target, axis=1)\n",
    "        \n",
    "        action_idxlist = tf.transpose([tf.range(tf.shape(self.q_value)[0]), self.a_])\n",
    "        \n",
    "        self.weights_params_learning = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"/learning_network\")\n",
    "        self.weights_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"/target_network\")\n",
    "        \n",
    "        self.tderror_one = LAMBDA_ONE * tf.abs(self.target_one_ - tf.reduce_max(self.q_value, axis=1))\n",
    "        self.loss_one = (LAMBDA_ONE * tf.square(self.target_one_ - tf.reduce_max(self.q_value, axis=1))) * self.is_weight_\n",
    "        self.tderror_n = LAMBDA1 * tf.abs(self.target_n_ - tf.reduce_max(self.q_value, axis=1))\n",
    "        self.loss_n = (LAMBDA1 * tf.square(self.target_n_ - tf.reduce_max(self.q_value, axis=1)))*self.is_weight_\n",
    "        self.loss_l2 = LAMBDA3 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params_learning])\n",
    "        \n",
    "#         self.tderror_one = LAMBDA_ONE * tf.abs(self.target_one_ - tf.gather_nd(self.q_value, indices=action_idxlist))\n",
    "#         self.loss_one = (LAMBDA_ONE * tf.square(self.target_one_ - tf.gather_nd(self.q_value, indices=action_idxlist))) * self.is_weight_\n",
    "#         self.tderror_n = LAMBDA1 * tf.abs(self.target_n_ - tf.gather_nd(self.q_value, indices=action_idxlist))\n",
    "#         self.loss_n = (LAMBDA1 * tf.square(self.target_n_ - tf.gather_nd(self.q_value, indices=action_idxlist)))*self.is_weight_\n",
    "#         self.loss_l2 = LAMBDA3 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params_learning])\n",
    "        \n",
    "#         self.tderror_one = LAMBDA_ONE * tf.abs(tf.reduce_mean(self.target_one_ - self.q_value, axis=1))\n",
    "#         self.loss_one = (LAMBDA_ONE * tf.square(tf.reduce_mean(self.target_one_ - self.q_value, axis=1))) * self.is_weight_\n",
    "#         self.tderror_n = LAMBDA1 * tf.abs(tf.reduce_mean(self.target_n_ - self.q_value, axis=1))\n",
    "#         self.loss_n = (LAMBDA1 * tf.square(tf.reduce_mean(self.target_n_ - self.q_value, axis=1)))*self.is_weight_\n",
    "#         self.loss_l2 = LAMBDA3 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params_learning])\n",
    "        \n",
    "        self.loss_mergin = LAMBDA2 * ((tf.stop_gradient(tf.reduce_max(self.q_value + self.mergin_, axis=1)) - tf.gather_nd(self.q_value,indices=action_idxlist))*self.isdemo_)\n",
    "        \n",
    "        self.tderror_total = self.tderror_one + self.tderror_n + self.loss_mergin\n",
    "        self.loss_total = tf.reduce_mean(self.loss_one +  self.loss_n + self.loss_mergin + self.loss_l2)\n",
    "#         self.tderror_total = self.tderror_n\n",
    "#         self.loss_total = tf.reduce_mean(self.loss_n+ self.loss_mergin + self.loss_l2)\n",
    "        \n",
    "        self.grads = tf.gradients(self.loss_total ,self.weights_params_learning)\n",
    "        \n",
    "        self.copy_params = [t.assign(l) for l,t in zip(self.weights_params_learning, self.weights_params_target)]\n",
    "        \n",
    "    def copy_network_learning2target(self, sess):\n",
    "        return sess.run(self.copy_params)\n",
    "        \n",
    "    def pull_parameter_server(self, sess):\n",
    "        return sess.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self,sess):\n",
    "        return sess.run(self.push_local_weight_params)\n",
    "        \n",
    "    def get_weights_learngin(self, sess):\n",
    "        return sess.run(self.weights_params_learning)\n",
    "    \n",
    "    def get_weights_target(self, sess):\n",
    "        return sess.run(self.weights_params_target)\n",
    "    \n",
    "    def get_loss(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), N_AGENT_ACTION)) * MERGIN_VALUE\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "#         l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "        l_one, tderror_total = sess.run([self.loss_n, self.tderror_n], feed_dict)\n",
    "        return l_one, 0,0,0, tderror_total\n",
    "    \n",
    "    def get_losstotal(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), N_AGENT_ACTION)) * MERGIN_VALUE\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "        loss_total = sess.run([self.loss_total], feed_dict)\n",
    "        return loss_total[0]\n",
    "    \n",
    "    def get_grads(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), N_AGENT_ACTION)) * MERGIN_VALUE\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "        grads = sess.run(self.grads, feed_dict)\n",
    "        return grads\n",
    "    \n",
    "    def update_parameter_server(self, sess, s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        assert np.ndim(s1) == 4\n",
    "        mergin_value = np.ones((len(s1), N_AGENT_ACTION)) * MERGIN_VALUE\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "#         _,l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.update, self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "#         _,l_one,l_mergin,l_l2 ,tderror_total = sess.run([self.update_global_weight_params,self.loss_n,self.loss_mergin,self.loss_l2, self.tderror_total], feed_dict)\n",
    "#         return l_one, 0,l_mergin,l_l2, tderror_total\n",
    "        _,l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.update_global_weight_params, self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "        return l_one, l_n, l_mergin, l_l2, tderror_total\n",
    "    \n",
    "    def check_weights(self, sess):\n",
    "        weights = SESS.run(self.weights_params_learning)\n",
    "        assert np.isnan([np.mean(w) for w in weights]).any()==False , print(weights)\n",
    "\n",
    "    def get_qvalue_learning(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_value, {self.state1_: s1})\n",
    "    \n",
    "    def get_qvalue_lerning_max(self, sess, s1):\n",
    "        return sess.run(self.q_learing_max, {self.state1_:s1})\n",
    "\n",
    "    def get_qvalue_target(self, sess ,s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_value_target, {self.state1_target_:s1})\n",
    "    \n",
    "    def get_qvalue_target_max(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_target_max, {self.state1_target_:s1})\n",
    "    \n",
    "    def get_qvalue_max_learningaction(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        action_idx, q_value = sess.run([self.q_argmax, self.q_value_target], {self.state1_:s1, self.state1_target_:s1})\n",
    "        return q_value[range(np.shape(s1)[0]), action_idx]\n",
    "    \n",
    "    def get_policy(self, sess, s1):\n",
    "        return sess.run(self.q_prob, {self.state1_: s1})\n",
    "    \n",
    "    def get_best_action(self,sess, s1):\n",
    "        return sess.run(self.q_argmax, {self.state1_:s1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 16\n",
    "        kernel_size = [6,6]\n",
    "        stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.layers.conv2d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        filters=num_outputs,\\\n",
    "                                        strides=stride,padding=padding,activation=activation,\\\n",
    "                                        kernel_initializer=weights_init,\\\n",
    "                                        bias_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "        return tf.nn.max_pool2d(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 16\n",
    "        kernel_size = [3,3]\n",
    "        stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.layers.conv2d(pre_layer,kernel_size=kernel_size,filters=num_outputs,\\\n",
    "                                        strides=stride,padding=padding,activation=activation,\\\n",
    "                                        kernel_initializer=weights_init,bias_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "        return tf.nn.max_pool2d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        shape = pre_layer.get_shape()\n",
    "#         return tf.reshape(pre_layer, shape=(-1,shape[1], shape[2]*shape[3]*shape[4]))\n",
    "        return tf.reshape(pre_layer, shape=(-1,shape[1]*shape[2]*shape[3]))\n",
    "\n",
    "    \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs =1024\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def q_value(pre_layer):\n",
    "        num_outputs=N_AGENT_ACTION\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,weights_initializer=weights_init, biases_initializer=bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demo_one(replay_memory, demo_path):\n",
    "    for demo in demo_path[:]:\n",
    "        print(demo)\n",
    "        file = h5py.File(demo, 'r')\n",
    "        episodes = list(file.keys())[1:]\n",
    "        game = GameInstanceBasic(DoomGame(),name=\"noname\",n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "        total_n_transit = 0\n",
    "        for e in episodes:\n",
    "            states_row = file[e+\"/states\"][:]\n",
    "            action_row = file[e+\"/action\"][:]\n",
    "            \n",
    "            n_transit = len(states_row)\n",
    "            valid_transit_count = 0\n",
    "\n",
    "            memory = []\n",
    "            \n",
    "            for i in range(0, n_transit):\n",
    "                \n",
    "                if not (sum(action_row[i]) == 0): \n",
    "                    s1_ = states_row[i]\n",
    "                    if i == n_transit -1 :\n",
    "                        isterminal = True\n",
    "                        s2_ = np.zeros(RESOLUTION)\n",
    "                        r = REWARDS['kill']\n",
    "                    else:\n",
    "                        isterminal = False\n",
    "                        s2_ = states_row[i+1]\n",
    "                        r = REWARDS['living']\n",
    "\n",
    "                    action = np.where(action_row[i]==1)[0][0]\n",
    "    #                 action = 0\n",
    "    #                 for i, e_a in enumerate(action_row[i]):\n",
    "    #                     action += e_a * 2**i\n",
    "\n",
    "                    memory.append([s1_,action, s2_, r, isterminal, True])\n",
    "\n",
    "                    if len(memory) == N_ADV or isterminal==True:\n",
    "                        R_adv = 0\n",
    "                        len_memory = len(memory)\n",
    "                        _, _, s2_adv, _, _, _ = memory[-1]\n",
    "                        for i in range(len_memory - 1, -1, -1):\n",
    "                            s1,a, s2 ,r,isterminal,isdemo = memory[i]\n",
    "                            R_adv = r + GAMMA*R_adv\n",
    "                            replaymemory.store(np.array([s1, a,s2 ,s2_adv,r ,R_adv ,isterminal, isdemo]))\n",
    "                        memory = []\n",
    "                    valid_transit_count += 1\n",
    "                total_n_transit += valid_transit_count\n",
    "                    \n",
    "            \n",
    "        file.close()\n",
    "    replay_memory.set_permanent_data(total_n_transit)\n",
    "    print(len(replay_memory), \"data are stored\")\n",
    "    file.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_positivedata(replay_memory, data_path_list):\n",
    "    for data_path in data_path_list:\n",
    "        print(data_path)\n",
    "        p_data = np.load(data_path)\n",
    "        for d in p_data:\n",
    "            replay_memory.store(d)\n",
    "    n_data = len(replay_memory)\n",
    "    replay_memory.set_permanent_data(n_data)\n",
    "    print(n_data, \"data are stored\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"learning_imitation\":\n",
    "    print(LOGDIR)\n",
    "    replaymemory = ReplayMemory(10000)\n",
    "    load_demo_one(replaymemory, DEMO_PATH)\n",
    "    \n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        parameter_server = ParameterServer(sess,LOGDIR)\n",
    "\n",
    "        starttime = datetime.now().timestamp()\n",
    "        end_time = (datetime.now() + timedelta(minutes=30)).timestamp()\n",
    "        \n",
    "        coordinator = tf.train.Coordinator()\n",
    "\n",
    "        name = \"worker_imitation\"\n",
    "        game_instance = GameInstanceBasic(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network,random_seed=0)\n",
    "#         imitation_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time, random_seed=0)\n",
    "        imitation_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, n_step=N_STEPS, random_seed=0)\n",
    "        imitation_env.log_server = parameter_server\n",
    "        imitation_env.replay_memory = replaymemory\n",
    "        thread_imitation = threading.Thread(target=imitation_env.run_prelearning, args=(coordinator,))\n",
    "\n",
    "        name = \"test\"\n",
    "        game_instance = GameInstanceBasic(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network,random_seed=100)\n",
    "#         test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time, random_seed=100)\n",
    "        test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, n_step=N_STEPS, random_seed=0)\n",
    "        test_env.log_server = parameter_server\n",
    "        thread_test = threading.Thread(target=test_env.run_test, args=(coordinator,))\n",
    "        \n",
    "        parameter_server.write_graph(sess)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "\n",
    "        print(\"-----Start IMITATION LEARNING----\")\n",
    "        threads = [thread_imitation,thread_test]\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "#         coordinator.join(threads)\n",
    "        while True:\n",
    "            time.sleep(10)\n",
    "            if imitation_env.progress >= 1.0:\n",
    "                coordinator.request_stop()\n",
    "                break\n",
    "\n",
    "        parameter_server.save_model(sess=sess, step=15, model_path=MODEL_PATH)\n",
    "\n",
    "        print(LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"learning_async\":\n",
    "    print(LOGDIR)\n",
    "    replaymemory = ReplayMemory(50000)\n",
    "    load_demo_one(replaymemory, DEMO_PATH)\n",
    "#     load_positivedata(replaymemory, POSITIVEDATA_PATH)\n",
    "    \n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        parameter_server = ParameterServer(sess,LOGDIR)\n",
    "\n",
    "        starttime = datetime.now().timestamp()\n",
    "        end_time = (datetime.now() + timedelta(minutes=10)).timestamp()\n",
    "        \n",
    "        coordinator = tf.train.Coordinator()\n",
    "        \n",
    "        environments, threads = [], []\n",
    "        \n",
    "        for i in range(N_WORKERS):\n",
    "            name = \"worker_%d\"%(i+1)\n",
    "            game_instance=GameInstanceBasic(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "#             game_instance=GameInstanceSimpleBasic(DoomGame(),name=name,config_path=CONFIG_FILE_PATH)\n",
    "            network = NetworkLocal(name, parameter_server)\n",
    "            agent = Agent(network, random_seed=i)\n",
    "#             e = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time, random_seed=i)\n",
    "            e = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, n_step=N_STEPS, random_seed=i)\n",
    "            e.replay_memory = replaymemory\n",
    "            environments.append(e)\n",
    "\n",
    "        environments[0].log_server = parameter_server\n",
    "        environments[0].times_act = []\n",
    "        environments[0].times_update = []\n",
    "        \n",
    "#         name = \"updating\"\n",
    "#         game_instance_update=GameInstanceBasic(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "# #             game_instance=GameInstanceSimpleBasic(DoomGame(),name=name,config_path=CONFIG_FILE_PATH)\n",
    "#         network_update = NetworkLocal(name, parameter_server)\n",
    "#         agent_update = Agent(network, random_seed=99)\n",
    "#         update_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time, random_seed=99)\n",
    "#         update_env.replay_memory = replaymemory\n",
    "#         thread_update = threading.Thread(target=update_env.run_prelearning, args=(coordinator,))\n",
    "        \n",
    "#         update_env.log_server = parameter_server\n",
    "#         threads.append(thread_update)\n",
    "\n",
    "        name = \"test\"\n",
    "#         test_seed = np.random.randint(1000)\n",
    "        test_seed = 100\n",
    "        game_instance=GameInstanceBasic(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "#         game_instance=GameInstanceSimpleBasic(DoomGame(),name=name,config_path=CONFIG_FILE_PATH)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network, random_seed=test_seed)\n",
    "#         test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time, random_seed=test_seed)\n",
    "        test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, n_step=N_STEPS, random_seed=test_seed)\n",
    "        test_env.log_server = parameter_server\n",
    "        test_env.rewards_detail = []\n",
    "        thread_test = threading.Thread(target=test_env.run_test, args=(coordinator,))\n",
    "\n",
    "    for e in environments:\n",
    "#         threads.append(threading.Thread(target=e.run_exploring, args=(coordinator,)))\n",
    "            threads.append(threading.Thread(target=e.run_learning, args=(coordinator,)))\n",
    "\n",
    "    threads.append(thread_test)\n",
    "\n",
    "    parameter_server.write_graph(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#         parameter_server.load_model(sess=sess, step=15, model_path=\"./models/model_imitation181221/model.ckpt\")\n",
    "#         parameter_server.load_model(sess=sess, step=15, model_path=\"./models/largebasic_random/model_largebasicrandom_imitation190109/model.ckpt\")\n",
    "#         parameter_server.load_model(sess=sess, step=15, model_path=\"models/model_temp/model_2019-01-16-15-32-54/model.ckpt\")\n",
    "#     parameter_server.load_model(sess=sess, step=15, model_path=\"./models/model_temp/model_2019-01-27-15-58-51/model.ckpt\")\n",
    "    parameter_server.load_model(sess=sess, step=15, model_path=IMIT_MODEL_PATH)\n",
    "\n",
    "    print(\"-----Start ASYNC LEARNING----\")\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "#     coordinator.join(threads)\n",
    "    while True:\n",
    "        time.sleep(10)\n",
    "        if np.array([e.progress >= 1.0 for e in environments]).all():\n",
    "            coordinator.request_stop()\n",
    "            break\n",
    "    \n",
    "    parameter_server.save_model(sess=sess, step=15, model_path=MODEL_PATH)\n",
    "\n",
    "    GIF_BUFF = []\n",
    "    REWARD_BUFF = []\n",
    "    r,f,d,imgs,_,step = test_env.test_agent(gif_buff=GIF_BUFF,reward_buff=REWARD_BUFF)\n",
    "    GIF_BUFF[0].save('gifs/test.gif',save_all=True, append_images=GIF_BUFF[1:], optimize=False, duration=40*4, loop=0)\n",
    "\n",
    "    print(LOGDIR)\n",
    "    print(sum([e.step for e in environments]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"test\":\n",
    "    print(LOGDIR)\n",
    "    replaymemory = ReplayMemory(50000)\n",
    "    load_demo_one(replaymemory, DEMO_PATH)\n",
    "#     load_positivedata(replaymemory, POSITIVEDATA_PATH)\n",
    "    \n",
    "    tf.set_random_seed(0)\n",
    "    \n",
    "#     N_STEPS = 25000\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        parameter_server = ParameterServer(sess,LOGDIR)\n",
    "\n",
    "        starttime = datetime.now().timestamp()\n",
    "        end_time = (datetime.now() + timedelta(minutes=60)).timestamp()\n",
    "        \n",
    "        coordinator = tf.train.Coordinator()\n",
    "\n",
    "        name = \"test\"\n",
    "#         test_seed = np.random.randint(1000)\n",
    "        test_seed = 100\n",
    "        game_instance=GameInstanceBasic(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "#         game_instance=GameInstanceSimpleBasic(DoomGame(),name=name,config_path=CONFIG_FILE_PATH)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network, random_seed=test_seed)\n",
    "        test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time, random_seed=test_seed)\n",
    "#         test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, n_step=N_STEPS, random_seed=test_seed)\n",
    "        test_env.log_server = parameter_server\n",
    "\n",
    "#         parameter_server.load_model(sess=sess, step=15, model_path=\"./models/model_imitation181221/model.ckpt\")\n",
    "#         parameter_server.load_model(sess=sess, step=15, model_path=\"./models/largebasic_random/model_largebasicrandom_imitation190109/model.ckpt\")\n",
    "#         parameter_server.load_model(sess=sess, step=15, model_path=\"models/model_temp/model_2019-01-16-15-32-54/model.ckpt\")\n",
    "    parameter_server.load_model(sess=sess, step=15, model_path=\"../data/demo_dqn/models/model_2019-01-31-10-44-57/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_priority(replaymemory):\n",
    "    size = len(replaymemory)\n",
    "    lengh = replaymemory.tree.capacity\n",
    "    start_idx = lengh - 1\n",
    "    end_idx = start_idx + size\n",
    "    priority = replaymemory.tree.tree[start_idx:end_idx]\n",
    "    plt.plot(priority)\n",
    "    \n",
    "def save_gif10(env):\n",
    "    GIF_BUFF_TOTAL = []\n",
    "    for i in range(10):\n",
    "        buff = []\n",
    "        val = test_env.test_agent(gif_buff=buff)\n",
    "        print(\"REWARD:\",val[0])\n",
    "        GIF_BUFF_TOTAL = GIF_BUFF_TOTAL + buff\n",
    "    GIF_BUFF_TOTAL[0].save('gifs/test.gif',save_all=True, append_images=GIF_BUFF_TOTAL[1:], optimize=False, duration=40*4, loop=0)\n",
    "\n",
    "def plot_conv(env,s1):\n",
    "    conv = sess.run(env.network.conv1,{env.network.state1_:[s1]})[0]\n",
    "    display_img = conv[-1]\n",
    "    print(display_img.shape)\n",
    "    fig,axes = plt.subplots(4,8,figsize=(20,15))\n",
    "    display_img = display_img.transpose((2,0,1))\n",
    "    for ax,img in zip(axes.ravel(), display_img):\n",
    "        ax.imshow(img)\n",
    "        \n",
    "def plot_conv_onw(env,s1):\n",
    "    conv = sess.run(env.network.conv1,{env.network.state1_:[s1]})[0]\n",
    "    display_img = conv\n",
    "    print(display_img.shape)\n",
    "    fig,axes = plt.subplots(4,8,figsize=(20,15))\n",
    "    display_img = display_img.transpose((2,0,1))\n",
    "    for ax,img in zip(axes.ravel(), display_img):\n",
    "        ax.imshow(img)\n",
    "\n",
    "def plot_q_learning(env, s1):\n",
    "    q_value = env.network.get_qvalue_learning(sess,s1)\n",
    "#     q_value = env.network.get_qvalue_target(sess,s1)\n",
    "    fig,axes = plt.subplots(10,figsize=(20,20))\n",
    "    for ax,q  in zip(axes.ravel(), q_value):\n",
    "        ax.bar(range(len(q)), q)\n",
    "        \n",
    "    return q_value\n",
    "\n",
    "def plot_q_target(env, s1):\n",
    "    q_value = env.network.get_qvalue_target(sess,s1)\n",
    "    fig,axes = plt.subplots(10,figsize=(20,20))\n",
    "    for ax,q  in zip(axes.ravel(), q_value):\n",
    "        ax.bar(range(len(q)), q)\n",
    "        \n",
    "    return q_value\n",
    "\n",
    "def plot_q_softmax(env, s1):\n",
    "    q_value = env.network.get_policy(sess,s1)\n",
    "    fig,axes = plt.subplots(10,figsize=(20,20))\n",
    "    for ax,q  in zip(axes.ravel(), q_value):\n",
    "        ax.bar(range(len(q)), q)\n",
    "        \n",
    "    return q_value\n",
    "\n",
    "def plot_diff_qvalue(env, s1):\n",
    "    q_l = env.network.get_qvalue_learning(sess,s1)\n",
    "    q_t = env.network.get_qvalue_target(sess,s1)\n",
    "    fig,axes = plt.subplots(10,figsize=(20,20))\n",
    "    for ax,q  in zip(axes.ravel(),q_t-q_l):\n",
    "        ax.bar(range(len(q)), q)\n",
    "\n",
    "def plot_s1(s1):\n",
    "    fig,axes = plt.subplots(10,5,figsize=(20,20))\n",
    "    for ax,s  in zip(axes,s1):\n",
    "        for a,img in zip(ax,s):\n",
    "            a.imshow(img)\n",
    "            \n",
    "def plot_s1_one(s1):\n",
    "    fig,axes = plt.subplots(len(s1),figsize=(20,20))\n",
    "    for ax,s  in zip(axes,s1):\n",
    "        ax.imshow(s)\n",
    "\n",
    "def plot_tderror(env, s1,action, s2_one,s2_adv,r_one, r_adv,isdemo, isterminal):\n",
    "    predicted_q_adv  = env.network.get_qvalue_max_learningaction(sess,s2_adv)\n",
    "    predicted_q_one = env.network.get_qvalue_max_learningaction(sess,s2_one)\n",
    "    \n",
    "    isnotterminal = np.ones((len(isterminal),)) - isterminal\n",
    "    target_one = r_one + GAMMA*predicted_q_one * isnotterminal\n",
    "    target_adv = r_adv + GAMMA**N_ADV * predicted_q_adv * isnotterminal\n",
    "    action = [int(a) for a in action]\n",
    "    isweight = np.ones((len(action),))\n",
    "    loss_values = env.network.get_loss(sess, s1,action,target_one, target_adv,isdemo,isweight)\n",
    "    plt.bar(range(len(isweight)),loss_values[-1])\n",
    "    return loss_values[-1]\n",
    "\n",
    "def plot_loss_one(env, s1,action, s2_one,s2_adv,r_one, r_adv,isdemo, isterminal):\n",
    "    predicted_q_adv  = env.network.get_qvalue_max_learningaction(sess,s2_adv)\n",
    "    predicted_q_one = env.network.get_qvalue_max_learningaction(sess,s2_one)\n",
    "    \n",
    "    isnotterminal = np.ones((len(isterminal),)) - isterminal\n",
    "    target_one = r_one + GAMMA*predicted_q_one * isnotterminal\n",
    "    target_adv = r_adv + GAMMA**N_ADV * predicted_q_adv * isnotterminal\n",
    "    action = [int(a) for a in action]\n",
    "    isweight = np.ones((len(action),))\n",
    "    loss_values = env.network.get_loss(sess, s1,action,target_one, target_adv,isdemo,isweight)\n",
    "    print(loss_values[0])\n",
    "    print(np.mean(loss_values[0]))\n",
    "    plt.bar(range(len(action)), loss_values[0])\n",
    "\n",
    "def plot_loss_adv(env, s1,action, s2_one,s2_adv,r_one, r_adv,isdemo, isterminal):\n",
    "    predicted_q_adv  = env.network.get_qvalue_max_learningaction(sess,s2_adv)\n",
    "    predicted_q_one = env.network.get_qvalue_max_learningaction(sess,s2_one)\n",
    "    \n",
    "    isnotterminal = np.ones((len(isterminal),)) - isterminal\n",
    "    target_one = r_one + GAMMA*predicted_q_one * isnotterminal\n",
    "    target_adv = r_adv + GAMMA**N_ADV * predicted_q_adv * isnotterminal\n",
    "    action = [int(a) for a in action]\n",
    "    isweight = np.ones((len(action),))\n",
    "    loss_values = env.network.get_loss(sess, s1,action,target_one, target_adv,isdemo,isweight)\n",
    "    print(loss_values[1])\n",
    "    print(np.mean(loss_values[1]))\n",
    "    plt.bar(range(len(action)), loss_values[1])\n",
    "    \n",
    "def plot_losstotal(env, s1,action, s2_one,s2_adv,r_one, r_adv,isdemo, isterminal):\n",
    "    predicted_q_adv  = env.network.get_qvalue_max_learningaction(sess,s2_adv)\n",
    "    predicted_q_one = env.network.get_qvalue_max_learningaction(sess,s2_one)\n",
    "    \n",
    "    isnotterminal = np.ones((len(isterminal),)) - isterminal\n",
    "    target_one = r_one + GAMMA*predicted_q_one * isnotterminal\n",
    "    target_adv = r_adv + GAMMA**N_ADV * predicted_q_adv * isnotterminal\n",
    "    action = [int(a) for a in action]\n",
    "    isweight = np.ones((len(action),))\n",
    "    loss_total = env.network.get_losstotal(sess, s1,action,target_one, target_adv,isdemo,isweight)\n",
    "    print(loss_total)\n",
    "\n",
    "def plot_loss_class(env, s1,action, s2_one,s2_adv,r_one, r_adv,isdemo, isterminal):\n",
    "    predicted_q_adv  = env.network.get_qvalue_max_learningaction(sess,s2_adv)\n",
    "    predicted_q_one = env.network.get_qvalue_max_learningaction(sess,s2_one)\n",
    "    \n",
    "    isnotterminal = np.ones((len(isterminal),)) - isterminal\n",
    "    target_one = r_one + GAMMA*predicted_q_one * isnotterminal\n",
    "    target_adv = r_adv + GAMMA**N_ADV * predicted_q_adv * isnotterminal\n",
    "    action = [int(a) for a in action]\n",
    "    isweight = np.ones((len(action),))\n",
    "    loss_values = env.network.get_loss(sess, s1,action,target_one, target_adv,isdemo,isweight)\n",
    "    print(loss_values[2])\n",
    "    plt.bar(range(len(action)), loss_values[2])\n",
    "    \n",
    "def plot_freq_sample(replaymemory):\n",
    "    idx = []\n",
    "    demo_r = []\n",
    "    for i in range(10000):\n",
    "        tree_idx,data,_ = replaymemory.sample(1, 0.5)\n",
    "        idx.append(tree_idx)\n",
    "    idx = np.array(idx).reshape((-1))\n",
    "    print(idx)\n",
    "    count = np.zeros((len(replaymemory),))\n",
    "    for i in idx:\n",
    "        count[i-replaymemory.tree.capacity]+= 1\n",
    "    plt.plot(count)\n",
    "    \n",
    "def play_games(env, t=50):\n",
    "    kill,reward,step = [],[],[]\n",
    "    for i in range(t):\n",
    "        r,f, d,k,t,s = env.test_agent()\n",
    "        kill.append(k)\n",
    "        reward.append(r)\n",
    "        step.append(s)\n",
    "    return np.array(kill), np.array(reward), np.array(step)\n",
    "\n",
    "def plot_filter_conv1(env):\n",
    "    weights = env.network.get_weights_learngin(sess)\n",
    "    weights_conv1 = weights[0]\n",
    "    fig,axes = plt.subplots(3,16,figsize=(20,20))\n",
    "    weights_conv1 = weights_conv1.transpose(2,3,0,1)\n",
    "    for ax,img in zip(axes.ravel(), weights_conv1.reshape(-1,6,6)):\n",
    "        ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_losstotal(env=imitation_env, s1=demo_s,action=demo_a,s2_one=demo_s2_one, s2_adv=demo_s2_adv, \\\n",
    "#              r_one=demo_r_one, r_adv=demo_r_adv ,isterminal=demo_t,isdemo=demo_d)\n",
    "\n",
    "# for i,d in enumerate(zip(demo_a, demo_r_one)):\n",
    "#     print(i,d[0],d[1])\n",
    "\n",
    "# plot_tderror(env=environments[0], s1=demo_s,action=demo_a,s2_one=demo_s2_one, s2_adv=demo_s2_adv, \\\n",
    "#              r_one=demo_r_one, r_adv=demo_r_adv ,isterminal=demo_t,isdemo=demo_d)\n",
    "\n",
    "# plot_loss_one(env=imitation_env, s1=demo_s,action=demo_a,s2_one=demo_s2_one, s2_adv=demo_s2_adv, \\\n",
    "#              r_one=demo_r_one, r_adv=demo_r_adv ,isterminal=demo_t,isdemo=demo_d)\n",
    "\n",
    "# hoge = plot_q_learning(env=environments[0], s1=demo_s)\n",
    "\n",
    "# kills, rewards,steps = play_games(test_env,t=100)\n",
    "\n",
    "# steps[rewards < 0] = 100\n",
    "\n",
    "# print(np.mean(kills),\"+-\",np.var(kills))\n",
    "# print(np.mean(rewards),\"+-\",np.var(rewards))\n",
    "# print(np.mean(steps),\"+-\",np.var(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# tf.set_random_seed(0)\n",
    "    \n",
    "# config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.log_device_placement = False\n",
    "# config.allow_soft_placement = True\n",
    "# sess = tf.Session(config=config)\n",
    "# parameter_server = ParameterServer(sess,LOGDIR)\n",
    "\n",
    "# starttime = datetime.now().timestamp()\n",
    "# end_time = (datetime.now() + timedelta(minutes=60)).timestamp()\n",
    "\n",
    "# coordinator = tf.train.Coordinator()\n",
    "\n",
    "# environments, threads = [], []\n",
    "# name = \"test\"\n",
    "# game_instance=GameInstanceBasic(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "# #         game_instance=GameInstanceSimpleBasic(DoomGame(),name=name,config_path=CONFIG_FILE_PATH)\n",
    "# network = NetworkLocal(name, parameter_server)\n",
    "# agent = Agent(network)\n",
    "# test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time, random_seed=0)\n",
    "# test_env.log_server = parameter_server\n",
    "# test_env.rewards_detail = []\n",
    "# thread_test = threading.Thread(target=test_env.run_test, args=(coordinator,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"test\":\n",
    "    MODEL_PATH = \"./models/model_test/model.ckpt\"\n",
    "    TEST_GRADS = []\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    starttime = datetime.now().timestamp()\n",
    "    end_time = (datetime.now() + timedelta(minutes=15)).timestamp()\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        parameter_server = ParameterServer(sess,LOGDIR)\n",
    "        parameter_server.load_model(sess=sess, model_path=MODEL_PATH, step=15)\n",
    "        \n",
    "        name = \"test\"\n",
    "        game_instance = GameInstanceSimpleDeathmatch(DoomGame(),name=name,n_bots=1,config_path=CONFIG_FILE_PATH, reward_param=REWARDS, steps_update_origin=10,timelimit=2)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network)\n",
    "        test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time)\n",
    "        GIF_BUFF = []\n",
    "        REWARD_BUFF = []\n",
    "        r,f,d,_,imgs = test_env.test_agent(gif_buff=GIF_BUFF,reward_buff=REWARD_BUFF)\n",
    "        GIF_BUFF[0].save('gifs/test.gif',save_all=True, append_images=GIF_BUFF[1:], optimize=False, duration=40*4, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"test_game_instance\":\n",
    "    environments[0].game.new_episode()\n",
    "    pre_x = 0\n",
    "    pre_y = 0\n",
    "\n",
    "    print(environments[0].game.get_pos_x(),\"diff:\",environments[0].game.get_pos_x()-pre_x, \",\", environments[0].game.get_pos_y(),\"diff:\",environments[0].game.get_pos_y()-pre_y)\n",
    "    pre_x = environments[0].game.get_pos_x()\n",
    "    pre_y = environments[0].game.get_pos_y()\n",
    "    print(environments[0].game.make_action([0,0,0,1,0,1], FRAME_REPEAT))\n",
    "    # print(environments[0].game.game)\n",
    "    plt.imshow(environments[0].preprocess( environments[0].game.get_screen_buff()))\n",
    "\n",
    "    if(environments[0].game.is_player_dead()):\n",
    "        environments[0].game.respawn_player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"check_rewards\":\n",
    "    files = os.listdir(\"./playlogs/playlog_test/\")\n",
    "    files.sort()\n",
    "    rewards = []\n",
    "    for f in files[:]:\n",
    "        with open(os.path.join(\"./playlogs/playlog_test/\", f), 'rb') as file:\n",
    "            rewards.append((f, pickle.load(file)))\n",
    "\n",
    "def make_data(rewards_all):\n",
    "    data_frames = []\n",
    "    for f_n, reward in rewards_all:\n",
    "        reward_dist = []\n",
    "        reward_frag = []\n",
    "        reward_healthloss = []\n",
    "        reward_suicide = []\n",
    "        reward_total = []\n",
    "        for log in reward:\n",
    "            _,r = log\n",
    "            reward_dist.append(r['dist'])\n",
    "            reward_frag.append(r['frag'])\n",
    "            reward_healthloss.append(r['healthloss'])\n",
    "            reward_suicide.append(r['suicide'])\n",
    "            reward_total.append(sum(r.values()))\n",
    "            \n",
    "        df = pd.DataFrame({'dist':reward_dist, 'frag': reward_frag, 'healthloss':reward_healthloss, 'suicide':reward_suicide})\n",
    "        data_frames.append(df)\n",
    "    return data_frames\n",
    "\n",
    "def plot_rewards_all(rewards_all):\n",
    "    \n",
    "    dist_all = [sum(r['dist'].values) for r in rewards_all]\n",
    "    frag_all = [sum(r['frag'].values) for r in rewards_all]\n",
    "    healthloss_all = np.array( [sum(r['healthloss'].values) for r in rewards_all])\n",
    "    healthloss_all= np.where(healthloss_all < -100, 0, healthloss_all)\n",
    "    suicide_all = [sum(r['suicide'].values) for r in rewards_all]\n",
    "    total = [sum(r) for r in zip(dist_all, frag_all, healthloss_all, suicide_all)]\n",
    "    \n",
    "    f = plt.figure()\n",
    "    f.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "    ax_dist = f.add_subplot(2,3,1)\n",
    "    ax_frag = f.add_subplot(2,3,2)\n",
    "    ax_healthloss = f.add_subplot(2,3,3)\n",
    "    \n",
    "    ax_suicide = f.add_subplot(2,3,4)\n",
    "    ax_total = f.add_subplot(2,3,5)\n",
    "    ax_dist.set_title(\"reward_dist\")\n",
    "    ax_frag.set_title(\"rewad_frag\")\n",
    "    ax_healthloss.set_title(\"reward_healthloss\")\n",
    "    ax_suicide.set_title(\"reward_suicide\")\n",
    "    ax_total.set_title(\"reward_total\")\n",
    "    ax_dist.plot(dist_all)\n",
    "    ax_frag.plot(frag_all)\n",
    "    ax_healthloss.plot(healthloss_all)\n",
    "    ax_suicide.plot(suicide_all)\n",
    "    ax_total.plot(total)\n",
    "    return f\n",
    "\n",
    "def plot_rewards_match(rewards):\n",
    "    reward_dist = rewards['dist'].values\n",
    "    reward_frag = rewards['frag'].values\n",
    "    reward_healthloss = rewards['healthloss'].values\n",
    "    reward_suicide = rewards['suicide'].values\n",
    "    reward_total = reward_frag + reward_healthloss + reward_suicide + reward_dist\n",
    "\n",
    "    f = plt.figure()\n",
    "    f.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "    ax_dist = f.add_subplot(2,3,1)\n",
    "    ax_frag = f.add_subplot(2,3,2)\n",
    "    ax_healthloss = f.add_subplot(2,3,3)\n",
    "    ax_suicide = f.add_subplot(2,3,4)\n",
    "    ax_total = f.add_subplot(2,3,5)\n",
    "    ax_dist.set_title(\"reward_dist\")\n",
    "    ax_frag.set_title(\"rewad_frag\")\n",
    "    ax_healthloss.set_title(\"reward_healthloss\")\n",
    "    ax_suicide.set_title(\"reward_suicide\")\n",
    "    ax_total.set_title(\"reward_total\")\n",
    "    ax_dist.plot(reward_dist)\n",
    "    ax_frag.plot(reward_frag)\n",
    "    ax_healthloss.plot(reward_healthloss)\n",
    "    ax_suicide.plot(reward_suicide)\n",
    "    ax_total.plot(reward_total)\n",
    "    return f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
