{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import time,random,threading\n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "CONFIG_FILE_PATH = \"./config/simpler_basic.cfg\"\n",
    "MODEL_PATH = \"./model_v00/model_v00.ckpt\"\n",
    "RESOLUTION = (40,60,1)\n",
    "\n",
    "N_ADV = 5\n",
    "\n",
    "UPDATE_FREQ = 10\n",
    "\n",
    "N_ACTION = 6\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "BOTS_NUM = 5\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "RMSProbDecaly = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,name, parameter_server):\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(CONFIG_FILE_PATH)\n",
    "        self.game.set_window_visible(True)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "        self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "#         self.game.set_screen_format(ScreenFormat.CRCGCB)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "\n",
    "        self.network = Network_local(name, parameter_server)\n",
    "        self.agent = Agent(name,self.network)\n",
    "        \n",
    "        self.pre_death = 0\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.game.new_episode()\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION,mode='constant')\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        global frames\n",
    "        self.start_episode()\n",
    "        \n",
    "        #Copy params from global\n",
    "        self.agent.network.pull_parameter_server()\n",
    "\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while not self.game.is_episode_finished():\n",
    "\n",
    "            s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "            action = self.agent.act_test(s1)\n",
    "            reward = self.game.make_action(action,1)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            \n",
    "            print(\"\\t # %d: action:\"%step,action,\"reward:\",reward)\n",
    "            \n",
    "            step += 1\n",
    "            total_reward += reward\n",
    "                \n",
    "        print(\"----------TEST at %d step-------------\"%(frames))\n",
    "        print(\"REWARD\",total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def state():\n",
    "        name = \"STATE\"\n",
    "        shape = [None,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]]\n",
    "        return tf.placeholder(tf.float32,shape=shape,name=name)\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 8\n",
    "        kernel_size = [6,6]\n",
    "        stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs, \\\n",
    "                                            stride=stride,padding=padding,activation_fn=activation, \\\n",
    "                                           weights_initializer=weights_init, \\\n",
    "                                            biases_initializer=bias_init)\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 16\n",
    "        kernel_size = [3,3]\n",
    "        stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs, \\\n",
    "                                            stride=stride,padding=padding,activation_fn=activation, \\\n",
    "                                           weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        return tf.contrib.layers.flatten(pre_layer)\n",
    "        \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs = 512\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def policy_mu(pre_layer):\n",
    "        num_outputs = 2\n",
    "        activation_fn = tf.nn.sigmoid\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init)*200 - 100\n",
    "    \n",
    "    def policy_gamma(pre_layer):\n",
    "        num_outputs = 2\n",
    "        activation_fn = tf.nn.softplus\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.sqrt(tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init))\n",
    "    def value(pre_layer):\n",
    "        num_outputs = 1\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                weights_initializer=weights_init, biases_initializer=bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --グローバルなTensorFlowのDeep Neural Networkのクラスです　-------\n",
    "class ParameterServer:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope(\"parameter_server\"):      # スレッド名で重み変数に名前を与え、識別します（Name Space）\n",
    "            self._build_model()            # ニューラルネットワークの形を決定\n",
    "            \n",
    "        with tf.variable_scope(\"summary\"):\n",
    "            self._summary()\n",
    "            \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)    # loss関数を最小化していくoptimizerの定義です\n",
    "        \n",
    "        print(\"-------GLOBAL-------\")\n",
    "        for w in self.weights_params:\n",
    "            print(w)\n",
    "\n",
    "    def _build_model(self):\n",
    "            self.state = NetworkSetting.state()\n",
    "            self.conv1 = NetworkSetting.conv1(self.state)\n",
    "            self.conv2 = NetworkSetting.conv2(self.conv1)\n",
    "            reshape = NetworkSetting.reshape(self.conv2)\n",
    "            fc1 = NetworkSetting.fc1(reshape)\n",
    "\n",
    "            with tf.variable_scope(\"policy\"):\n",
    "                self.mu = NetworkSetting.policy_mu(fc1)\n",
    "                self.gamma = NetworkSetting.policy_gamma(fc1)\n",
    "            \n",
    "            with tf.variable_scope(\"value\"):\n",
    "                self.value = NetworkSetting.value(fc1)\n",
    "    \n",
    "    # Recording node in tensorboard\n",
    "    def _summary(self):\n",
    "        \n",
    "        self.a_t = tf.placeholder(tf.float32, shape=(None, N_ACTION))\n",
    "        self.r_t = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "        # Normal Distributions as Policy\n",
    "        p_aim = tf.distributions.Normal(loc=self.mu[:,0],scale=self.gamma[:,0])\n",
    "        p_fire = tf.distributions.Normal(loc=self.mu[:,1],scale=self.gamma[:,1])\n",
    "\n",
    "        # Probability for Action_Aim\n",
    "        prob_aim = tf.reshape(p_aim.prob(self.a_t[:,0]),[-1,1],name=\"prob_aim\")\n",
    "\n",
    "        # Probability for Action_Fire\n",
    "        cdf_fire = p_fire.cdf(0.5)\n",
    "        prob_fire = cdf_fire * (tf.ones_like(self.a_t[:,1])-self.a_t[:,1]) + (tf.ones_like(self.a_t[:,1])-cdf_fire)*self.a_t[:,1]\n",
    "        prob_fire = tf.reshape(prob_fire,[-1,1],name=\"prob_fire\")\n",
    "\n",
    "        log_policy_aim = tf.log(prob_aim + 1e-10, name=\"log_policy_aim\")\n",
    "        log_policy_fire = tf.log(prob_fire + 1e-10, name=\"log_policy_fire\")\n",
    "\n",
    "        advantage = self.r_t - self.value\n",
    "\n",
    "        self.loss_policy_aim = -log_policy_aim * tf.stop_gradient(advantage)\n",
    "        self.loss_policy_fire = -log_policy_fire * tf.stop_gradient(advantage)\n",
    "\n",
    "        self.loss_value = tf.square(advantage)\n",
    "        \n",
    "        tf.summary.scalar('loss_aim',self.loss_policy_aim[0][0])\n",
    "        tf.summary.scalar('loss_fire', self.loss_policy_fire[0][0])\n",
    "        tf.summary.scalar('loss_value', self.loss_value[0][0])\n",
    "        \n",
    "        state_shape = self.state.get_shape()\n",
    "        conv1_shape = self.conv1.get_shape()\n",
    "        conv2_shape = self.conv2.get_shape()\n",
    "        tf.summary.image('state',tf.reshape(self.state,[-1, state_shape[1], state_shape[2], state_shape[3]]),1)\n",
    "        tf.summary.image('conv1',tf.reshape(self.conv1,[-1, conv1_shape[1], conv1_shape[2], 1]),1)\n",
    "        tf.summary.image('conv2',tf.reshape(self.conv2,[-1, conv2_shape[1], conv2_shape[2], 1]),1)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\",SESS.graph)\n",
    "        \n",
    "    def calc_loss(self,step,s1,a,r):\n",
    "        loss_aim,loss_fire,loss_v = SESS.run([self.loss_policy_aim, \\\n",
    "                                          self.loss_policy_fire, \\\n",
    "                                          self.loss_value], \\\n",
    "                                         feed_dict={self.state:s1,self.a_t:a,self.r_t:r})\n",
    "        return loss_aim,loss_fire,loss_v\n",
    "    \n",
    "    def write_summary(self,step,s1,a,r):\n",
    "        m = SESS.run(self.merged,feed_dict={self.state:s1,self.a_t:a,self.r_t:r})\n",
    "        self.writer.add_summary(m,step)\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.saver.save(SESS, MODEL_PATH)\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.saver.restore(SESS,MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,name,network):\n",
    "        self.name = name\n",
    "        self.network = network\n",
    "        self.memory = []\n",
    "    \n",
    "    def act(self,s1):\n",
    "        \n",
    "        global frames\n",
    "        \n",
    "        if frames>=EPS_STEPS:\n",
    "            eps = EPS_END\n",
    "        else:\n",
    "            eps = EPS_START + frames*(EPS_END - EPS_START) / EPS_STEPS\n",
    "        \n",
    "        if random.random() < eps:\n",
    "            aim = random.random() * MAX_AIM\n",
    "            attack = random.randint(0,1)\n",
    "            return [aim,attack]\n",
    "        else:\n",
    "            s1 = np.array([s1])\n",
    "            action_aim, action_fire = self.network.predict_actions(s1)\n",
    "            return [action_aim[0],action_fire[0]]\n",
    "        \n",
    "    def act_test(self,s1):\n",
    "        s1 = np.array([s1])\n",
    "        action_aim, action_fire = self.network.predict_actions(s1)\n",
    "        return [action_aim[0],action_fire[0]]\n",
    "    \n",
    "    def advantage_push_network(self,s1,action,reward,s2,isterminal):\n",
    "        \n",
    "        self.memory.append((s1,action,reward,s2))\n",
    "        \n",
    "        if isterminal:\n",
    "            for i in range(len(self.memory)-1,-1,-1):\n",
    "                s1,a,r,s2 = self.memory[i]\n",
    "                if i==N_ADV-1:\n",
    "                    self.R = 0\n",
    "                else:\n",
    "                    self.R = r + GAMMA*self.R\n",
    "                \n",
    "                self.network.train_push(s1,a,self.R,s2,isterminal)\n",
    "            \n",
    "            self.memory = []\n",
    "            self.R = 0\n",
    "            self.network.update_parameter_server()\n",
    "\n",
    "        if len(self.memory)>=N_ADV:\n",
    "            \n",
    "            for i in range(N_ADV-1,-1,-1):\n",
    "                s1,a,r,s2 = self.memory[i]\n",
    "                if i==N_ADV-1:\n",
    "                    self.R = self.network.predict_value(np.array([s1]))[0][0]\n",
    "                else:\n",
    "                    self.R = r + GAMMA*self.R\n",
    "                \n",
    "                self.network.train_push(s1,a,self.R,s2,isterminal)\n",
    "            \n",
    "            self.memory = []\n",
    "            self.R = 0\n",
    "            self.network.update_parameter_server()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_local(object):\n",
    "    def __init__(self,name,parameter_server):\n",
    "        self.name = name\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._model()\n",
    "            self._build_graph(parameter_server)\n",
    "            \n",
    "        self.s1 = np.empty(shape=(100,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.s2 = np.empty(shape=(100,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.reward = np.empty(shape=(100,1),dtype=np.float32)\n",
    "        self.action = np.empty(shape=(100,2),dtype=np.float32)\n",
    "        self.isterminal = np.empty(shape=(100,1),dtype=np.int8)\n",
    "        self.queue_pointer = 0\n",
    "        \n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self):\n",
    "        \n",
    "        self.state = NetworkSetting.state()\n",
    "        conv1 = NetworkSetting.conv1(self.state)\n",
    "        conv2 = NetworkSetting.conv2(conv1)\n",
    "        reshape = NetworkSetting.reshape(conv2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "\n",
    "        with tf.variable_scope(\"policy\"):\n",
    "            self.mu = NetworkSetting.policy_mu(fc1)\n",
    "            self.gamma = NetworkSetting.policy_gamma(fc1)\n",
    "\n",
    "        with tf.variable_scope(\"value\"):\n",
    "            self.value = NetworkSetting.value(fc1)\n",
    "            \n",
    "    def _build_graph(self,parameter_server):\n",
    "#         with tf.variable_scope(self.name+\"_graph\"):\n",
    "        self.a_t = tf.placeholder(tf.float32, shape=(None, N_ACTION))\n",
    "        self.r_t = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "        # Normal Distributions as Policy\n",
    "        self.p_aim = tf.distributions.Normal(loc=self.mu[:,0],scale=self.gamma[:,0] + 0.5)\n",
    "        self.p_fire = tf.distributions.Normal(loc=self.mu[:,1],scale=self.gamma[:,1] + 0.5)\n",
    "        \n",
    "        self.sample_aim = self.p_aim.sample([1])\n",
    "        self.sample_fire = self.p_fire.sample([1])\n",
    "\n",
    "        # Probability for Action_Aim\n",
    "        prob_aim = tf.reshape(self.p_aim.prob(self.a_t[:,0]),[-1,1],name=\"prob_aim\")\n",
    "\n",
    "        # Probability for Action_Fire\n",
    "        self.cdf_fire = self.p_fire.cdf(0.5)\n",
    "        self.prob_fire = self.cdf_fire * (tf.ones_like(self.a_t[:,1])-self.a_t[:,1]) + (tf.ones_like(self.a_t[:,1])-self.cdf_fire)*self.a_t[:,1]\n",
    "        self.prob_fire = tf.reshape(self.prob_fire,[-1,1],name=\"prob_fire\")\n",
    "\n",
    "        self.prob = tf.concat([prob_aim,self.prob_fire],1)\n",
    "\n",
    "        log_policy_aim = tf.log(prob_aim + 1e-10, name=\"log_policy_aim\")\n",
    "        log_policy_fire = tf.log(self.prob_fire + 1e-10, name=\"log_policy_fire\")\n",
    "\n",
    "        advantage = self.r_t - self.value\n",
    "\n",
    "        loss_policy_aim = -log_policy_aim * tf.stop_gradient(advantage)\n",
    "        loss_policy_fire = -log_policy_fire * tf.stop_gradient(advantage)\n",
    "\n",
    "        loss_value = tf.square(advantage)\n",
    "\n",
    "        self.loss_total = tf.reduce_mean(loss_policy_aim + loss_policy_fire + loss_value)\n",
    "\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "        self.grads = tf.gradients(self.loss_total, self.weights_params)\n",
    "\n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))\n",
    "\n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params,parameter_server.weights_params)]\n",
    "\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params)]\n",
    "        \n",
    "    def pull_parameter_server(self):\n",
    "        SESS.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self):\n",
    "        SESS.run(self.push_local_weight_params)\n",
    "        \n",
    "    def show_weights(self):\n",
    "        hoge = SESS.run(self.weights_params)\n",
    "        for i in range(len(hoge)):\n",
    "            print(hoge[i])\n",
    "            \n",
    "    def update_parameter_server(self):\n",
    "        if self.queue_pointer > 0:\n",
    "            s1 = self.s1[0:self.queue_pointer]\n",
    "            s2 = self.s2[0:self.queue_pointer]\n",
    "            r = self.reward[0:self.queue_pointer]\n",
    "            a = self.action[0:self.queue_pointer]\n",
    "            feed_dict = {self.state: s1,self.a_t:a, self.r_t:r}\n",
    "            SESS.run(self.update_global_weight_params,feed_dict)\n",
    "            self.queue_pointer = 0\n",
    "    \n",
    "    def predict_value(self,s):\n",
    "        v = SESS.run(self.value,feed_dict={self.state:s})\n",
    "        return v\n",
    "    \n",
    "    def predict_actions(self,s):\n",
    "        feed_dict = {self.state:s}\n",
    "        [action_aim, action_fire] = SESS.run([self.sample_aim, self.sample_fire],feed_dict)\n",
    "        # Encode action_fire to 0 or 1 \n",
    "        action_fire[action_fire>=0] = 1\n",
    "        action_fire[action_fire<0] = 0\n",
    "        return [action_aim[0],action_fire[0]]\n",
    "    \n",
    "    def predict_probability(self,s,a):\n",
    "        feed_dict = {self.state:s, self.a_t:a}\n",
    "        prob = SESS.run(self.prob, feed_dict)\n",
    "        return prob\n",
    "    \n",
    "    def train_push(self,s,a,r,s_,isterminal):\n",
    "        self.s1[self.queue_pointer] = s\n",
    "        self.s2[self.queue_pointer] = s_\n",
    "        self.action[self.queue_pointer] = a\n",
    "        self.reward[self.queue_pointer] = r\n",
    "        self.isterminal[self.queue_pointer] = isterminal\n",
    "        self.queue_pointer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------GLOBAL-------\n",
      "<tf.Variable 'parameter_server/Conv/weights:0' shape=(6, 6, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/Conv/biases:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/Conv_1/weights:0' shape=(3, 3, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/Conv_1/biases:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/fully_connected/weights:0' shape=(1120, 512) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected/weights:0' shape=(512, 2) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected/biases:0' shape=(2,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected_1/weights:0' shape=(512, 2) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/policy/fully_connected_1/biases:0' shape=(2,) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/value/fully_connected/weights:0' shape=(512, 1) dtype=float32_ref>\n",
      "<tf.Variable 'parameter_server/value/fully_connected/biases:0' shape=(1,) dtype=float32_ref>\n",
      "INFO:tensorflow:Restoring parameters from ./model_v00/model_v00.ckpt\n",
      "\t # 0: action: [-0.511026, 1.0] reward: -1.0\n",
      "\t # 1: action: [-1.7448442, 1.0] reward: -1.0\n",
      "\t # 2: action: [1.7588187, 1.0] reward: -1.0\n",
      "\t # 3: action: [-1.4693503, 1.0] reward: -1.0\n",
      "\t # 4: action: [3.8298807, 1.0] reward: -6.0\n",
      "\t # 5: action: [-1.6233054, 1.0] reward: -1.0\n",
      "\t # 6: action: [1.2834929, 1.0] reward: -1.0\n",
      "\t # 7: action: [0.68321204, 1.0] reward: -1.0\n",
      "\t # 8: action: [-0.2560836, 1.0] reward: -1.0\n",
      "\t # 9: action: [-2.4578757, 1.0] reward: -1.0\n",
      "\t # 10: action: [2.15423, 1.0] reward: -1.0\n",
      "\t # 11: action: [1.9329057, 1.0] reward: -1.0\n",
      "\t # 12: action: [-3.266975, 1.0] reward: -1.0\n",
      "\t # 13: action: [-0.370395, 1.0] reward: -1.0\n",
      "\t # 14: action: [-0.27015817, 1.0] reward: -1.0\n",
      "\t # 15: action: [0.42922735, 1.0] reward: -1.0\n",
      "\t # 16: action: [0.90788454, 1.0] reward: -1.0\n",
      "\t # 17: action: [0.84253967, 1.0] reward: -1.0\n",
      "\t # 18: action: [-2.0640202, 1.0] reward: -6.0\n",
      "\t # 19: action: [2.0146956, 1.0] reward: -1.0\n",
      "\t # 20: action: [1.5686038, 1.0] reward: -1.0\n",
      "\t # 21: action: [-5.361067, 1.0] reward: -1.0\n",
      "\t # 22: action: [2.709439, 1.0] reward: -1.0\n",
      "\t # 23: action: [1.4887005, 1.0] reward: -1.0\n",
      "\t # 24: action: [-1.5840969, 1.0] reward: -1.0\n",
      "\t # 25: action: [1.4849241, 1.0] reward: -1.0\n",
      "\t # 26: action: [-1.4080135, 1.0] reward: -1.0\n",
      "\t # 27: action: [-0.97625697, 1.0] reward: -1.0\n",
      "\t # 28: action: [2.8507786, 1.0] reward: -1.0\n",
      "\t # 29: action: [-1.2279638, 1.0] reward: -1.0\n",
      "\t # 30: action: [1.5916489, 1.0] reward: -1.0\n",
      "\t # 31: action: [-2.7147002, 1.0] reward: -1.0\n",
      "\t # 32: action: [-0.399948, 1.0] reward: -6.0\n",
      "\t # 33: action: [1.9298939, 1.0] reward: -1.0\n",
      "\t # 34: action: [2.4186497, 1.0] reward: -1.0\n",
      "\t # 35: action: [-3.4597359, 1.0] reward: -1.0\n",
      "\t # 36: action: [-2.0918481, 1.0] reward: -1.0\n",
      "\t # 37: action: [0.22481549, 1.0] reward: -1.0\n",
      "\t # 38: action: [1.3160157, 1.0] reward: -1.0\n",
      "\t # 39: action: [2.0552666, 1.0] reward: -1.0\n",
      "\t # 40: action: [-3.5325038, 1.0] reward: -1.0\n",
      "\t # 41: action: [-0.9762055, 1.0] reward: -1.0\n",
      "\t # 42: action: [3.314645, 1.0] reward: -1.0\n",
      "\t # 43: action: [-1.8229288, 1.0] reward: -1.0\n",
      "\t # 44: action: [-1.7307707, 1.0] reward: -1.0\n",
      "\t # 45: action: [0.92564285, 1.0] reward: -1.0\n",
      "\t # 46: action: [2.1806898, 1.0] reward: -6.0\n",
      "\t # 47: action: [0.42499924, 1.0] reward: -1.0\n",
      "\t # 48: action: [0.9206139, 1.0] reward: -1.0\n",
      "\t # 49: action: [-0.88811374, 1.0] reward: -1.0\n",
      "\t # 50: action: [-1.6128653, 1.0] reward: -1.0\n",
      "\t # 51: action: [0.9014136, 1.0] reward: -1.0\n",
      "\t # 52: action: [-1.1990994, 1.0] reward: -1.0\n",
      "\t # 53: action: [-0.4118085, 1.0] reward: -1.0\n",
      "\t # 54: action: [-0.26139045, 1.0] reward: -1.0\n",
      "\t # 55: action: [1.0337642, 1.0] reward: -1.0\n",
      "\t # 56: action: [0.82870746, 1.0] reward: -1.0\n",
      "\t # 57: action: [1.2655632, 1.0] reward: -1.0\n",
      "\t # 58: action: [1.5044672, 1.0] reward: -1.0\n",
      "\t # 59: action: [-2.963188, 1.0] reward: -1.0\n",
      "\t # 60: action: [1.9576694, 1.0] reward: -6.0\n",
      "\t # 61: action: [-0.97978413, 1.0] reward: -1.0\n",
      "\t # 62: action: [2.8086858, 1.0] reward: -1.0\n",
      "\t # 63: action: [-0.48938823, 1.0] reward: -1.0\n",
      "\t # 64: action: [-0.8692273, 1.0] reward: -1.0\n",
      "\t # 65: action: [-4.896206, 1.0] reward: -1.0\n",
      "\t # 66: action: [4.0928917, 1.0] reward: -1.0\n",
      "\t # 67: action: [0.26029366, 1.0] reward: -1.0\n",
      "\t # 68: action: [0.025885105, 1.0] reward: -1.0\n",
      "\t # 69: action: [-0.49940574, 1.0] reward: -1.0\n",
      "\t # 70: action: [0.4428045, 1.0] reward: -1.0\n",
      "\t # 71: action: [-0.84034574, 1.0] reward: -1.0\n",
      "\t # 72: action: [0.7558426, 1.0] reward: -1.0\n",
      "\t # 73: action: [-1.2840358, 1.0] reward: -1.0\n",
      "\t # 74: action: [0.710505, 1.0] reward: -6.0\n",
      "\t # 75: action: [-0.40333867, 1.0] reward: -1.0\n",
      "\t # 76: action: [-0.38290584, 1.0] reward: -1.0\n",
      "\t # 77: action: [-1.3861201, 1.0] reward: -1.0\n",
      "\t # 78: action: [3.246732, 1.0] reward: -1.0\n",
      "\t # 79: action: [-0.7137134, 1.0] reward: -1.0\n",
      "\t # 80: action: [1.490685, 1.0] reward: -1.0\n",
      "\t # 81: action: [-4.2057076, 1.0] reward: -1.0\n",
      "\t # 82: action: [1.082555, 1.0] reward: -1.0\n",
      "\t # 83: action: [0.9294439, 1.0] reward: -1.0\n",
      "\t # 84: action: [-2.4684017, 1.0] reward: -1.0\n",
      "\t # 85: action: [-0.33294594, 1.0] reward: -1.0\n",
      "\t # 86: action: [4.0234346, 1.0] reward: -1.0\n",
      "\t # 87: action: [-3.3955746, 1.0] reward: -1.0\n",
      "\t # 88: action: [-0.01155746, 1.0] reward: -6.0\n",
      "\t # 89: action: [-1.9383309, 1.0] reward: -1.0\n",
      "\t # 90: action: [6.4437857, 1.0] reward: -1.0\n",
      "\t # 91: action: [-4.3305116, 1.0] reward: -1.0\n",
      "\t # 92: action: [2.1997488, 1.0] reward: -1.0\n",
      "\t # 93: action: [-3.3375475, 1.0] reward: -1.0\n",
      "\t # 94: action: [-0.2551396, 1.0] reward: -1.0\n",
      "\t # 95: action: [2.0660715, 1.0] reward: -1.0\n",
      "\t # 96: action: [1.5170231, 1.0] reward: -1.0\n",
      "\t # 97: action: [-0.42838055, 1.0] reward: -1.0\n",
      "\t # 98: action: [-3.4824433, 1.0] reward: -1.0\n",
      "\t # 99: action: [2.2581735, 1.0] reward: -1.0\n",
      "\t # 100: action: [1.7963758, 1.0] reward: -1.0\n",
      "\t # 101: action: [-2.565284, 1.0] reward: -1.0\n",
      "\t # 102: action: [1.1454039, 1.0] reward: -6.0\n",
      "\t # 103: action: [1.6836244, 1.0] reward: -1.0\n",
      "\t # 104: action: [0.21251327, 1.0] reward: -1.0\n",
      "\t # 105: action: [-0.03812027, 1.0] reward: -1.0\n",
      "\t # 106: action: [1.6782832, 1.0] reward: -1.0\n",
      "\t # 107: action: [-2.2468905, 1.0] reward: -1.0\n",
      "\t # 108: action: [-1.11642, 1.0] reward: -1.0\n",
      "\t # 109: action: [2.6495216, 1.0] reward: -1.0\n",
      "\t # 110: action: [-3.8065057, 1.0] reward: -1.0\n",
      "\t # 111: action: [-3.127276, 1.0] reward: -1.0\n",
      "\t # 112: action: [0.8381706, 1.0] reward: -1.0\n",
      "\t # 113: action: [3.1986477, 1.0] reward: -1.0\n",
      "\t # 114: action: [-1.22196, 1.0] reward: -1.0\n",
      "\t # 115: action: [-0.2291143, 1.0] reward: -1.0\n",
      "\t # 116: action: [1.5685976, 1.0] reward: -6.0\n",
      "\t # 117: action: [2.7827895, 1.0] reward: -1.0\n",
      "\t # 118: action: [-3.237234, 1.0] reward: -1.0\n",
      "\t # 119: action: [3.421824, 1.0] reward: -1.0\n",
      "\t # 120: action: [-1.7257798, 1.0] reward: -1.0\n",
      "\t # 121: action: [-2.1633105, 1.0] reward: -1.0\n",
      "\t # 122: action: [-1.1988277, 1.0] reward: -1.0\n",
      "\t # 123: action: [2.7757227, 1.0] reward: -1.0\n",
      "\t # 124: action: [1.6916232, 1.0] reward: -1.0\n",
      "\t # 125: action: [-3.0912275, 1.0] reward: -1.0\n",
      "\t # 126: action: [0.2396338, 1.0] reward: -1.0\n",
      "\t # 127: action: [1.798991, 1.0] reward: -1.0\n",
      "\t # 128: action: [-0.694013, 1.0] reward: -1.0\n",
      "\t # 129: action: [-0.77462673, 1.0] reward: -1.0\n",
      "\t # 130: action: [1.9773273, 1.0] reward: -6.0\n",
      "\t # 131: action: [-0.22926867, 1.0] reward: -1.0\n",
      "\t # 132: action: [2.941286, 1.0] reward: -1.0\n",
      "\t # 133: action: [-3.4757013, 1.0] reward: -1.0\n",
      "\t # 134: action: [-2.872619, 1.0] reward: -1.0\n",
      "\t # 135: action: [3.186609, 1.0] reward: -1.0\n",
      "\t # 136: action: [2.6928992, 1.0] reward: -1.0\n",
      "\t # 137: action: [-0.015755653, 1.0] reward: -1.0\n",
      "\t # 138: action: [-2.3808112, 1.0] reward: -1.0\n",
      "\t # 139: action: [1.6215382, 1.0] reward: -1.0\n",
      "\t # 140: action: [-2.6416192, 1.0] reward: -1.0\n",
      "\t # 141: action: [-1.3449969, 1.0] reward: -1.0\n",
      "\t # 142: action: [1.9163835, 1.0] reward: -1.0\n",
      "\t # 143: action: [-3.6460266, 1.0] reward: -1.0\n",
      "\t # 144: action: [3.2470279, 1.0] reward: -6.0\n",
      "\t # 145: action: [0.61023796, 1.0] reward: -1.0\n",
      "\t # 146: action: [0.018670797, 1.0] reward: -1.0\n",
      "\t # 147: action: [3.9757261, 1.0] reward: -1.0\n",
      "\t # 148: action: [-1.3893181, 1.0] reward: -1.0\n",
      "\t # 149: action: [-2.6633427, 1.0] reward: -1.0\n",
      "\t # 150: action: [-1.1386466, 1.0] reward: -1.0\n",
      "\t # 151: action: [2.7624402, 1.0] reward: -1.0\n",
      "\t # 152: action: [0.19079244, 1.0] reward: -1.0\n",
      "\t # 153: action: [-0.0046076775, 1.0] reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t # 154: action: [-0.9140035, 1.0] reward: -1.0\n",
      "\t # 155: action: [-1.5667979, 1.0] reward: -1.0\n",
      "\t # 156: action: [3.1573043, 1.0] reward: -1.0\n",
      "\t # 157: action: [-4.9273686, 1.0] reward: -1.0\n",
      "\t # 158: action: [-0.28316385, 1.0] reward: -6.0\n",
      "\t # 159: action: [3.2746685, 1.0] reward: -1.0\n",
      "\t # 160: action: [1.6403807, 1.0] reward: -1.0\n",
      "\t # 161: action: [-4.950821, 1.0] reward: -1.0\n",
      "\t # 162: action: [4.502384, 1.0] reward: -1.0\n",
      "\t # 163: action: [-3.6167405, 1.0] reward: -1.0\n",
      "\t # 164: action: [1.645468, 1.0] reward: -1.0\n",
      "\t # 165: action: [-2.5260115, 1.0] reward: -1.0\n",
      "\t # 166: action: [3.0785115, 1.0] reward: -1.0\n",
      "\t # 167: action: [1.0291239, 1.0] reward: -1.0\n",
      "\t # 168: action: [-1.9652627, 1.0] reward: -1.0\n",
      "\t # 169: action: [-0.8623738, 1.0] reward: -1.0\n",
      "\t # 170: action: [0.8044554, 1.0] reward: -1.0\n",
      "\t # 171: action: [0.082751036, 1.0] reward: -1.0\n",
      "\t # 172: action: [-1.0942974, 1.0] reward: -6.0\n",
      "\t # 173: action: [-0.17013168, 1.0] reward: -1.0\n",
      "\t # 174: action: [2.0569513, 1.0] reward: -1.0\n",
      "\t # 175: action: [2.8615172, 1.0] reward: -1.0\n",
      "\t # 176: action: [-2.1693919, 1.0] reward: -1.0\n",
      "\t # 177: action: [1.3799572, 1.0] reward: -1.0\n",
      "\t # 178: action: [-3.9459474, 1.0] reward: -1.0\n",
      "\t # 179: action: [0.9309822, 1.0] reward: -1.0\n",
      "\t # 180: action: [0.41020417, 1.0] reward: -1.0\n",
      "\t # 181: action: [-2.2780318, 1.0] reward: -1.0\n",
      "\t # 182: action: [-0.16360486, 1.0] reward: -1.0\n",
      "\t # 183: action: [2.8425927, 1.0] reward: -1.0\n",
      "\t # 184: action: [-2.3806288, 1.0] reward: -1.0\n",
      "\t # 185: action: [1.261987, 1.0] reward: -1.0\n",
      "\t # 186: action: [-0.86438835, 1.0] reward: -6.0\n",
      "\t # 187: action: [2.9268885, 1.0] reward: -1.0\n",
      "\t # 188: action: [-0.5369315, 1.0] reward: -1.0\n",
      "\t # 189: action: [0.76935893, 1.0] reward: -1.0\n",
      "\t # 190: action: [-1.8535917, 1.0] reward: -1.0\n",
      "\t # 191: action: [1.4496865, 1.0] reward: -1.0\n",
      "\t # 192: action: [-0.9259405, 1.0] reward: -1.0\n",
      "\t # 193: action: [-2.4768093, 1.0] reward: -1.0\n",
      "\t # 194: action: [-1.0644956, 1.0] reward: -1.0\n",
      "\t # 195: action: [1.5725539, 1.0] reward: -1.0\n",
      "\t # 196: action: [1.1478516, 1.0] reward: -1.0\n",
      "\t # 197: action: [2.5078797, 1.0] reward: -1.0\n",
      "\t # 198: action: [-2.4546623, 1.0] reward: -1.0\n",
      "\t # 199: action: [-0.0071833134, 1.0] reward: -1.0\n",
      "\t # 200: action: [-0.75829005, 1.0] reward: -6.0\n",
      "\t # 201: action: [2.6353006, 1.0] reward: -1.0\n",
      "\t # 202: action: [1.9894924, 1.0] reward: -1.0\n",
      "\t # 203: action: [-1.0629786, 1.0] reward: -1.0\n",
      "\t # 204: action: [-1.366299, 1.0] reward: -1.0\n",
      "\t # 205: action: [0.93677765, 1.0] reward: -1.0\n",
      "\t # 206: action: [-2.6606696, 1.0] reward: -1.0\n",
      "\t # 207: action: [1.9360983, 1.0] reward: -1.0\n",
      "\t # 208: action: [0.28516364, 1.0] reward: -1.0\n",
      "\t # 209: action: [-1.3415948, 1.0] reward: -1.0\n",
      "\t # 210: action: [-1.6823691, 1.0] reward: -1.0\n",
      "\t # 211: action: [-0.42046595, 1.0] reward: -1.0\n",
      "\t # 212: action: [-0.37007186, 1.0] reward: -1.0\n",
      "\t # 213: action: [1.4541541, 1.0] reward: -1.0\n",
      "\t # 214: action: [-1.7541339, 1.0] reward: -6.0\n",
      "\t # 215: action: [-0.7222328, 1.0] reward: -1.0\n",
      "\t # 216: action: [3.6167507, 1.0] reward: -1.0\n",
      "\t # 217: action: [-1.3990448, 1.0] reward: -1.0\n",
      "\t # 218: action: [1.1059065, 1.0] reward: -1.0\n",
      "\t # 219: action: [-0.4278556, 1.0] reward: -1.0\n",
      "\t # 220: action: [0.74133337, 1.0] reward: -1.0\n",
      "\t # 221: action: [-1.9448959, 1.0] reward: -1.0\n",
      "\t # 222: action: [-2.331973, 1.0] reward: -1.0\n",
      "\t # 223: action: [3.3558197, 1.0] reward: -1.0\n",
      "\t # 224: action: [-0.2882588, 1.0] reward: -1.0\n",
      "\t # 225: action: [-2.4927647, 1.0] reward: -1.0\n",
      "\t # 226: action: [3.588843, 1.0] reward: -1.0\n",
      "\t # 227: action: [-1.6970358, 1.0] reward: -1.0\n",
      "\t # 228: action: [-0.9520842, 1.0] reward: -6.0\n",
      "\t # 229: action: [0.96174765, 1.0] reward: -1.0\n",
      "\t # 230: action: [1.019681, 1.0] reward: -1.0\n",
      "\t # 231: action: [2.5692086, 1.0] reward: -1.0\n",
      "\t # 232: action: [-3.723804, 1.0] reward: -1.0\n",
      "\t # 233: action: [1.0017107, 1.0] reward: -1.0\n",
      "\t # 234: action: [1.9293218, 1.0] reward: -1.0\n",
      "\t # 235: action: [-0.7646759, 1.0] reward: -1.0\n",
      "\t # 236: action: [-1.2007337, 1.0] reward: -1.0\n",
      "\t # 237: action: [-0.25456166, 1.0] reward: -1.0\n",
      "\t # 238: action: [1.0633297, 1.0] reward: -1.0\n",
      "\t # 239: action: [-0.27414936, 1.0] reward: -1.0\n",
      "\t # 240: action: [-0.144526, 1.0] reward: -1.0\n",
      "\t # 241: action: [0.06686379, 1.0] reward: -1.0\n",
      "\t # 242: action: [-0.67901856, 1.0] reward: -6.0\n",
      "\t # 243: action: [0.43909836, 1.0] reward: -1.0\n",
      "\t # 244: action: [1.8194182, 1.0] reward: -1.0\n",
      "\t # 245: action: [0.30311182, 1.0] reward: -1.0\n",
      "\t # 246: action: [-0.21449447, 1.0] reward: -1.0\n",
      "\t # 247: action: [0.25215617, 1.0] reward: -1.0\n",
      "\t # 248: action: [-1.5438936, 1.0] reward: -1.0\n",
      "\t # 249: action: [0.95876616, 1.0] reward: -1.0\n",
      "\t # 250: action: [-0.34485984, 1.0] reward: -1.0\n",
      "\t # 251: action: [3.493481, 1.0] reward: -1.0\n",
      "\t # 252: action: [-1.4859195, 1.0] reward: -1.0\n",
      "\t # 253: action: [-4.4789658, 1.0] reward: -1.0\n",
      "\t # 254: action: [-0.98702353, 1.0] reward: -1.0\n",
      "\t # 255: action: [-0.84281814, 1.0] reward: -1.0\n",
      "\t # 256: action: [1.8761655, 1.0] reward: -6.0\n",
      "\t # 257: action: [0.6747178, 1.0] reward: -1.0\n",
      "\t # 258: action: [1.2857052, 1.0] reward: -1.0\n",
      "\t # 259: action: [0.48827767, 1.0] reward: -1.0\n",
      "\t # 260: action: [3.4620764, 1.0] reward: -1.0\n",
      "\t # 261: action: [-2.8486052, 1.0] reward: -1.0\n",
      "\t # 262: action: [-2.0275264, 1.0] reward: -1.0\n",
      "\t # 263: action: [0.08076799, 1.0] reward: -1.0\n",
      "\t # 264: action: [0.29005092, 1.0] reward: -1.0\n",
      "\t # 265: action: [3.362997, 1.0] reward: -1.0\n",
      "\t # 266: action: [-4.668529, 1.0] reward: -1.0\n",
      "\t # 267: action: [-2.5418262, 1.0] reward: -1.0\n",
      "\t # 268: action: [5.075762, 1.0] reward: -1.0\n",
      "\t # 269: action: [0.6835896, 1.0] reward: -1.0\n",
      "\t # 270: action: [-0.72143936, 1.0] reward: -6.0\n",
      "\t # 271: action: [2.312488, 1.0] reward: -1.0\n",
      "\t # 272: action: [-4.0830736, 1.0] reward: -1.0\n",
      "\t # 273: action: [1.7300853, 1.0] reward: -1.0\n",
      "\t # 274: action: [-0.95896935, 1.0] reward: -1.0\n",
      "\t # 275: action: [1.2315605, 1.0] reward: -1.0\n",
      "\t # 276: action: [0.014687121, 1.0] reward: -1.0\n",
      "\t # 277: action: [0.77363485, 1.0] reward: -1.0\n",
      "\t # 278: action: [-2.395423, 1.0] reward: -1.0\n",
      "\t # 279: action: [0.4622004, 1.0] reward: -1.0\n",
      "\t # 280: action: [0.7446264, 1.0] reward: -1.0\n",
      "\t # 281: action: [-2.467983, 1.0] reward: -1.0\n",
      "\t # 282: action: [0.71584964, 1.0] reward: -1.0\n",
      "\t # 283: action: [0.14328322, 1.0] reward: -1.0\n",
      "\t # 284: action: [-2.1755939, 1.0] reward: -6.0\n",
      "\t # 285: action: [-0.9737289, 1.0] reward: -1.0\n",
      "\t # 286: action: [4.2504497, 1.0] reward: -1.0\n",
      "\t # 287: action: [1.4091332, 1.0] reward: -1.0\n",
      "\t # 288: action: [0.66951656, 1.0] reward: -1.0\n",
      "\t # 289: action: [-4.7360134, 1.0] reward: -1.0\n",
      "\t # 290: action: [1.9983214, 1.0] reward: -1.0\n",
      "\t # 291: action: [1.3860058, 1.0] reward: -1.0\n",
      "\t # 292: action: [2.0659113, 1.0] reward: -1.0\n",
      "\t # 293: action: [-2.9986525, 1.0] reward: -1.0\n",
      "\t # 294: action: [1.4111398, 1.0] reward: -1.0\n",
      "\t # 295: action: [-0.5386269, 1.0] reward: -1.0\n",
      "\t # 296: action: [1.5391154, 1.0] reward: -1.0\n",
      "\t # 297: action: [-1.959337, 1.0] reward: -1.0\n",
      "\t # 298: action: [-3.3408542, 1.0] reward: -6.0\n",
      "\t # 299: action: [1.6253676, 1.0] reward: -1.0\n",
      "----------TEST at 0 step-------------\n",
      "REWARD -410.0\n"
     ]
    }
   ],
   "source": [
    "# -- main ここからメイン関数です------------------------------\n",
    "# global変数の定義と、セッションの開始です\n",
    "frames = 0              # 全スレッドで共有して使用する総ステップ数\n",
    "SESS = tf.Session()     # TensorFlowのセッション開始\n",
    "\n",
    "# M1.スレッドを作成します\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    parameter_server = ParameterServer()    # 全スレッドで共有するパラメータを持つエンティティです\n",
    "    parameter_server.load_model()\n",
    "    \n",
    "test_env = Environment(\"test_env\", parameter_server)\n",
    "\n",
    "# TensorFlowでマルチスレッドを実行します\n",
    "SESS.run(tf.global_variables_initializer())     # TensorFlowを使う場合、最初に変数初期化をして、実行します\n",
    "\n",
    "test_env.run()\n",
    "test_env.game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
