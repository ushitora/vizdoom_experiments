{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import skimage.color, skimage.transform\n",
    "from vizdoom import *\n",
    "import os, time, random, threading, h5py, math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from game_instance import GameInstanceBasic, GameInstance\n",
    "from global_constants import *\n",
    "from datetime import datetime, timedelta\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from replay_memory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = \"./logs/log_test/\"\n",
    "MODEL_PATH =  \"./models/model_test/model.ckpt\"\n",
    "CONFIG_FILE_PATH = \"./config/simpler_basic.cfg\"\n",
    "WEIGHTS_PATH = [\"./weights_merged/conv1_kernel_expand.npy\", \"./weights_merged/conv1_bias.npy\", \"./weights_merged/conv2_kernel_expand.npy\", \"./weights_merged/conv2_bias.npy\"]\n",
    "DEMO_PATH = [\"./demonstration/basic/demodata_basic_01.hdf5\"]\n",
    "__name__ = \"learning\"\n",
    "N_ACTION = 3\n",
    "N_AGENT_ACTION = N_ACTION\n",
    "BOTS_NUM = 10\n",
    "N_WORKERS = 30\n",
    "REWARDS = {'living':-0.01, 'health_loss':-1, 'medkit':50, 'ammo':0.0, 'frag':500, 'dist':3e-1, 'suicide':-500} \n",
    "LSTM_SIZE = 1024\n",
    "N_ADV = 5\n",
    "N_IMG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(LOGDIR):\n",
    "    os.remove(os.path.join(LOGDIR, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,sess,  name, game_instance, network, agent, start_time, end_time):\n",
    "#     def __init__(self,sess,  name, start_time, end_time, parameter_server):\n",
    "        self.name = name\n",
    "        self.sess = sess\n",
    "        self.game = game_instance\n",
    "        self.network = network\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.progress = 0.0\n",
    "        self.log_server = None\n",
    "        \n",
    "        self.replay_memory = None\n",
    "        \n",
    "        self.step = 0\n",
    "        self.model_gen_count = 0\n",
    "        print(self.name,\" initialized...\")\n",
    "        \n",
    "    def run_learning(self, coordinator):\n",
    "        print(self.name + \" start learning\")\n",
    "        \n",
    "        self.game.new_episode(BOTS_NUM)\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                self.learning_step()\n",
    "                self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "#                     coordinator.request_stop()\n",
    "        except Exception as e:\n",
    "            coordinator.request_stop(e)\n",
    "        return 0\n",
    "    \n",
    "    def run_prelearning(self, coordinator):\n",
    "        assert self.replay_memory is not None\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                l_p, l_v, l_c = self.prelearning_step()\n",
    "                self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            coordinator.request_stop(e)\n",
    "        return 0\n",
    "    \n",
    "    def run_test(self, coordinator):\n",
    "        \n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                reward,frag, death,_ = self.test_agent()\n",
    "                print(\"----------TEST at\",(datetime.now()), \"---------\")\n",
    "                print(\"FRAG:\",frag, \"DEATH:\",death)\n",
    "                print(\"REWARD\",reward)\n",
    "\n",
    "                if self.log_server is not None:\n",
    "                    self.log_server.write_score(self.sess,self.step,  reward, frag, death)\n",
    "                    if self.progress >= self.model_gen_count/12:\n",
    "                        self.model_gen_count += 1\n",
    "                        self.log_server.save_model(sess=self.sess, model_path=MODEL_PATH, step=self.model_gen_count+1)\n",
    "                    \n",
    "                self.step += 1\n",
    "                time.sleep(60)\n",
    "                self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "#                     coordinator.request_stop()\n",
    "        except Exception as e:\n",
    "            coordinator.request_stop(e)\n",
    "            \n",
    "    def learning_step(self):\n",
    "        l_p = 0\n",
    "        l_v = 0\n",
    "        self.network.pull_parameter_server_all(self.sess)\n",
    "        if not self.game.is_episode_finished() and self.game.get_screen_buff() is not None:\n",
    "\n",
    "            s1_ = self.preprocess(self.game.get_screen_buff())\n",
    "            agent_action_idx = self.agent.act_eps_greedy(self.sess, s1_, self.progress)\n",
    "            engin_action = self.convert_action_agent2engine(agent_action_idx)\n",
    "            r,r_detail = self.game.make_action(engin_action , FRAME_REPEAT)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "\n",
    "            if r <= 0:\n",
    "                self.agent.push_batch(s1_, agent_action_idx, r, isterminal)\n",
    "            else:\n",
    "                self.agent.push_batch(s1_, agent_action_idx, r, True)\n",
    "                self.agent.train_network(self.sess)\n",
    "                self.game.new_episode(BOTS_NUM)\n",
    "                self.agent.clear_batch()\n",
    "                self.agent.clear_obs()\n",
    "                return l_p, l_v\n",
    "\n",
    "            if self.agent.is_trainable():\n",
    "                l_p, l_v = self.agent.train_network(self.sess)\n",
    "                if self.log_server is not None:\n",
    "                    if self.step % 10 == 0:\n",
    "                        self.log_server.write_loss(self.sess,self.step, l_p[0], l_v[0])\n",
    "                        self.log_server.write_weights(self.sess, self.step)\n",
    "            \n",
    "            self.step += 1\n",
    "                \n",
    "        else:\n",
    "            self.game.new_episode(BOTS_NUM)\n",
    "            self.agent.train_network(self.sess)\n",
    "            self.agent.clear_batch()\n",
    "            self.agent.clear_obs()\n",
    "        return l_p, l_v\n",
    "    \n",
    "    def prelearning_step(self):\n",
    "        self.network.pull_parameter_server_all(self.sess)\n",
    "        states, actions, rewards, isterminals = self.replay_memory.sample_uniform(3)\n",
    "        value = np.reshape(self.network.get_value(self.sess, states), [-1])\n",
    "#         rewards = rewards + value*(np.ones_like(isterminals) - isterminals)\n",
    "        rewards = rewards + value*(~isterminals)\n",
    "        print(rewards)\n",
    "        print(isterminals)\n",
    "        print(actions)\n",
    "        l_p, l_v, l_c = self.network.update_parameter_server_imitation(self.sess,states, actions, rewards, isterminals)\n",
    "        if self.log_server is not None:\n",
    "            if self.step % 10 == 0:\n",
    "                self.log_server.write_loss(self.sess, self.step, l_p[0], l_v[0], l_c[0])\n",
    "                self.log_server.write_weights(self.sess, self.step)\n",
    "        self.step += 1\n",
    "        return l_p, l_v, l_c\n",
    "            \n",
    "    def test_agent(self, gif_buff=None, reward_buff=None):\n",
    "        \n",
    "        self.game.new_episode(BOTS_NUM)\n",
    "        \n",
    "#         Copy params from global\n",
    "        self.network.pull_parameter_server_all(self.sess)\n",
    "\n",
    "        step = 0\n",
    "        gif_img = []\n",
    "        total_reward = 0\n",
    "        total_detail = {}\n",
    "        while not self.game.is_episode_finished():\n",
    "            s1_row = self.game.get_screen_buff()\n",
    "            s1 = self.preprocess(s1_row)\n",
    "            if gif_buff is not None:\n",
    "                gif_img.append(s1_row.transpose(1,2,0))\n",
    "            action = self.agent.act_greedy(self.sess,s1)\n",
    "            engine_action = self.convert_action_agent2engine(action)\n",
    "            reward,reward_detail = self.game.make_action(engine_action,FRAME_REPEAT)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            total_reward += reward\n",
    "            for k in reward_detail.keys():\n",
    "                if not k in total_detail.keys():\n",
    "                    total_detail[k] = reward_detail[k]\n",
    "                else:\n",
    "                    total_detail[k] += reward_detail[k]\n",
    "            step += 1\n",
    "            if reward_buff is not None:\n",
    "                reward_buff.append((engine_action, reward))\n",
    "            \n",
    "            if (self.game.is_player_dead()):\n",
    "                self.game.respawn_player()\n",
    "        \n",
    "        save_img = []\n",
    "        if gif_buff is not None:\n",
    "            for i in range(len(gif_img)):\n",
    "                save_img.append(Image.fromarray(np.uint8(gif_img[i])))\n",
    "            gif_buff += save_img\n",
    "        return total_reward, self.game.get_frag_count(), self.game.get_death_count(), gif_img\n",
    "        \n",
    "    def convert_action_engine2agent(self,engine_action):\n",
    "#         return engine_action.index(1)\n",
    "        assert type(engine_action) == type(list()), print(\"type: \", type(engine_action))\n",
    "        ans = 0\n",
    "        for i, e_a in enumerate(engine_action):\n",
    "            ans += e_a * 2**i\n",
    "        return ans\n",
    "    \n",
    "    def convert_action_agent2engine(self,agent_action):\n",
    "#         ans = [0 for i in range(3)]\n",
    "#         ans[agent_action] = 1\n",
    "#         return ans\n",
    "        assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print(\"type(agent_action)=\",type(agent_action))\n",
    "        ans = []\n",
    "        for i in range(6):\n",
    "            ans.append(agent_action%2)\n",
    "            agent_action = int(agent_action / 2)\n",
    "        return ans\n",
    "    \n",
    "    def add_buff(self, s1,a,r,isterminal):\n",
    "        self.obs[0][self.buff_pointer] = s1\n",
    "        self.obs[1][self.buff_pointer] = a\n",
    "        self.obs[2][self.buff_pointer] = r\n",
    "        self.obs[3][self.buff_pointer] = isterminal\n",
    "        self.buff_pointer += 1\n",
    "        return 0\n",
    "    \n",
    "    def clear_buff(self):\n",
    "        self.obs = [np.zeros(shape=(self.n_adv,) + RESOLUTION, dtype=np.float32), \\\n",
    "                         np.zeros(shape=(self.n_adv,), dtype=np.int8), \\\n",
    "                         np.zeros(shape=(self.n_adv,), dtype=np.float32), \\\n",
    "                         np.ones(shape=(self.n_adv,), dtype=np.int8)]\n",
    "        self.buff_pointer = 0\n",
    "        return 0\n",
    "    \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3 and img.shape[0]==3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION, mode=\"constant\")\n",
    "        img = img.astype(np.float32)\n",
    "#         img = (img)/255.0\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterServer:\n",
    "    def __init__(self, sess, log_dir):\n",
    "        self.sess = sess\n",
    "        with tf.variable_scope(\"parameter_server\", reuse=tf.AUTO_REUSE):\n",
    "            self.state1_ = tf.placeholder(tf.float32, shape=(None,N_ADV,) + RESOLUTION)\n",
    "            self._build_model(self.state1_)\n",
    "\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "        self.weights_params_fc = self.weights_params[4:]\n",
    "#         self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "            \n",
    "        with tf.variable_scope(\"summary\", reuse=tf.AUTO_REUSE):\n",
    "            self._build_summary(sess,log_dir)\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep = 20)\n",
    "        \n",
    "#         print(\"-------GLOBAL-------\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "\n",
    "    def _build_model(self,state):\n",
    "            self.conv1 = NetworkSetting.conv1(state)\n",
    "            self.maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "            self.conv2 = NetworkSetting.conv2(self.maxpool1)\n",
    "            self.maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "            reshape = NetworkSetting.reshape(self.maxpool2)\n",
    "            rnn ,self.rnn_len ,self.row_output = NetworkSetting.lstm(reshape, state)\n",
    "            fc1 = NetworkSetting.fc1(rnn)\n",
    "\n",
    "            with tf.variable_scope(\"policy\"):\n",
    "                self.policy = NetworkSetting.policy(fc1)\n",
    "            \n",
    "            with tf.variable_scope(\"value\"):\n",
    "                self.value = NetworkSetting.value(fc1)\n",
    "                \n",
    "            self.policy_prob = tf.nn.softmax(self.policy)\n",
    "                \n",
    "            print(\"---------MODEL SHAPE-------------\")\n",
    "            print(state.get_shape())\n",
    "            print(self.conv1.get_shape())\n",
    "            print(self.conv2.get_shape())\n",
    "            print(reshape.get_shape())\n",
    "            print(fc1.get_shape())\n",
    "            print(self.policy.get_shape())\n",
    "            print(self.value.get_shape())\n",
    "                \n",
    "    def _build_summary(self,sess, log_dir):\n",
    "        \n",
    "        self.reward_ = tf.placeholder(tf.float32,shape=(), name=\"reward\")\n",
    "        self.frag_ = tf.placeholder(tf.float32, shape=(), name=\"frag\")\n",
    "        self.death_ = tf.placeholder(tf.float32, shape=(), name=\"death\")\n",
    "        self.loss_p_ = tf.placeholder(tf.float32, shape=(), name=\"loss_policy\")\n",
    "        self.loss_v_ = tf.placeholder(tf.float32, shape=(), name=\"loss_value\")\n",
    "        self.loss_c_ = tf.placeholder(tf.float32, shape=(), name=\"loss_class\")\n",
    "        \n",
    "        with tf.variable_scope(\"Summary_Score\"):\n",
    "            s = [tf.summary.scalar('reward', self.reward_, family=\"score\"), tf.summary.scalar('frag', self.frag_, family=\"score\"), tf.summary.scalar(\"death\", self.death_, family=\"score\")]\n",
    "            self.summary_reward = tf.summary.merge(s)\n",
    "        \n",
    "        with tf.variable_scope(\"Summary_Loss\"):\n",
    "            self.summary_loss = tf.summary.merge([tf.summary.scalar('loss_policy', self.loss_p_, family=\"loss\"), tf.summary.scalar('loss_value', self.loss_v_, family=\"loss\"), tf.summary.scalar('loss_class', self.loss_c_, family=\"loss\")])\n",
    "        \n",
    "        with tf.variable_scope(\"Summary_Images\"):\n",
    "            conv1_display = tf.reshape(tf.transpose(self.conv1, [0,1,4,2,3]), (-1, self.conv1.get_shape()[1],self.conv1.get_shape()[2]))\n",
    "            conv2_display = tf.reshape(tf.transpose(self.conv2, [0,1,4,2,3]), (-1, self.conv2.get_shape()[1],self.conv2.get_shape()[2]))\n",
    "            conv1_display = tf.expand_dims(conv1_display, -1)\n",
    "            conv2_display = tf.expand_dims(conv2_display, -1)\n",
    "\n",
    "            state_shape = self.state1_.get_shape()\n",
    "            conv1_shape = conv1_display.get_shape()\n",
    "            conv2_shape = conv2_display.get_shape()\n",
    "\n",
    "            s_img = []\n",
    "            s_img.append(tf.summary.image('state',tf.reshape(self.state1_,[-1, state_shape[2], state_shape[3], state_shape[4]]), 1, family=\"state1\"))\n",
    "            s_img.append(tf.summary.image('conv1',tf.reshape(self.conv1,[-1, conv1_shape[1], conv1_shape[2], 1]), family=\"conv1\"))\n",
    "            s_img.append(tf.summary.image('conv2',tf.reshape(self.conv2,[-1, conv2_shape[1], conv2_shape[2], 1]), family=\"conv2\"))\n",
    "\n",
    "            self.summary_image = tf.summary.merge(s_img)\n",
    "            \n",
    "        with tf.variable_scope(\"Summary_Weights\"):\n",
    "            s = [tf.summary.histogram(values=w, name=w.name, family=\"weights\") for w in self.weights_params]\n",
    "            self.summary_weights = tf.summary.merge(s)\n",
    "\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "        \n",
    "    def write_graph(self, sess):\n",
    "        self.writer.add_graph(sess.graph)\n",
    "        \n",
    "    def write_score(self,sess, step ,reward, frag, death):\n",
    "        m = sess.run(self.summary_reward, feed_dict={self.reward_:reward, self.frag_:frag, self.death_:death})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_loss(self,sess, step, l_p, l_v,l_c):\n",
    "        m = sess.run(self.summary_loss, feed_dict={self.loss_p_: l_p, self.loss_v_:l_v, self.loss_c_:l_c})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_img(self,sess, step, state):\n",
    "        m = sess.run(self.summary_image, feed_dict={self.state1_: state})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_weights(self, sess, step):\n",
    "        m = sess.run(self.summary_weights)\n",
    "        return self.writer.add_summary(m, step)\n",
    "        \n",
    "    def load_model(self, sess, model_path, step):\n",
    "        self.saver.restore(sess, model_path+'-'+str(step))\n",
    "    \n",
    "    def save_model(self, sess,  model_path, step):\n",
    "        self.saver.save(sess, model_path, global_step = step)\n",
    "        \n",
    "    def load_cnnweights(self, sess, weights_path):\n",
    "        assert len(weights_path) == 4\n",
    "        cnn_weights = self.weights_params[:4]\n",
    "        w_demo = [np.load(w_p) for w_p in weights_path]\n",
    "        plh = [tf.placeholder(tf.float32, shape=w.shape) for w in w_demo]\n",
    "        assign_op = [w.assign(p) for w, p in zip(cnn_weights, plh)]\n",
    "        feed_dict = {p:w for w,p in zip(w_demo, plh)}\n",
    "        sess.run(assign_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        \n",
    "        self.obs = {}\n",
    "        self.obs['s1'] = np.zeros((N_ADV, )+ RESOLUTION, dtype=np.float32)\n",
    "        \n",
    "        self.states_buff = np.zeros((N_ADV*3-2,) + RESOLUTION, dtype=np.float32)\n",
    "        self.rewards_buff = np.zeros((N_ADV*3-2,), dtype=np.float32)\n",
    "        self.actions_buff = np.zeros((N_ADV*3-2, ), dtype=np.float32)\n",
    "        self.isterminals_buff = np.ones((N_ADV*3-2, ), dtype=np.float32)\n",
    "        self.buff_pointer = 0\n",
    "        \n",
    "    def calc_eps(self, progress):\n",
    "        if progress < 0.2:\n",
    "            return EPS_MIN\n",
    "        elif progress >= 0.2 and progress < 0.8:\n",
    "            return ((EPS_MAX - EPS_MIN)/ 0.6) * progress + ( EPS_MIN -  (EPS_MAX - EPS_MIN)/ 0.6 * 0.2)\n",
    "        else :\n",
    "            return EPS_MAX\n",
    "\n",
    "    def act_eps_greedy(self, sess, s1, progress):\n",
    "        assert progress >= 0.0 and progress <=1.0\n",
    "        \n",
    "        self.push_obs(s1)\n",
    "        eps = self.calc_eps(progress)\n",
    "        if random.random() <= eps:\n",
    "            p = self.network.get_policy(sess, [self.obs['s1']])[0]\n",
    "            a_idx = np.random.choice(N_AGENT_ACTION, p=p)\n",
    "        else:\n",
    "            a_idx = np.random.randint(N_AGENT_ACTION)\n",
    "            \n",
    "        return a_idx\n",
    "    \n",
    "    def act_greedy(self, sess, s1):\n",
    "        p = self.network.get_policy(sess, [self.obs['s1']])[0]\n",
    "        a_idx = np.random.choice(N_AGENT_ACTION, p=p)\n",
    "        return a_idx\n",
    "    \n",
    "    def get_gradients(self, sess):\n",
    "        batch={'s1':[], 'actions':[], 'rewards':[], 'isterminals':[]}\n",
    "        for i in range(N_ADV):\n",
    "            batch['s1'].append(self.states_buff[i:i+N_ADV])\n",
    "            batch['actions'].append(self.actions_buff[i+N_ADV])\n",
    "            batch['isterminals'].append(self.isterminals_buff[i+N_ADV])\n",
    "            batch['rewards'].append(sum([r*GAMMA**j for j,r in enumerate(self.rewards_buff[i:i+N_ADV])]))\n",
    "            \n",
    "        return self.network.get_gradients(sess, batch['s1'], batch['actions'], batch['rewards'], batch['isterminals'])\n",
    "    \n",
    "    def train_network(self, sess):\n",
    "        batch={'s1':[], 'actions':[], 'rewards':[], 'isterminals':[]}\n",
    "        self.buff_pointer = N_ADV\n",
    "        for i in range(N_ADV):\n",
    "            batch['s1'].append(self.states_buff[i:i+N_ADV])\n",
    "            batch['actions'].append(self.actions_buff[i+N_ADV])\n",
    "            batch['isterminals'].append(self.isterminals_buff[i+N_ADV])\n",
    "            batch['rewards'].append(sum([r*GAMMA**j for j,r in enumerate(self.rewards_buff[i:i+N_ADV])]))\n",
    "            \n",
    "        self.states_buff = np.roll(self.states_buff, shift=-N_ADV, axis=0)\n",
    "        self.rewards_buff = np.roll(self.rewards_buff, shift=-N_ADV)\n",
    "        self.isterminals_buff = np.roll(self.isterminals_buff, shift=-N_ADV)\n",
    "        self.actions_buff = np.roll(self.actions_buff, shift=-N_ADV)\n",
    "        self.test_batch = batch\n",
    "        return self.network.update_parameter_server(sess, batch['s1'], batch['actions'], batch['rewards'], batch['isterminals'])\n",
    "    \n",
    "    def push_obs(self, s1):\n",
    "        self.obs['s1'] = np.roll(self.obs['s1'],shift=-1, axis=0)\n",
    "        self.obs['s1'][-1] = s1\n",
    "        \n",
    "    def clear_obs(self):\n",
    "        self.obs = {}\n",
    "        self.obs['s1'] = np.zeros((N_ADV,)+ RESOLUTION, dtype=np.float32)\n",
    "        \n",
    "    def push_batch(self, s1, action, reward, isterminal):\n",
    "        self.states_buff[self.buff_pointer] = s1\n",
    "        self.actions_buff[self.buff_pointer] = action\n",
    "        self.rewards_buff[self.buff_pointer] = reward\n",
    "        self.isterminals_buff[self.buff_pointer] = isterminal\n",
    "        self.buff_pointer += 1\n",
    "    \n",
    "    def clear_batch(self):\n",
    "        self.states_buff = np.zeros((N_ADV*3-2,) + RESOLUTION, dtype=np.float32)\n",
    "        self.rewards_buff = np.zeros((N_ADV*3-2,), dtype=np.float32)\n",
    "        self.actions_buff = np.zeros((N_ADV*3-2, ), dtype=np.float32)\n",
    "        self.isterminals_buff = np.ones((N_ADV*3-2, ), dtype=np.float32)\n",
    "        self.buff_pointer = 0\n",
    "    \n",
    "    def is_trainable(self):\n",
    "        if self.buff_pointer == len(self.states_buff):\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name, parameter_server):\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name+\"_learner\", reuse=tf.AUTO_REUSE):\n",
    "            self.state1_ = tf.placeholder(tf.float32,shape=(None,N_ADV)+RESOLUTION, name=\"state_1\")\n",
    "            self.a_ = tf.placeholder(tf.float32, shape=(None,), name=\"action\")\n",
    "            self.r_adv = tf.placeholder(tf.float32, shape=(None,), name=\"reward_advantage\")\n",
    "            self.isterminal_ = tf.placeholder(tf.float32, shape=(None,), name=\"isterminal\")\n",
    "            self.policy, self.value, self.conv1, self.conv2 = self._model(self.state1_)\n",
    "\n",
    "            self._build_graph()\n",
    "            \n",
    "#         self.update_global_weight_params = \\\n",
    "#             parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params_fc))\n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))\n",
    "\n",
    "    def _model(self,state):\n",
    "\n",
    "#         with tf.variable_scope(self.name + \"_nottrainable\"):\n",
    "        conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        \n",
    "        with tf.variable_scope(self.name + \"_trainable\"):\n",
    "            rnn ,l ,self.row_output = NetworkSetting.lstm(reshape, state)\n",
    "            fc1 = NetworkSetting.fc1(rnn)\n",
    "\n",
    "            policy = NetworkSetting.policy(fc1)\n",
    "            value = NetworkSetting.value(fc1)\n",
    "            \n",
    "        self.test_len = l\n",
    "        self.rnn = rnn\n",
    "        \n",
    "        return policy, value, conv1, conv2\n",
    "\n",
    "    def _build_graph(self):\n",
    "\n",
    "        one_hot = tf.one_hot(tf.cast(self.a_, tf.int32), depth=N_AGENT_ACTION)\n",
    "        \n",
    "        self.policy_prob = tf.nn.softmax(self.policy)\n",
    "        \n",
    "        log_prob = -tf.log(tf.reduce_sum(self.policy_prob * one_hot, axis=1)+1e-10)\n",
    "        advantage = self.r_adv - tf.reshape(self.value,[-1])\n",
    "        self.loss_policy = log_prob * tf.stop_gradient(advantage)\n",
    "        self.loss_value = tf.square(advantage) * 0.01\n",
    "        self.loss_class = tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot,logits=self.policy)*tf.stop_gradient(advantage)\n",
    "#         self.loss_total = tf.reduce_mean(self.loss_policy + self.loss_value)\n",
    "        self.loss_total = tf.reduce_mean(self.loss_policy + self.loss_value + self.loss_class)\n",
    "        \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"_learner\")\n",
    "        self.weights_params_fc = self.weights_params[4:]\n",
    "        \n",
    "#         self.grads = tf.gradients(self.loss_total ,self.weights_params_fc)\n",
    "        self.grads = tf.gradients(self.loss_total ,self.weights_params)\n",
    "        \n",
    "        self.pull_global_weight_params_fc = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params_fc,parameter_server.weights_params_fc)]\n",
    "        self.push_local_weight_params_fc = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params_fc,self.weights_params_fc)]\n",
    "        \n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params,parameter_server.weights_params)]\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params)]\n",
    "        \n",
    "    def pull_parameter_server(self, sess):\n",
    "        sess.run(self.pull_global_weight_params_fc)\n",
    "    \n",
    "    def push_parameter_server(self,sess):\n",
    "        sess.run(self.push_local_weight_params_fc)\n",
    "        \n",
    "    def pull_parameter_server_all(self, sess):\n",
    "        sess.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server_all(self):\n",
    "        sess.run(self.push_local_weight_params)\n",
    "        \n",
    "    def get_weights(self, sess):\n",
    "        return sess.run(self.weights_params)\n",
    "    \n",
    "    def get_gradients(self,sess, s1, a, r, isterminal):\n",
    "        assert np.ndim(s1) == 5\n",
    "        \n",
    "        feed_dict = {self.state1_: s1, self.a_:a, self.r_adv:r,  self.isterminal_:isterminal}\n",
    "        return sess.run(self.grads, feed_dict)\n",
    "    \n",
    "    def update_parameter_server(self,sess, s1, a, r, isterminal):\n",
    "        assert np.ndim(s1) == 5\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_adv:r}\n",
    "        _, l_p, l_v = sess.run([self.update_global_weight_params, self.loss_policy, self.loss_value],feed_dict)\n",
    "        return l_p, l_v\n",
    "    \n",
    "    def update_parameter_server_imitation(self,sess, s1, a, r, isterminal):\n",
    "        assert np.ndim(s1) == 5\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_adv:r}\n",
    "        _, l_p, l_v, l_c = sess.run([self.update_global_weight_params, self.loss_policy, self.loss_value, self.loss_class],feed_dict)\n",
    "        return l_p, l_v, l_c\n",
    "    \n",
    "    def check_weights(self, sess):\n",
    "        weights = SESS.run(self.weights_params)\n",
    "        assert np.isnan([np.mean(w) for w in weights]).any()==False , print(weights)\n",
    "\n",
    "    def get_policy(self, sess, s1):\n",
    "        assert np.ndim(s1) == 5\n",
    "        return sess.run(self.policy_prob, {self.state1_: s1})\n",
    "\n",
    "    def get_value(self, sess, s1):\n",
    "        assert np.ndim(s1) == 5        \n",
    "        return sess.run(self.value, {self.state1_:s1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [1,6,6]\n",
    "        stride = [1,3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.layers.conv3d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        filters=num_outputs,\\\n",
    "                                        strides=stride,padding=padding,activation=activation,\\\n",
    "                                        kernel_initializer=weights_init,\\\n",
    "                                        bias_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [1,3,3]\n",
    "        stride = [1,2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.layers.conv3d(pre_layer,kernel_size=kernel_size,filters=num_outputs,\\\n",
    "                                        strides=stride,padding=padding,activation=activation,\\\n",
    "                                        kernel_initializer=weights_init,bias_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        shape = pre_layer.get_shape()\n",
    "        return tf.reshape(pre_layer, shape=(-1,shape[1], shape[2]*shape[3]*shape[4]))\n",
    "    \n",
    "    def lstm(pre_layer, state):\n",
    "        batch_size = tf.shape(pre_layer)[0]\n",
    "        temp = tf.reduce_max(state, axis=4)\n",
    "        temp = tf.reduce_max(temp, axis=3)\n",
    "        temp = tf.reduce_max(temp, axis=2)\n",
    "        lengh = tf.cast(tf.reduce_sum(tf.sign(temp) , axis=1),dtype=tf.int32) \n",
    "        cell = tf.nn.rnn_cell.LSTMCell(LSTM_SIZE)\n",
    "        rnn_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        rnn_out, state_out = tf.nn.dynamic_rnn(cell, pre_layer, initial_state=rnn_state, dtype=tf.float32)\n",
    "        out_idx = tf.range(0, batch_size) * N_ADV + (lengh  -1)\n",
    "        output = tf.gather(tf.reshape(rnn_out, [-1, LSTM_SIZE]), out_idx)\n",
    "        return output, lengh, rnn_out\n",
    "    \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs =1024\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def policy(pre_layer):\n",
    "        num_outputs=N_AGENT_ACTION\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def value(pre_layer):\n",
    "        num_outputs = 1\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,weights_initializer=weights_init, biases_initializer=bias_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demo(replay_memory, demo_path):\n",
    "    for demo in demo_path[:1]:\n",
    "        print(demo)\n",
    "        file = h5py.File(demo, 'r')\n",
    "        episodes = list(file.keys())[1:]\n",
    "        total_n_transit = 0\n",
    "        for e in episodes:\n",
    "            states_row = file[e+\"/states\"][:]\n",
    "            action_row = file[e+\"/action\"][:]\n",
    "            \n",
    "            n_transit = len(states_row)\n",
    "            total_n_transit += n_transit\n",
    "            \n",
    "            rewards = np.zeros((n_transit+N_ADV,))\n",
    "            for i in range(n_transit):\n",
    "                if i==n_transit-1:\n",
    "                    rewards[i] = 100.0\n",
    "                else:\n",
    "                    rewards[i] = -5.\n",
    "            \n",
    "            actions = []\n",
    "            for a_r in action_row:\n",
    "                ans = np.where(a_r == 1)[0][0]\n",
    "                actions.append(ans)\n",
    "            actions = np.array(actions)\n",
    "            \n",
    "            rewards_adv = []\n",
    "            for i in range(n_transit):\n",
    "                r = rewards[i:i+N_ADV]\n",
    "                rewards_adv.append(r)\n",
    "            rewards_adv = np.array(rewards_adv)\n",
    "                \n",
    "            for i in range(n_transit):\n",
    "                s = np.zeros((N_ADV,)+RESOLUTION)\n",
    "                if i < N_ADV:\n",
    "                    s[:i+1] = states_row[:i+1]\n",
    "                else:\n",
    "                    s = states_row[i+1-N_ADV:i+1]\n",
    "                a = actions[i]\n",
    "                r = sum([r_*GAMMA**j for j,r_ in enumerate(rewards_adv[i])])\n",
    "                if i==n_transit-1:\n",
    "                    batch = [np.copy(s), a, r ,True]\n",
    "                else:\n",
    "                    batch = [np.copy(s), a, r, False]\n",
    "                replay_memory.store(batch)\n",
    "                \n",
    "    replay_memory.tree.set_parmanent_data(total_n_transit)\n",
    "    print(len(replay_memory), \"data are stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaymemory = ReplayMemory(10000)\n",
    "rewards = load_demo(replaymemory, DEMO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"learning\":\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        TEST_GRADS = []\n",
    "        parameter_server = ParameterServer(sess,LOGDIR)\n",
    "\n",
    "        starttime = datetime.now().timestamp()\n",
    "        end_time = (datetime.now() + timedelta(minutes=10)).timestamp()\n",
    "        \n",
    "        coordinator = tf.train.Coordinator()\n",
    "\n",
    "        name = \"worker_imitation\"\n",
    "        game_instance=GameInstanceBasic(DoomGame(), name=name,  config_file_path=CONFIG_FILE_PATH,n_adv=N_ADV)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network)\n",
    "        imitation_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time)\n",
    "        imitation_env.log_server = parameter_server\n",
    "        imitation_env.replay_memory = replaymemory\n",
    "        thread_imitation = threading.Thread(target=imitation_env.run_prelearning, args=(coordinator,))\n",
    "\n",
    "        name = \"test\"\n",
    "        game_instance=GameInstanceBasic(DoomGame(), name=name,config_file_path=CONFIG_FILE_PATH, n_adv=N_ADV)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network)\n",
    "        test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time)\n",
    "        test_env.log_server = parameter_server\n",
    "        thread_test = threading.Thread(target=test_env.run_test, args=(coordinator,))\n",
    "        \n",
    "        parameter_server.write_graph(sess)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         parameter_server.load_cnnweights(sess=sess, weights_path=WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        print(\"-----Start IMITATION LEARNING----\")\n",
    "        threads = [thread_imitation, thread_test]\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        coordinator.join(threads)\n",
    "\n",
    "        parameter_server.save_model(sess=sess, step=1, model_path=MODEL_PATH)\n",
    "\n",
    "        GIF_BUFF = []\n",
    "        REWARD_BUFF = []\n",
    "        r,f,d,imgs = test_env.test_agent(gif_buff=GIF_BUFF,reward_buff=REWARD_BUFF)\n",
    "        GIF_BUFF[0].save('gifs/test.gif',save_all=True, append_images=GIF_BUFF[1:], optimize=False, duration=40*4, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_server.save_model(sess=sess, step=4, model_path=\"./models/imitation_learn/model_181107/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIF_BUFF = []\n",
    "REWARD_BUFF = []\n",
    "r,f,d,imgs = test_env.test_agent(gif_buff=GIF_BUFF,reward_buff=REWARD_BUFF)\n",
    "GIF_BUFF[0].save('gifs/test.gif',save_all=True, append_images=GIF_BUFF[1:], optimize=False, duration=40*4, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"test\":\n",
    "    MODEL_PATH = \"./models/copy_params/model_a3c_rnn_copy_181029/model.ckpt\"\n",
    "#     MODEL_PATH = \"./models/copy_params/model_a3c_rnn_copy_181105/model.ckpt\"\n",
    "    TEST_GRADS = []\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    starttime = datetime.now().timestamp()\n",
    "    end_time = (datetime.now() + timedelta(minutes=15)).timestamp()\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        parameter_server = ParameterServer(sess,LOGDIR)\n",
    "        parameter_server.load_model(sess=sess, model_path=MODEL_PATH, step=2)\n",
    "        \n",
    "        name = \"test\"\n",
    "        game_instance=GameIn(DoomGame(), name=name, config_file_path=CONFIG_FILE_PATH,rewards=REWARDS, n_adv=N_ADV)\n",
    "        network = NetworkLocal(name, parameter_server)\n",
    "        agent = Agent(network)\n",
    "        test_env = Environment(sess = sess ,name=name, agent=agent, game_instance=game_instance, network=network, start_time=starttime, end_time=end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_temp = np.load(\"./test_imgs.npy\")\n",
    "test_img_temp = np.concatenate([test_img_temp, [np.zeros_like(test_img_temp[0])]])\n",
    "\n",
    "test_img = np.reshape(test_img_temp, (-1, 5)+RESOLUTION)\n",
    "\n",
    "for i in range(5):\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(1,1,1)\n",
    "    ax.imshow(test_img[10,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {parameter_server.state1_: test_img[:]}\n",
    "\n",
    "pol,val,c2 = sess.run([parameter_server.policy_prob,parameter_server.value, parameter_server.conv2], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = np.reshape(c2, (10,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(c2):\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(1,1,1)\n",
    "    ax.scatter(range(len(c)), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_policy = []\n",
    "loss_value = []\n",
    "loss_class = []\n",
    "for i in range(1000):\n",
    "    b = replaymemory.tree.data[i]\n",
    "    feed_dict = {test_env.network.state1_:[b[0]], test_env.network.a_:[b[1]], test_env.network.r_adv:[b[2]]}\n",
    "    l_p, l_v, l_c = sess.run([test_env.network.loss_policy, test_env.network.loss_value, test_env.network.loss_class], feed_dict)\n",
    "    loss_policy.append(l_p)\n",
    "    loss_value.append(l_v)\n",
    "    loss_class.append(l_c)\n",
    "loss_policy = np.reshape(np.array(loss_policy), [-1])\n",
    "loss_value = np.reshape(np.array(loss_value), [-1])\n",
    "loss_class = np.reshape(np.array(loss_class), [-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(loss_policy) > 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(loss_policy)), loss_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(loss_class)), loss_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(loss_value)), loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = replaymemory.tree.data[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(1,1,1)\n",
    "    ax.imshow(b[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(loss_policy)), loss_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pol:\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(1,1,1)\n",
    "    ax.bar(range(len(p)),p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"test_game_instance\":\n",
    "    environments[0].game.new_episode(BOTS_NUM)\n",
    "    pre_x = 0\n",
    "    pre_y = 0\n",
    "\n",
    "    print(environments[0].game.get_pos_x(),\"diff:\",environments[0].game.get_pos_x()-pre_x, \",\", environments[0].game.get_pos_y(),\"diff:\",environments[0].game.get_pos_y()-pre_y)\n",
    "    pre_x = environments[0].game.get_pos_x()\n",
    "    pre_y = environments[0].game.get_pos_y()\n",
    "    print(environments[0].game.make_action([0,0,0,1,0,1], FRAME_REPEAT))\n",
    "    # print(environments[0].game.game)\n",
    "    plt.imshow(environments[0].preprocess( environments[0].game.get_screen_buff()))\n",
    "\n",
    "    if(environments[0].game.is_player_dead()):\n",
    "        environments[0].game.respawn_player()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
