{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from __future__ import print_function\n",
    "import math, os\n",
    "import time,random,threading\n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "CONFIG_FILE_PATH = \"./config/custom_config.cfg\"\n",
    "MODEL_PATH = \"./model_v00/model_v00.ckpt\"\n",
    "RESOLUTION = (120,120,3)\n",
    "\n",
    "N_ADV = 5\n",
    "\n",
    "N_WORKERS = 3\n",
    "\n",
    "WORKER_STEPS = 10000\n",
    "\n",
    "N_ACTION = 6\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "BOTS_NUM = 5\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.0\n",
    "EPS_STEPS = WORKER_STEPS*N_WORKERS\n",
    "\n",
    "REWARDS = {'living':-0.01, 'health_loss':-1, 'medkit':50, 'ammo':0.0, 'frag':500, 'dist':3e-2, 'suicide':-500} \n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "RMSProbDecaly = 0.99\n",
    "\n",
    "MERGED_WEIGHTS_PATH=[\"./weights_merged/conv1_kernel.npy\", \"./weights_merged/conv1_bias.npy\", \"./weights_merged/conv2_kernel.npy\", \"./weights_merged/conv2_bias.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --スレッドになるクラスです　-------\n",
    "class Worker_thread:\n",
    "    # スレッドは学習環境environmentを持ちます\n",
    "    def __init__(self, thread_name, parameter_server):\n",
    "        self.environment = Environment(thread_name, parameter_server,False)\n",
    "        print(thread_name,\" Initialized\")\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if not self.environment.finished:\n",
    "                self.environment.run()\n",
    "            else:\n",
    "                sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,name, parameter_server,record=False):\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(CONFIG_FILE_PATH)\n",
    "        self.game.set_window_visible(False)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "#         self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        self.game.set_screen_format(ScreenFormat.CRCGCB)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "        \n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,frag,pos_x,pos_y)\n",
    "        \n",
    "        self.network = Network_local(name, parameter_server)\n",
    "        self.agent = Agent(name,self.network)\n",
    "        \n",
    "        self.local_step = 0\n",
    "        \n",
    "        self.finished = False\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.record = record\n",
    "        \n",
    "        self.parameter_server = parameter_server\n",
    "        \n",
    "        self.frame_record = []\n",
    "        self.loss_policy = []\n",
    "        self.loss_value = []\n",
    "        self.entropy = []\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.game.new_episode()\n",
    "        for i in range(BOTS_NUM):\n",
    "            self.game.send_game_command(\"addbot\")\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION,mode='constant')\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def get_reward(self):\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        \n",
    "        r,r_detail = self.reward_gen.get_reward(health,ammo,frag,pos_x,pos_y)\n",
    "    \n",
    "        return r\n",
    "    \n",
    "    def run(self):\n",
    "        global frames\n",
    "        \n",
    "        self.start_episode()\n",
    "        \n",
    "        train_episode = 0\n",
    "        record_l_p = 0\n",
    "        record_l_v = 0\n",
    "        for step in range(WORKER_STEPS):\n",
    "            \n",
    "#             if step % 1000 == 0:\n",
    "#                 buff = self.name + \":\" + str(step) + \"step is passed\"\n",
    "#                 print(buff)\n",
    "            #Copy params from global\n",
    "            self.agent.network.pull_parameter_server()\n",
    "\n",
    "            if not self.game.is_episode_finished():\n",
    "                \n",
    "                if step%N_ADV==0 and not step==0:\n",
    "                    self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                  self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "                s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "                action = self.agent.act(s1)\n",
    "                self.game.make_action(action,5)\n",
    "                reward = self.get_reward()\n",
    "                isterminal = self.game.is_episode_finished()\n",
    "                s2 = self.preprocess(self.game.get_state().screen_buffer) if not isterminal else None\n",
    "                \n",
    "                if self.record==True:\n",
    "                    if l_p != 0:\n",
    "                        record_l_p = l_p\n",
    "                    if l_v != 0:\n",
    "                        record_l_v = l_v\n",
    "                        \n",
    "                    self.parameter_server.write_loss(frames, record_l_p, recordl_v)\n",
    "                    if step %100 == 0:\n",
    "                        self.parameter_server.write_image(s1)\n",
    "\n",
    "                l_p, l_v = self.agent.advantage_push_network(s1,action,reward,s2,isterminal)\n",
    "                \n",
    "                if self.game.is_player_dead():\n",
    "                    self.game.respawn_player()\n",
    "                    self.reward_gen.respawn_pos(self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                                self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                                self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "            else:\n",
    "                train_episode += 1\n",
    "                self.start_episode()\n",
    "                self.reward_gen.new_episode(health = self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                           ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                           posx = self.game.get_game_variable(GameVariable.POSITION_X), \\\n",
    "                                           posy = self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "            self.local_step += 1   \n",
    "            frames += 1\n",
    "                \n",
    "        print(self.name,\" finished\")\n",
    "        self.finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEnvironment(object):\n",
    "    def __init__(self,name, parameter_server):\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(CONFIG_FILE_PATH)\n",
    "        self.game.set_window_visible(False)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "#         self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        self.game.set_screen_format(ScreenFormat.CRCGCB)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "        \n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,frag,pos_x,pos_y)\n",
    "        \n",
    "        self.network = Network_local(name, parameter_server)\n",
    "        self.agent = Agent(name,self.network)\n",
    "        \n",
    "        self.pre_death = 0\n",
    "        \n",
    "        self.record_reward = []\n",
    "        self.record_frag = []\n",
    "        self.record_death = []\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.game.new_episode()\n",
    "        for i in range(BOTS_NUM):\n",
    "            self.game.send_game_command(\"addbot\")\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION,mode='constant')\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def get_reward(self):\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        \n",
    "        r,r_detail = self.reward_gen.get_reward(health,ammo,frag,pos_x,pos_y)\n",
    "    \n",
    "        return r\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        global frames\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,0,pos_x,pos_y)\n",
    "        \n",
    "        self.start_episode()\n",
    "        \n",
    "        #Copy params from global\n",
    "        self.agent.network.pull_parameter_server()\n",
    "\n",
    "        step = 0\n",
    "        while not self.game.is_episode_finished():\n",
    "            \n",
    "            if step%N_ADV==0 and not step==0:\n",
    "                self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                              self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "            s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "            action = self.agent.act(s1)\n",
    "            self.game.make_action(action,1)\n",
    "            reward = self.get_reward()\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "\n",
    "            if self.game.is_player_dead():\n",
    "                self.game.respawn_player()\n",
    "                self.reward_gen.respawn_pos(self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                            self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                            self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                            self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "            \n",
    "            step += 1\n",
    "                \n",
    "        print(\"----------TEST at %d step-------------\"%(frames))\n",
    "        print(\"FRAG:\",self.game.get_game_variable(GameVariable.FRAGCOUNT),\"DEATH:\",self.game.get_game_variable(GameVariable.DEATHCOUNT)-self.pre_death)\n",
    "        print(\"REWARD\",self.reward_gen.total_reward)\n",
    "        print(\"DETAIL:\",self.reward_gen.total_reward_detail)\n",
    "        self.record_frag.append(self.game.get_game_variable(GameVariable.FRAGCOUNT))\n",
    "        self.record_death.append(self.game.get_game_variable(GameVariable.DEATHCOUNT)-self.pre_death)\n",
    "        self.record_reward.append(self.reward_gen.total_reward)\n",
    "        self.pre_death = self.game.get_game_variable(GameVariable.DEATHCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardGenerater(object):\n",
    "    def __init__(self,health,ammo,frag,pos_x,pos_y):\n",
    "\n",
    "        # Reward\n",
    "        self.rewards = REWARDS\n",
    "        self.dist_unit = 6.0\n",
    "        \n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y\n",
    "        \n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "        self.pre_frag = frag\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self.total_reward_detail = {'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "\n",
    "    \n",
    "    def get_reward(self,health,ammo,frag,pos_x,pos_y):\n",
    "        \n",
    "        if abs(health) > 10000:\n",
    "            health = 100.0\n",
    "\n",
    "        if self.origin_x == 0 and self.origin_y == 0:\n",
    "            self.origin_x = pos_x\n",
    "            self.origin_y = pos_y\n",
    "        \n",
    "        self.reward_detail = self.calc_reward(frag-self.pre_frag,0.0, \\\n",
    "                                              health-self.pre_health,\\\n",
    "                                              ammo-self.pre_ammo, \\\n",
    "                                              pos_x-self.origin_x, \\\n",
    "                                              pos_y-self.origin_y)\n",
    "        self.reward = sum(self.reward_detail.values())\n",
    "\n",
    "        for k,v in self.reward_detail.items():\n",
    "            self.total_reward_detail[k] += v\n",
    "        self.total_reward = sum(self.total_reward_detail.values())\n",
    "\n",
    "        self.pre_frag = frag\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "                    \n",
    "        return (self.reward, self.reward_detail)\n",
    "    \n",
    "    def calc_reward(self,m_frag,m_death,m_health,m_ammo,m_posx,m_posy):\n",
    "\n",
    "        ret_detail = {}\n",
    "\n",
    "        ret_detail['living'] = self.rewards['living']\n",
    "\n",
    "        if m_frag >= 0:\n",
    "            ret_detail['frag'] = (m_frag)*self.rewards['frag']\n",
    "            ret_detail['suicide'] = 0.0\n",
    "        else:\n",
    "            ret_detail['suicide'] = (m_frag*-1)*(self.rewards['suicide'])\n",
    "            ret_detail['frag'] = 0.0\n",
    "        \n",
    "        ret_detail['dist'] = int((math.sqrt((m_posx)**2 + (m_posy)**2))/self.dist_unit) * (self.rewards['dist'] * self.dist_unit)\n",
    "        \n",
    "        if m_health > 0:\n",
    "            ret_detail['medkit'] = self.rewards['medkit']\n",
    "            ret_detail['health_loss'] = 0.0\n",
    "        else:\n",
    "            ret_detail['medkit'] = 0.0\n",
    "            ret_detail['health_loss'] = (m_health)*self.rewards['health_loss'] * (-1)\n",
    "\n",
    "        ret_detail['ammo'] = (m_ammo)*self.rewards['ammo'] if m_ammo>0 else 0.0\n",
    "        \n",
    "        return ret_detail \n",
    "    \n",
    "    def respawn_pos(self,health,ammo,posx, posy):\n",
    "        self.origin_x = posx\n",
    "        self.origin_y = posy\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "\n",
    "    def new_episode(self,health,ammo,posx,posy):\n",
    "        self.respawn_pos(health,ammo,posx,posy)\n",
    "        self.pre_frag = 0\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.total_reward_detail={'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "    \n",
    "    def update_origin(self,pos_x, pos_y):\n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def state():\n",
    "        name = \"STATE\"\n",
    "        shape = [None,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]]\n",
    "        return tf.placeholder(tf.float32,shape=shape,name=name)\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [6,6]\n",
    "        stride = 3\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs, \\\n",
    "                                            stride=stride,padding=padding,activation_fn=activation, \\\n",
    "                                           weights_initializer=weights_init, \\\n",
    "                                            biases_initializer=bias_init)\n",
    "    def maxpool1(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [3,3]\n",
    "        stride = 2\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs, \\\n",
    "                                            stride=stride,padding=padding,activation_fn=activation, \\\n",
    "                                           weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        return tf.contrib.layers.flatten(pre_layer)\n",
    "        \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs = 512\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def policy(pre_layer):\n",
    "        num_outputs=6\n",
    "        activation_fn = tf.nn.softmax\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                    weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    def value(pre_layer):\n",
    "        num_outputs = 1\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                weights_initializer=weights_init, biases_initializer=bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --グローバルなTensorFlowのDeep Neural Networkのクラスです　-------\n",
    "class ParameterServer:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope(\"parameter_server\", reuse=tf.AUTO_REUSE):      # スレッド名で重み変数に名前を与え、識別します（Name Space）\n",
    "            self._build_model()            # ニューラルネットワークの形を決定\n",
    "            \n",
    "        with tf.variable_scope(\"summary\", reuse=tf.AUTO_REUSE):\n",
    "            self._build_summary()\n",
    "\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "        self.train_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server/trainable\")\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)    # loss関数を最小化していくoptimizerの定義です\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "#         print(\"-------GLOBAL-------\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.state = NetworkSetting.state()            \n",
    "        self.conv1 = NetworkSetting.conv1(self.state)\n",
    "        self.maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        self.conv2 = NetworkSetting.conv2(self.maxpool1)\n",
    "        self.maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        reshape = NetworkSetting.reshape(self.maxpool2)\n",
    "        with tf.variable_scope(\"trainable\"):\n",
    "            fc1 = NetworkSetting.fc1(reshape)\n",
    "\n",
    "            self.policy = NetworkSetting.policy(fc1)\n",
    "            self.value = NetworkSetting.value(fc1)\n",
    "\n",
    "        print(\"---------MODEL SHAPE-------------\")\n",
    "        print(self.state.get_shape())\n",
    "        print(self.conv1.get_shape())\n",
    "        print(self.conv2.get_shape())\n",
    "        print(reshape.get_shape())\n",
    "        print(fc1.get_shape())\n",
    "        print(self.policy.get_shape())\n",
    "        print(self.value.get_shape())\n",
    "                \n",
    "    def _build_summary(self):\n",
    "        \n",
    "        self.reward_ = tf.placeholder(tf.float32,shape=())\n",
    "        self.loss_p_ = tf.placeholder(tf.float32, shape=())\n",
    "        self.loss_v_ = tf.placeholder(tf.float32, shape=())\n",
    "        \n",
    "        self.summary_reward = tf.summary.merge([tf.summary.scalar('reward', self.reward_)])\n",
    "        \n",
    "        self.summary_loss = tf.summary.merge([tf.summary.scalar('loss_policy', self.loss_p_), tf.summary.scalar('loss_value', self.loss_v_)])\n",
    "        \n",
    "        conv1_display = tf.reshape(tf.transpose(self.conv1, [0,3,1,2]), (-1, self.conv1.get_shape()[1],self.conv1.get_shape()[2]))\n",
    "        conv2_display = tf.reshape(tf.transpose(self.conv2, [0,3,1,2]), (-1, self.conv2.get_shape()[1],self.conv2.get_shape()[2]))\n",
    "        conv1_display = tf.expand_dims(conv1_display, -1)\n",
    "        conv2_display = tf.expand_dims(conv2_display, -1)\n",
    "        \n",
    "        state_shape = self.state.get_shape()\n",
    "        conv1_shape = conv1_display.get_shape()\n",
    "        conv2_shape = conv2_display.get_shape()\n",
    "\n",
    "        s_img = []\n",
    "        s_img.append(tf.summary.image('state',tf.reshape(self.state,[-1, state_shape[1], state_shape[2], state_shape[3]]), 1))\n",
    "        s_img.append(tf.summary.image('conv1',tf.reshape(self.conv1,[-1, conv1_shape[1], conv1_shape[2], 1])))\n",
    "        s_img.append(tf.summary.image('conv2',tf.reshape(self.conv2,[-1, conv2_shape[1], conv2_shape[2], 1])))\n",
    "        \n",
    "        self.summary_image = tf.summary.merge(s_img)\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\",SESS.graph)\n",
    "        \n",
    "    def write_reward(self, step ,reward):\n",
    "        m = SESS.run(self.summary_reward, feed_dict={self.reward_:r})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_loss(self, step, l_p, l_v):\n",
    "        m = SESS.run(self.summary_reward, feed_dict={self.loss_p_: l_p, self.loss_v_:l_v})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_img(self, step, state):\n",
    "        m = SESS.run(self.summary_image, feed_dict={self.state: state})\n",
    "        return self.writer.add_summary(m, step)\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.saver.save(SESS, MODEL_PATH)\n",
    "        \n",
    "    def load_cnnweights(self, weights_path):\n",
    "        assert len(weights_path) == 4\n",
    "        cnn_weights = self.weights_params[:4]\n",
    "        w_demo = [np.load(w_p) for w_p in weights_path]\n",
    "        plh = [tf.placeholder(tf.float32, shape=w.shape) for w in w_demo]\n",
    "        assign_op = [w.assign(p) for w, p in zip(cnn_weights, plh)]\n",
    "        feed_dict = {p:w for w,p in zip(w_demo, plh)}\n",
    "        SESS.run(assign_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,name,network):\n",
    "        self.name = name\n",
    "        self.network = network\n",
    "        self.memory = []\n",
    "    \n",
    "    def act(self,s1):\n",
    "        \n",
    "        global frames\n",
    "        \n",
    "        if frames>=EPS_STEPS:\n",
    "            eps = EPS_END\n",
    "        else:\n",
    "            eps = EPS_START + frames*(EPS_END - EPS_START) / EPS_STEPS\n",
    "        \n",
    "        if random.random() < eps:\n",
    "            action = np.zeros((N_ACTION,))\n",
    "            action[np.random.randint(0,6)] = 1\n",
    "            \n",
    "            return action.tolist()\n",
    "        else:\n",
    "            s1 = np.array([s1])\n",
    "            action_prob = self.network.predict_policy(s1)[0]\n",
    "            \n",
    "            action = np.zeros((N_ACTION,))\n",
    "            action[np.random.choice(N_ACTION,p=action_prob)] = 1\n",
    "#             action[np.argmax(action_prob)] = 1\n",
    "            return action.tolist()\n",
    "    \n",
    "    def test_act(self,s1):\n",
    "        s1 = np.array([s1])\n",
    "        action_prob = self.network.predict_policy(s1)[0]\n",
    "\n",
    "        action = np.zeros((N_ACTION,))\n",
    "        action[np.random.choice(N_ACTION,p=action_prob)] = 1\n",
    "#         action[np.argmax(action_prob)] = 1\n",
    "        return action.tolist()\n",
    "    \n",
    "    def advantage_push_network(self,s1,action,reward,s2,isterminal):\n",
    "        \n",
    "        self.memory.append((s1,action,reward,s2))\n",
    "        l_p = 0\n",
    "        l_v = 0\n",
    "        \n",
    "        if isterminal:\n",
    "            for i in range(len(self.memory)-1,-1,-1):\n",
    "                s1,a,r,s2 = self.memory[i]\n",
    "                if i==len(self.memory)-1:\n",
    "                    self.R = 0\n",
    "                else:\n",
    "                    self.R = r + GAMMA*self.R\n",
    "                \n",
    "                self.network.train_push(s1,a,self.R,s2,isterminal)\n",
    "            \n",
    "            self.memory = []\n",
    "            self.R = 0\n",
    "            l_p, l_v = self.network.update_parameter_server()\n",
    "\n",
    "        if len(self.memory)>=N_ADV:\n",
    "            \n",
    "            for i in range(N_ADV-1,-1,-1):\n",
    "                s1,a,r,s2 = self.memory[i]\n",
    "                if i==N_ADV-1:\n",
    "                    self.R = self.network.predict_value(np.array([s1]))[0][0]\n",
    "                else:\n",
    "                    self.R = r + GAMMA*self.R\n",
    "                \n",
    "                self.network.train_push(s1,a,self.R,s2,isterminal)\n",
    "            \n",
    "            self.memory = []\n",
    "            self.R = 0\n",
    "            l_p, l_v = self.network.update_parameter_server()\n",
    "            \n",
    "        return l_p, l_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_local(object):\n",
    "    def __init__(self,name,parameter_server):\n",
    "        self.name = name\n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            self._model()\n",
    "            self._build_graph(parameter_server)\n",
    "            \n",
    "        self.s1 = np.empty(shape=(100,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.s2 = np.empty(shape=(100,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.reward = np.empty(shape=(100,1),dtype=np.float32)\n",
    "        self.action = np.empty(shape=(100,N_ACTION),dtype=np.float32)\n",
    "        self.isterminal = np.empty(shape=(100,1),dtype=np.int8)\n",
    "        self.queue_pointer = 0\n",
    "\n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self):     # Kerasでネットワークの形を定義します\n",
    "        \n",
    "        self.state = NetworkSetting.state()\n",
    "        self.conv1 = NetworkSetting.conv1(self.state)\n",
    "        self.maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        self.conv2 = NetworkSetting.conv2(self.maxpool1)\n",
    "        self.maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        reshape = NetworkSetting.reshape(self.maxpool2)\n",
    "        \n",
    "        with tf.variable_scope(\"trainable\"):        \n",
    "            fc1 = NetworkSetting.fc1(reshape)\n",
    "\n",
    "            self.policy = NetworkSetting.policy(fc1)\n",
    "\n",
    "            self.value = NetworkSetting.value(fc1)\n",
    "            \n",
    "    def _build_graph(self,parameter_server):\n",
    "        \n",
    "        self.a_t = tf.placeholder(tf.float32, shape=(None, N_ACTION))\n",
    "        self.r_t = tf.placeholder(tf.float32, shape=(None,1))\n",
    "        \n",
    "        log_prob = tf.log(tf.reduce_sum(self.policy * self.a_t, axis=1, keep_dims=True)+1e-10)\n",
    "        advantage = self.r_t - self.value\n",
    "        \n",
    "        self.loss_policy = -log_prob * tf.stop_gradient(advantage)\n",
    "        self.loss_value = 0.5 * tf.square(advantage)\n",
    "        entropy = 0.05 * tf.reduce_sum(self.policy * tf.log(self.policy + 1e-10), axis=1, keep_dims=True)\n",
    "        self.loss_total = tf.reduce_mean(self.loss_policy + self.loss_value + entropy)\n",
    "        \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "        self.train_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"/trainable\")\n",
    "        self.grads = tf.gradients(self.loss_total, self.train_params)\n",
    "\n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.train_params))\n",
    "\n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.train_params,parameter_server.train_params)]\n",
    "\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.train_params,self.train_params)]\n",
    "        \n",
    "        self.pull_global_weights_params_all = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params,parameter_server.weights_params)]\n",
    "        \n",
    "    def pull_parameter_server(self):\n",
    "        SESS.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self):\n",
    "        SESS.run(self.push_local_weight_params)\n",
    "        \n",
    "    def show_weights(self):\n",
    "        hoge = SESS.run(self.weights_params)\n",
    "        for i in range(len(hoge)):\n",
    "            print(hoge[i])\n",
    "            \n",
    "    def update_parameter_server(self):\n",
    "        l_p = 0\n",
    "        l_v = 0\n",
    "        if self.queue_pointer > 0:\n",
    "            s1 = self.s1[0:self.queue_pointer]\n",
    "            s2 = self.s2[0:self.queue_pointer]\n",
    "            r = self.reward[0:self.queue_pointer]\n",
    "            a = self.action[0:self.queue_pointer]\n",
    "            feed_dict = {self.state: s1,self.a_t:a, self.r_t:r}\n",
    "            _, l_p, l_v = SESS.run([self.update_global_weight_params, self.loss_policy, self.loss_value],feed_dict)\n",
    "#             print(hoge.shape)\n",
    "#             print(hoge)\n",
    "            self.queue_pointer = 0\n",
    "        return l_p, l_v\n",
    "    \n",
    "    def predict_value(self,s):\n",
    "        v = SESS.run(self.value,feed_dict={self.state:s})\n",
    "        return v        \n",
    "    \n",
    "    def predict_policy(self,s):\n",
    "        feed_dict = {self.state:s}\n",
    "        prob = SESS.run(self.policy, feed_dict)\n",
    "        return prob\n",
    "    \n",
    "    def train_push(self,s,a,r,s_,isterminal):\n",
    "        self.s1[self.queue_pointer] = s\n",
    "        self.s2[self.queue_pointer] = s_\n",
    "        self.action[self.queue_pointer] = a\n",
    "        self.reward[self.queue_pointer] = r\n",
    "        self.isterminal[self.queue_pointer] = isterminal\n",
    "        self.queue_pointer += 1\n",
    "        \n",
    "    def pull_all_parameter_server(self):\n",
    "        SESS.run(self.pull_global_weights_params_all)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- main ここからメイン関数です------------------------------\n",
    "# M0.global変数の定義と、セッションの開始です\n",
    "frames = 0              # 全スレッドで共有して使用する総ステップ数\n",
    "isLearned = False       # 学習が終了したことを示すフラグ\n",
    "config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=\"0\"))\n",
    "config.log_device_placement = False\n",
    "config.allow_soft_placement = True\n",
    "SESS = tf.Session(config=config)\n",
    "# SESS = tf_debug.LocalCLIDebugWrapperSession(SESS)\n",
    "\n",
    "# スレッドを作成します\n",
    "with tf.device(\"/gpu:1\"):\n",
    "    parameter_server = ParameterServer()    # 全スレッドで共有するパラメータを持つエンティティです\n",
    "    parameter_server.load_cnnweights(MERGED_WEIGHTS_PATH)\n",
    "\n",
    "    threads = []     # 並列して走るスレッド\n",
    "    for i in range(N_WORKERS):\n",
    "        thread_name = \"local_thread\"+str(i+1)\n",
    "        threads.append(Worker_thread(thread_name=thread_name, parameter_server=parameter_server))\n",
    "\n",
    "SESS.run(tf.global_variables_initializer())     # TensorFlowを使う場合、最初に変数初期化をして、実行します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = TestEnvironment(\"test_env\",parameter_server)\n",
    "\n",
    "# TensorFlowでマルチスレッドを実行します\n",
    "SESS.run(tf.global_variables_initializer())     # TensorFlowを使う場合、最初に変数初期化をして、実行します\n",
    "\n",
    "# threads[0].environment.record=True\n",
    "\n",
    "running_threads = []\n",
    "start_time = time.time()\n",
    "for worker in threads:\n",
    "    job = lambda: worker.run()      # この辺は、マルチスレッドを走らせる作法だと思って良い\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "\n",
    "test_frame = 0\n",
    "while True:\n",
    "    \n",
    "    if frames >= test_frame and frames<test_frame+1000:\n",
    "        test_env.run()\n",
    "        test_frame += 1000\n",
    "    elif frames >= test_frame+1000:\n",
    "        print(\"TEST at %d~%d step cant be finished\"%(test_frame, test_frame+1000-1))\n",
    "        test_frame += 1000\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    isLearned = True\n",
    "    for worker in threads:\n",
    "        if not worker.environment.finished:\n",
    "            isLearned = False\n",
    "    \n",
    "    if isLearned:\n",
    "        break\n",
    "\n",
    "print(\"*****************************\\nTIME to LEARNING:%.3f [sec]\\n*****************************\"%(time.time()-start_time))\n",
    "\n",
    "# np.save(\"./records/reward.npy\",np.array(test_env.record_reward))\n",
    "# np.save(\"./records/frag.npy\",np.array(test_env.record_frag))\n",
    "# np.save(\"./records/death.npy\",np.array(test_env.record_death))\n",
    "# np.save(\"./records/loss_policy.npy\",threads[0].environment.loss_policy)\n",
    "# np.save(\"./records/loss_value.npy\",threads[0].environment.loss_value)\n",
    "# np.save(\"./records/entropy.npy\",threads[0].environment.entropy)\n",
    "# np.save(\"./records/frame.npy\",threads[0].environment.frame_record)\n",
    "\n",
    "# parameter_server.save_model()\n",
    "# print(\"Learning phase is finished\")\n",
    "for i in range(3):\n",
    "    test_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
