{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from random import sample, randint, random\n",
    "import time,random,threading,datetime\n",
    "from tqdm import tqdm\n",
    "import transition\n",
    "import tensorflow as tf\n",
    "import replay_memory\n",
    "import transition\n",
    "import h5py\n",
    "import math\n",
    "import sys, os, glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_PATH = \"./demodata_cig2017_v0-2.h5\"\n",
    "CONFIG_FILE_PATH = \"./config/custom_config.cfg\"\n",
    "LOG_DIR = \"./logs/logs_v13\"\n",
    "MODEL_PATH = \"./models/model_v13\"\n",
    "PREMODEL_PATH = \"./models/premodel_v13\"\n",
    "GIF_PATH = \"./gifs/final_v13.gif\"\n",
    "PREGIF_PATH = \"./gifs/pre_v13.gif\"\n",
    "\n",
    "TIME_LEARN = datetime.timedelta(hours=4)\n",
    "TIME_PRELEARN = datetime.timedelta(hours  = 8)\n",
    "\n",
    "SAVE_FILE = True\n",
    "\n",
    "USED_GPU = \"0\"\n",
    "\n",
    "# if SAVE_FILE:\n",
    "#     if len(glob.glob(LOG_DIR+\"/*\"))!=0 or len(glob.glob(MODEL_PATH))!=0:\n",
    "#         print(\"ERROR: Log or Model file exists already!\")\n",
    "#         sys.exit()\n",
    "        \n",
    "#     if not os.path.exists(MODEL_PATH):\n",
    "#         os.mkdir(MODEL_PATH)\n",
    "\n",
    "RESOLUTION = (120,180,3)\n",
    "\n",
    "N_ADV = 4\n",
    "\n",
    "FREQ_COPY = 10\n",
    "FREQ_TEST = 50\n",
    "\n",
    "N_WORKERS = 10\n",
    "N_PRESTEPS = 20000\n",
    "N_STEPS = 200\n",
    "TOTAL_STEPS = N_STEPS\n",
    "TOTAL_TIME = TIME_LEARN.seconds\n",
    "TOTAL_TIME_PRE = TIME_PRELEARN.seconds\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "LEARNING_RATE = 0.3\n",
    "RMSProbDecaly = 0.9\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "LAMBDA1 = 1.0\n",
    "LAMBDA2 = 1.0\n",
    "LAMBDA3 = 10e-3\n",
    "L_MIN = 0.8\n",
    "LSTM_SIZE = 512\n",
    "\n",
    "BETA_0 = 0.0\n",
    "\n",
    "N_ACTION = 6\n",
    "\n",
    "BOTS_NUM = 20\n",
    "\n",
    "FRAME_REPEAT = 4\n",
    "\n",
    "N_FOLDER = 3\n",
    "\n",
    "REWARDS = {'living':-1, 'health_loss':-0.01, 'medkit':50, 'ammo':0.0, 'frag':500, 'dist':5e-2, 'suicide':-100} \n",
    "\n",
    "CAPACITY = 10000\n",
    "\n",
    "EPS_START = 0.8\n",
    "EPS_END = 0.0\n",
    "LINEAR_EPS_START = 0.1\n",
    "LINEAR_EPS_END = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --class for Thread　-------\n",
    "class WorkerThread:\n",
    "    # Each Thread has an Environment to run Game and Learning.\n",
    "    def __init__(self, thread_name, parameter_server, replay_memory, isLearning=True):\n",
    "        self.environment = Environment(thread_name, parameter_server, replay_memory)\n",
    "        print(thread_name,\" Initialized\")\n",
    "        self.isLearning = isLearning\n",
    "\n",
    "    def run(self):\n",
    "        if self.isLearning:\n",
    "            while True:\n",
    "                if not self.environment.finished:\n",
    "                    self.environment.run()\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            # Run Test Environment\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,name, parameter_server,replay_memory, summary=False):\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(CONFIG_FILE_PATH)\n",
    "        self.game.set_window_visible(False)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "#         self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        self.game.set_screen_format(ScreenFormat.CRCGCB)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "        \n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,frag,pos_x,pos_y)\n",
    "        \n",
    "        self.network = NetworkLocal(name, parameter_server)\n",
    "        self.agent = Agent(self.network)\n",
    "        \n",
    "#         self.demo_buff = replay_memory.ReplayMemory(CAPACITY)\n",
    "        self.replay_memory = replay_memory\n",
    "        \n",
    "        self.local_step = 0\n",
    "        \n",
    "        self.finished = False\n",
    "        \n",
    "        self.pre_death = 0\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.summary = summary\n",
    "        self.parameter_server = parameter_server\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.game.new_episode()\n",
    "        self.agent.image_buff = []\n",
    "        for i in range(BOTS_NUM):\n",
    "            self.game.send_game_command(\"addbot\")\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION,mode='constant')\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def get_reward(self):\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        \n",
    "        r,r_detail = self.reward_gen.get_reward(health,ammo,frag,pos_x,pos_y)\n",
    "    \n",
    "        return r,r_detail\n",
    "    \n",
    "    def calc_beta(self):\n",
    "        global current_time\n",
    "        \n",
    "        return ((1.0 - BETA_0) / (TOTAL_TIME + TOTAL_TIME_PRE)) * current_time + BETA_0\n",
    "    \n",
    "    def make_batch(self):\n",
    "        \n",
    "        tree_idx, batch, is_weight = self.replay_memory.sample(BATCH_SIZE)\n",
    "        is_weight = np.power(is_weight, self.calc_beta())\n",
    "\n",
    "        s1 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)\n",
    "        s2 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)\n",
    "        actions = np.zeros((BATCH_SIZE ,),dtype=np.int8)\n",
    "        rewards = np.zeros((BATCH_SIZE, ),dtype=np.float32)\n",
    "        rewards_adv = np.zeros((BATCH_SIZE,),dtype=np.float32)\n",
    "        isterminals = np.zeros((BATCH_SIZE, ),dtype=np.int8)\n",
    "        isdemos = np.zeros((BATCH_SIZE,),dtype=np.int8)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            for j in range(N_ADV):\n",
    "                if not (batch[i][j] is None or type(batch[i][j].s1) == type(None)):\n",
    "                    s1[i][j] = batch[i][j].s1\n",
    "                if not (batch[i][j] is None or type(batch[i][j].s2) == type(None)):\n",
    "                    s2[i][j] = batch[i][j].s2\n",
    "#                     print(np.mean(s1[i][j]), np.std(s1[i][j]))\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            R = 0\n",
    "            for j in range(N_ADV-1, -1, -1):\n",
    "#                     s1[i][j] = batch[i][j].s1\n",
    "#                     s2[i][j] = batch[i][j].s2\n",
    "                if not batch[i][j].isterminal :\n",
    "                    if j == N_ADV-1:\n",
    "                        isterminals[i] = batch[i][j].isterminal\n",
    "                        actions[i] = batch[i][j].action\n",
    "                        rewards[i] = batch[i][j].reward\n",
    "#                             R = np.max(self.network.get_q_value(np.array([np.concatenate([s1[i][0:j], np.ones(shape=(N_ADV-j,)+RESOLUTION)*np.nan])]))[0])\n",
    "#                             print(s1[i].shape)\n",
    "                        R = np.max(self.network.get_q_value( np.array([s1[i]]) )[0])\n",
    "                    else:\n",
    "                        R = batch[i][j].reward + GAMMA * R\n",
    "                else:\n",
    "                    if not(type(batch[i][j].s1) == type(None)):\n",
    "                        isterminals[i] = batch[i][j].isterminal\n",
    "                        actions[i] = batch[i][j].action\n",
    "                        rewards[i] = batch[i][j].reward\n",
    "                    else:\n",
    "                        pass\n",
    "            rewards_adv[i] = R\n",
    "            isdemos[i] = batch[i][-1].isdemo\n",
    "            \n",
    "        return tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight\n",
    "        \n",
    "    \n",
    "    # Method for Previous Learning\n",
    "    def run_pre_learning(self):        \n",
    "        global frames, start_time_pre,current_time\n",
    "        \n",
    "#         for step in tqdm(range(N_PRESTEPS)):\n",
    "        step = 0\n",
    "        while True:\n",
    "            \n",
    "            self.network.pull_parameter_server()\n",
    "            \n",
    "            tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight = self.make_batch()\n",
    "\n",
    "            l_one, l_adv, l_cls, l_l2 = self.network.update_parameter_server_batch(s1,actions,rewards,rewards_adv,s2,isdemos, is_weight)\n",
    "            self.replay_memory.batch_update(tree_idx, l_one+l_adv+l_cls + l_l2, isdemos)\n",
    "            if step%1 == 0 and SAVE_FILE == True:\n",
    "#                 lo, la, lc = self.network.calc_loss(s1[0:1],actions[0:1],rewards[0:1],rewards_adv[0:1],s2[0:1],isdemos[0:1])\n",
    "                self.parameter_server.write_weights(frames)\n",
    "                self.parameter_server.write_loss(frames, np.mean(l_one), np.mean(l_adv), np.mean(l_cls), l_l2)\n",
    "                if step % 1 == 0:\n",
    "                    self.parameter_server.write_images(frames, s1[0:1])\n",
    "            if step%FREQ_COPY==0:\n",
    "                self.network.copy_learn2target()\n",
    "            frames += 1\n",
    "            step += 1\n",
    "            \n",
    "            current_time = datetime.datetime.now().timestamp() - start_time_pre.timestamp()\n",
    "            \n",
    "            if datetime.datetime.now() > TIME_PRELEARN + start_time_pre:\n",
    "                runout = True\n",
    "                break\n",
    "    \n",
    "    # Method for multi task learning\n",
    "    def run(self):\n",
    "        global frames,runout,current_time\n",
    "        \n",
    "        self.start_episode()\n",
    "        \n",
    "        train_episode = 0\n",
    "        step = 0\n",
    "        transitions = np.empty((N_ADV, ),dtype=object)\n",
    "        while True: \n",
    "#         for step in range(WORKER_STEPS):\n",
    "            #Copy params from global\n",
    "            self.agent.q_network.pull_parameter_server()\n",
    "\n",
    "            if not self.game.is_episode_finished():\n",
    "                \n",
    "                if step%N_ADV==0 and not step==0:\n",
    "                    self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                  self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "                s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "                action = self.agent.act_eps_greedy(s1)\n",
    "                self.game.make_action(action,FRAME_REPEAT)\n",
    "                reward,_ = self.get_reward()\n",
    "                isterminal = self.game.is_episode_finished()\n",
    "                s2 = self.preprocess(self.game.get_state().screen_buffer) if not isterminal else np.zeros(RESOLUTION)\n",
    "\n",
    "                transitions[step%N_ADV] = transition.Transition(s1,action.index(1),s2,reward,isterminal, False)\n",
    "                \n",
    "                if step % N_ADV == 0 and not step==0:\n",
    "                    self.replay_memory.store(np.copy(transitions))\n",
    "                    \n",
    "                tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight = self.make_batch()\n",
    "\n",
    "                l_one, l_adv, l_cls, l_l2 = self.network.update_parameter_server_batch(s1,actions,rewards,rewards_adv,s2,isdemos, is_weight)\n",
    "                self.replay_memory.batch_update(tree_idx, l_one+l_adv+l_cls + l_l2, isdemos)\n",
    "            \n",
    "                if self.summary==True and SAVE_FILE == True:\n",
    "                    self.parameter_server.write_weights(frames)\n",
    "                    self.parameter_server.write_loss(frames, np.mean(l_one), np.mean(l_adv), np.mean(l_cls), l_l2)\n",
    "                    if step % 1 == 0:\n",
    "                        self.parameter_server.write_images(frames, s1[0:1])\n",
    "            \n",
    "                if self.game.is_player_dead():\n",
    "                    self.game.respawn_player()\n",
    "                    self.reward_gen.respawn_pos(self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                                self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                                self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "                frames += 1\n",
    "                step += 1\n",
    "\n",
    "            else:\n",
    "                train_episode += 1\n",
    "                self.start_episode()\n",
    "                self.reward_gen.new_episode(health = self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                           ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                           posx = self.game.get_game_variable(GameVariable.POSITION_X), \\\n",
    "                                           posy = self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "            \n",
    "            current_time = datetime.datetime.now().timestamp() - start_time_pre.timestamp()\n",
    "            \n",
    "            if runout == True:\n",
    "                break\n",
    "                \n",
    "        print(self.name,\" finished\")\n",
    "        self.finished = True\n",
    "        \n",
    "    def run_test(self, save_gif=False, gif_path=GIF_PATH):\n",
    "        \n",
    "        global frames\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,0,pos_x,pos_y)\n",
    "        \n",
    "        self.start_episode()\n",
    "        \n",
    "        #Copy params from global\n",
    "        self.agent.q_network.pull_parameter_server()\n",
    "\n",
    "        step = 0\n",
    "        reward_adv = [0,0,0,0]\n",
    "        gif_img = []\n",
    "        while not self.game.is_episode_finished():\n",
    "            \n",
    "            if step%N_ADV==0 and not step==0:\n",
    "                self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                              self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "#             print(\"-----%d----\"%(step))\n",
    "            s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "            if save_gif==True:\n",
    "                if step%5==0:\n",
    "                    gif_img.append(s1)\n",
    "            action = self.agent.act_greedy(s1)\n",
    "            self.game.make_action(action,1)\n",
    "            reward,_ = self.get_reward()\n",
    "            reward_adv.append(reward)\n",
    "            reward_adv.pop(0)\n",
    "#             if(len(self.agent.image_buff) == 4):\n",
    "#                 print(\"policy:\", self.agent.network.predict_policy(self.agent.image_buff))\n",
    "#                 print(\"reward:\",sum(reward_adv))\n",
    "#                 print(\"value:\", self.agent.network.predict_value(self.agent.image_buff))\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "\n",
    "            if self.game.is_player_dead():\n",
    "                self.game.respawn_player()\n",
    "                self.reward_gen.respawn_pos(self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                            self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                            self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                            self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        save_img = []\n",
    "        \n",
    "        if save_gif == True:\n",
    "            for i in range(len(gif_img)):\n",
    "                save_img.append(Image.fromarray(np.uint8(gif_img[i]*255)))\n",
    "            save_img[0].save(gif_path,save_all=True,append_images=save_img[1:])\n",
    "        \n",
    "        print(\"----------TEST at %d step-------------\"%(frames))\n",
    "        ret_frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        ret_death = self.game.get_game_variable(GameVariable.DEATHCOUNT)-self.pre_death\n",
    "        ret_reward = self.reward_gen.total_reward\n",
    "        print(\"FRAG:\",ret_frag,\"DEATH:\",ret_death)\n",
    "        print(\"REWARD\",ret_reward)\n",
    "        print(\"DETAIL:\",self.reward_gen.total_reward_detail)\n",
    "        self.pre_death = self.game.get_game_variable(GameVariable.DEATHCOUNT)\n",
    "        return ret_reward,ret_frag,ret_death\n",
    "                 \n",
    "    def load_demonstration(self):\n",
    "        hdf5file = h5py.File(DEMO_PATH,\"r\")\n",
    "        folder = \"demodata_\"+str(0)\n",
    "        state1 = hdf5file[folder+\"/state1\"].value\n",
    "        state2 = hdf5file[folder+\"/state2\"].value\n",
    "        actions = hdf5file[folder+\"/actions\"].value\n",
    "        isterminals = hdf5file[folder+\"/isterminals\"].value\n",
    "        health = hdf5file[folder+\"/healths\"].value\n",
    "        ammo = hdf5file[folder+\"/ammos\"].value\n",
    "        posx = hdf5file[folder+\"/posxs\"].value\n",
    "        posy = hdf5file[folder+\"/posys\"].value\n",
    "        death = hdf5file[folder+\"/deaths\"].value\n",
    "        frag = hdf5file[folder+\"/frags\"].value\n",
    "\n",
    "        for i in range(1,N_FOLDER):\n",
    "            folder = \"demodata_\" +str(i)\n",
    "            state1 = np.concatenate((state1,hdf5file[folder+\"/state1\"].value),axis=0)\n",
    "            state2 = np.concatenate((state2,hdf5file[folder+\"/state2\"].value),axis=0)\n",
    "            actions = np.concatenate((actions,hdf5file[folder+\"/actions\"].value),axis=0)\n",
    "            isterminals = np.concatenate((isterminals,hdf5file[folder+\"/isterminals\"].value),axis=0)\n",
    "            health = np.concatenate((health,hdf5file[folder+\"/healths\"].value),axis=0)\n",
    "            ammo = np.concatenate((ammo,hdf5file[folder+\"/ammos\"].value),axis=0)\n",
    "            posx = np.concatenate((posx,hdf5file[folder+\"/posxs\"].value),axis=0)\n",
    "            posy = np.concatenate((posy,hdf5file[folder+\"/posys\"].value),axis=0)\n",
    "            death = np.concatenate((death,hdf5file[folder+\"/deaths\"].value),axis=0)\n",
    "            frag = np.concatenate((frag,hdf5file[folder+\"/frags\"].value),axis=0)\n",
    "\n",
    "        n_transit, n_step, _ = actions.shape\n",
    "        \n",
    "        n_step = N_ADV\n",
    "\n",
    "        print(\"SIZE of DEMO:\",actions.shape)\n",
    "\n",
    "        transit = np.empty((n_step,),dtype=object)\n",
    "\n",
    "        is_dead = False\n",
    "        is_finished = False\n",
    "\n",
    "        pre_health = 100\n",
    "        pre_ammo = 15\n",
    "        pre_frag = 0\n",
    "        pre_death = 0\n",
    "        pre_posx = 0.0\n",
    "        pre_posy = 0.0\n",
    "\n",
    "\n",
    "        for i in range(n_transit):\n",
    "\n",
    "            if i % 2 == 0:\n",
    "                pre_posx = posx[i][0]\n",
    "                pre_posy = posy[i][0]\n",
    "\n",
    "            for j in range(n_step):\n",
    "                if not is_finished:\n",
    "                    if is_dead :\n",
    "                        pre_posx = posx[i][j]\n",
    "                        pre_posy = posy[i][j]\n",
    "                        is_dead = False\n",
    "\n",
    "                    m_frag = frag[i][j] - pre_frag\n",
    "                    m_death = death[i][j] - pre_death\n",
    "                    m_health = health[i][j] - pre_health\n",
    "                    m_ammo = ammo[i][j] - pre_ammo\n",
    "                    m_posx = posx[i][j] - pre_posx\n",
    "                    m_posy = posy[i][j] - pre_posy\n",
    "\n",
    "                    if m_death >= 1:\n",
    "                        is_dead = True \n",
    "\n",
    "                    if isterminals[i][j] == True:\n",
    "                        is_finished = True\n",
    "\n",
    "                    r_d = self.reward_gen.calc_reward(m_frag,m_death,m_health,m_ammo,m_posx,m_posy)\n",
    "                    r = sum(r_d.values())\n",
    "                    transit[j] = transition.Transition(state1[i][j],actions[i][j],state2[i][j],r,isterminals[i][j],True)\n",
    "\n",
    "                    pre_frag = frag[i][j]\n",
    "                    pre_death = death[i][j]\n",
    "                    pre_health = health[i][j]\n",
    "                    pre_ammo = ammo[i][j]\n",
    "                else:\n",
    "                    transit[j] = transition.Transition(None,None,None,None,True,True)\n",
    "\n",
    "            is_finished = False\n",
    "\n",
    "            self.replay_memory.store(np.copy(transit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardGenerater(object):\n",
    "    def __init__(self,health,ammo,frag,pos_x,pos_y):\n",
    "\n",
    "        # Reward\n",
    "        self.rewards = REWARDS\n",
    "        self.dist_unit = 6.0\n",
    "        \n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y\n",
    "        \n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "        self.pre_frag = frag\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self.total_reward_detail = {'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "\n",
    "    \n",
    "    def get_reward(self,health,ammo,frag,pos_x,pos_y):\n",
    "        \n",
    "        if abs(health) > 10000:\n",
    "            health = 100.0\n",
    "\n",
    "        if self.origin_x == 0 and self.origin_y == 0:\n",
    "            self.origin_x = pos_x\n",
    "            self.origin_y = pos_y\n",
    "        \n",
    "        self.reward_detail = self.calc_reward(frag-self.pre_frag,0.0, \\\n",
    "                                              health-self.pre_health,\\\n",
    "                                              ammo-self.pre_ammo, \\\n",
    "                                              pos_x-self.origin_x, \\\n",
    "                                              pos_y-self.origin_y)\n",
    "        self.reward = sum(self.reward_detail.values())\n",
    "\n",
    "        for k,v in self.reward_detail.items():\n",
    "            self.total_reward_detail[k] += v\n",
    "        self.total_reward = sum(self.total_reward_detail.values())\n",
    "\n",
    "        self.pre_frag = frag\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "                    \n",
    "        return (self.reward, self.reward_detail)\n",
    "    \n",
    "    def calc_reward(self,m_frag,m_death,m_health,m_ammo,m_posx,m_posy):\n",
    "\n",
    "        ret_detail = {}\n",
    "\n",
    "        ret_detail['living'] = self.rewards['living']\n",
    "\n",
    "        if m_frag >= 0:\n",
    "            ret_detail['frag'] = (m_frag)*self.rewards['frag']\n",
    "            ret_detail['suicide'] = 0.0\n",
    "        else:\n",
    "            ret_detail['suicide'] = (m_frag*-1)*(self.rewards['suicide'])\n",
    "            ret_detail['frag'] = 0.0\n",
    "        \n",
    "        ret_detail['dist'] = int((math.sqrt((m_posx)**2 + (m_posy)**2))/self.dist_unit) * (self.rewards['dist'] * self.dist_unit)\n",
    "        \n",
    "        if m_health > 0:\n",
    "            ret_detail['medkit'] = self.rewards['medkit']\n",
    "            ret_detail['health_loss'] = 0.0\n",
    "        else:\n",
    "            ret_detail['medkit'] = 0.0\n",
    "            ret_detail['health_loss'] = (m_health)*self.rewards['health_loss'] * (-1)\n",
    "\n",
    "        ret_detail['ammo'] = (m_ammo)*self.rewards['ammo'] if m_ammo>0 else 0.0\n",
    "        \n",
    "        return ret_detail \n",
    "    \n",
    "    def respawn_pos(self,health,ammo,posx, posy):\n",
    "        self.origin_x = posx\n",
    "        self.origin_y = posy\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "\n",
    "    def new_episode(self,health,ammo,posx,posy):\n",
    "        self.respawn_pos(health,ammo,posx,posy)\n",
    "        self.pre_frag = 0\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.total_reward_detail={'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "    \n",
    "    def update_origin(self,pos_x, pos_y):\n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling should not execute when the tree is not full !!!\n",
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity, permanent_data=0):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # stores not probabilities but priorities !!!\n",
    "        self.data = np.zeros(capacity, dtype=object)  # stores transitions\n",
    "        self.permanent_data = permanent_data  # numbers of data which never be replaced, for demo data protection\n",
    "        assert 0 <= self.permanent_data <= self.capacity  # equal is also illegal\n",
    "        self.full = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.data_pointer\n",
    "\n",
    "    def set_parmanent_data(self,n_parmanent_data):\n",
    "        self.permanent_data = n_parmanent_data\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, p)\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.full = True\n",
    "            self.data_pointer = self.data_pointer % self.capacity + self.permanent_data  # make sure demo data permanent\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if v <= self.tree[left_child_idx]:\n",
    "                parent_idx = left_child_idx\n",
    "            else:\n",
    "                v -= self.tree[left_child_idx]\n",
    "                parent_idx = right_child_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    epsilon = 0.001  # small amount to avoid zero priority\n",
    "    demo_epsilon = 1.0  # 1.0  # extra\n",
    "    alpha = 0.4  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.6  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity, permanent_data=0):\n",
    "        self.permanent_data = permanent_data\n",
    "        self.tree = SumTree(capacity, permanent_data)\n",
    "#         self.data_name = data_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree)\n",
    "\n",
    "    def full(self):\n",
    "        return self.tree.full\n",
    "\n",
    "    def store(self, transition):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        self.tree.add(max_p, transition)  # set the max_p for new transition\n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx = np.empty((n,), dtype=np.int32)\n",
    "        b_memory = np.empty((n, self.tree.data[0].size), dtype=object)\n",
    "        ISWeights = np.empty((n,))\n",
    "        pri_seg = self.tree.total_p / n\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        if self.tree.full:\n",
    "            min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p\n",
    "            assert min_prob > 0\n",
    "\n",
    "            for i in range(n):\n",
    "                v = np.random.uniform(pri_seg * i, pri_seg * (i + 1))\n",
    "                idx, p, data = self.tree.get_leaf(v)  # note: idx is the index in self.tree.tree\n",
    "                prob = p / self.tree.total_p\n",
    "#                 ISWeights[i] = np.power(prob/min_prob, -self.beta)\n",
    "                ISWeights[i] = prob/min_prob\n",
    "                b_idx[i], b_memory[i] = idx, data\n",
    "        else:\n",
    "            min_prob = np.min(self.tree.tree[self.tree.capacity-1:self.tree.capacity+self.tree.data_pointer-1]) / self.tree.total_p\n",
    "            assert min_prob > 0\n",
    "\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    v = np.random.uniform(self.abs_err_upper, pri_seg * (i + 1))\n",
    "                else:\n",
    "                    v = np.random.uniform(pri_seg * i, pri_seg * (i + 1))\n",
    "                idx, p, data = self.tree.get_leaf(v)  # note: idx is the index in self.tree.tree\n",
    "                prob = p / self.tree.total_p\n",
    "#                 ISWeights[i] = np.power(prob/min_prob, -self.beta)\n",
    "                ISWeights[i] = prob/min_prob\n",
    "                b_idx[i], b_memory[i] = idx, data\n",
    "\n",
    "        return b_idx, b_memory, ISWeights  # note: b_idx stores indexes in self.tree.tree, not in self.tree.data !!!\n",
    "\n",
    "    # update priority\n",
    "    def batch_update(self, tree_idxes, abs_errors ,is_demo):\n",
    "        for i, d in enumerate(is_demo):\n",
    "            if d == True:\n",
    "                abs_errors[i] + self.demo_epsilon\n",
    "            else:\n",
    "                abs_errors[i] + self.epsilon\n",
    "        \n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idxes, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,q_network):\n",
    "        \n",
    "        self.q_network = q_network\n",
    "        \n",
    "#         self.image_buff = np.zeros(shape=(N_ADV,)+RESOLUTION)\n",
    "        self.image_buff = []\n",
    "        self.memory = []\n",
    "        self.batch = {'s1':[], 'action':[], 's2':[] ,'reward':[], 'reward_adv':[], 'isdemo':[]}\n",
    "        self.R = 0\n",
    "        \n",
    "        self.s1_record = np.zeros((1,N_ADV,)+RESOLUTION)\n",
    "        self.loss_one_record = 0\n",
    "        self.loss_adv_record = 0\n",
    "        self.loss_class_record = 0\n",
    "        self.loss_l2_record = 0\n",
    "        \n",
    "    def calc_eps_step(self):\n",
    "            global frames\n",
    "\n",
    "            if frames<TOTAL_STEPS*LINEAR_EPS_START:\n",
    "                eps = EPS_START\n",
    "            elif frames>=TOTAL_STEPS*LINEAR_EPS_START and frames<TOTAL_STEPS*LINEAR_EPS_END:\n",
    "                eps = EPS_START + frames*(EPS_END-EPS_START)/(TOTAL_STEPS)\n",
    "            else:\n",
    "                eps = EPS_END\n",
    "            return eps\n",
    "        \n",
    "    def calc_eps_time(self):\n",
    "        \n",
    "        current_time_async = current_time - TOTAL_TIME_PRE\n",
    "        if current_time_async < TOTAL_TIME * LINEAR_EPS_START:\n",
    "            eps = EPS_START\n",
    "        elif current_time_async >= TOTAL_TIME * LINEAR_EPS_START and current_time_async < TOTAL_TIME*LINEAR_EPS_END:\n",
    "            eps = EPS_START + current_time_async*(EPS_END-EPS_START)/(TOTAL_TIME)\n",
    "        else:\n",
    "            eps = EPS_END\n",
    "            \n",
    "        return eps\n",
    "\n",
    "    def act_eps_greedy(self,s1):\n",
    "        \n",
    "        self.image_buff.append(s1)\n",
    "        ret_action = np.zeros((N_ACTION,))\n",
    "        \n",
    "        if not len(self.image_buff) == N_ADV + 1:\n",
    "            buff = self.image_buff + [np.zeros_like(s1) for _ in range(N_ADV - len(self.image_buff))]\n",
    "        else:\n",
    "            self.image_buff.pop(0)\n",
    "            buff = self.image_buff\n",
    "            \n",
    "#         print(self.name,[np.mean(s) for s in buff])\n",
    "\n",
    "        eps = self.calc_eps_time()\n",
    "\n",
    "        if random.random() > eps:\n",
    "            a_idx = self.q_network.predict_best_action(buff)\n",
    "        else:\n",
    "            a_idx = random.randint(0,N_ACTION-1)\n",
    "                \n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "    \n",
    "    def act_greedy(self,s1):\n",
    "\n",
    "        self.image_buff.append(s1)\n",
    "        ret_action = np.zeros((N_ACTION,))\n",
    "        if len(self.image_buff) == N_ADV + 1:\n",
    "            eps = self.calc_eps_time()\n",
    "\n",
    "            self.image_buff.pop(0)\n",
    "            \n",
    "            a_idx = self.q_network.predict_best_action(self.image_buff)\n",
    "        else:\n",
    "            a_idx = randint(0,N_ACTION-1)\n",
    "\n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "    \n",
    "    def push_advantage(self,s1_,a_,r_,s2_,isterminal,isdemo):\n",
    "        self.memory.append((s1_,a_,r_,s2_,isdemo))\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.memory = []\n",
    "    \n",
    "    def push_to_batch(self, s1, action, s2, reward, reward_adv, isdemo):\n",
    "        self.batch['s1'].append(s1)\n",
    "        self.batch['action'].append(action)\n",
    "        self.batch['s2'].append(s2)\n",
    "        self.batch['reward'].append(reward)\n",
    "        self.batch['reward_adv'].append(reward_adv)\n",
    "        self.batch['isdemo'].append(isdemo)\n",
    "        return 0\n",
    "    \n",
    "    def clear_batch(self):\n",
    "        self.batch = {'s1':[], 'action':[], 's2':[] ,'reward':[], 'reward_adv':[], 'isdemo':[]}\n",
    "        return 0\n",
    "    \n",
    "    def make_batch_learn(self):\n",
    "        n = len(self.batch['action'])\n",
    "        s1 = np.zeros((n, N_ADV,)+RESOLUTION)\n",
    "        s2 = np.zeros((n, N_ADV,)+RESOLUTION)\n",
    "        for i in range(n):\n",
    "            s1[i, :n - i] = self.batch['s1'][:n - i]\n",
    "            s2[i, :n - i] = self.batch['s2'][:n - i]\n",
    "        \n",
    "        self.s1_record = s1[0:1]\n",
    "        \n",
    "        self.loss_one_record, self.loss_adv_record, self.loss_class_record = \\\n",
    "        self.q_network.update_parameter_server_batch(s1, self.batch['action'], self.batch['reward'], \\\n",
    "                                                         self.batch['reward_adv'], s2, self.batch['isdemo']) \n",
    "        return 0\n",
    "    \n",
    "    def learn_advantage(self, isterminal):\n",
    "        \n",
    "        if len(self.memory)==N_ADV or isterminal:\n",
    "            tail_idx = len(self.memory)-1\n",
    "            \n",
    "            s1_buff = np.zeros((N_ADV, )+RESOLUTION)\n",
    "            for i in range(tail_idx+1):\n",
    "                s1_buff[i] = self.memory[i][0]\n",
    "            \n",
    "            for i in range(tail_idx,-1,-1):\n",
    "                s1,a,r,s2,d = self.memory[i]\n",
    "                if i==tail_idx:\n",
    "                    if not isterminal:\n",
    "#                         print(np.max(self.q_network.get_q_value(s1)[0]))\n",
    "                        self.R = np.max(self.q_network.get_q_value(s1_buff)[0])\n",
    "                        \n",
    "                    else:\n",
    "                        self.R = 0\n",
    "                else:\n",
    "                    self.R =  r + GAMMA*self.R\n",
    "            \n",
    "#                 self.q_network.train_push(s1,a,r,self.R,s2,d)\n",
    "                self.push_to_batch(s1,a,s2,r,self.R,d)\n",
    "            \n",
    "#             self.q_network.update_parameter_server()\n",
    "#             self.q_network.update_parameter_server_batch(self.batch['s1'], self.batch['action'], self.batch['reward'], \\\n",
    "#                                                          self.batch['reward_adv'], self.batch['s2'], self.batch['isdemo'])\n",
    "\n",
    "#             print(np.shape(self.batch['s1']))\n",
    "#             print(np.shape(self.batch['s2']))\n",
    "#             print(np.shape(self.batch['s2']))\n",
    "            self.make_batch_learn()\n",
    "            self.q_network.copy_learn2target()\n",
    "            self.R = 0\n",
    "            self.clear_memory()\n",
    "            self.clear_batch()\n",
    "            \n",
    "#             return self.q_network.calc_loss([s1],[a],[r],[self.R],[s2],[d])\n",
    "#         return 0.0,0.0,0.0\n",
    "    \n",
    "    def calc_loss(self):\n",
    "        \n",
    "        if len(self.memory) == N_ADV :\n",
    "            tail_idx = len(self.memory) - 1\n",
    "            s1_buff = np.ones((1, tail_idx+1, )+RESOLUTION) * np.nan\n",
    "            s2_buff = np.ones((1, tail_idx+1, )+RESOLUTION) * np.nan\n",
    "            for i in range(tail_idx+1):\n",
    "                s1_buff[0, i] = self.memory[i][0]\n",
    "                s2_buff[0, i] = self.memory[i][3]\n",
    "            \n",
    "            for i in range(tail_idx, -1, -1):\n",
    "                s1 , a, r, s2, d = self.memory[i]\n",
    "                if i == tail_idx :\n",
    "                    R = np.max(self.q_network.get_q_value(s1_buff)[0])\n",
    "                else:\n",
    "                    R = r * GAMMA * R\n",
    "                \n",
    "                _, last_action, last_r, _, last_d = self.memory[tail_idx]\n",
    "                \n",
    "                return [s1_buff] + self.q_network.calc_loss(s1_buff, [last_action], [last_r], [R] ,s2_buff ,[last_d])\n",
    "        \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def encode(pre_layer):\n",
    "        s = tf.shape(pre_layer)\n",
    "        return tf.reshape(pre_layer, shape=(-1,)+RESOLUTION)\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [1,6,6]\n",
    "        stride = [1,3,3]\n",
    "#         kernel_size = [6,6]\n",
    "#         stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "#         weights_init = tf.constant_initializer(2.0)\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,\\\n",
    "                                        biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "#         return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 64\n",
    "        kernel_size = [1,3,3]\n",
    "        stride = [1,2,2]\n",
    "#         kernel_size = [3,3]\n",
    "#         stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "#         return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        print(pre_layer)\n",
    "#         return tf.contrib.layers.flatten(pre_layer)\n",
    "        a = tf.shape(pre_layer)[1]\n",
    "        b = tf.shape(pre_layer)[2]\n",
    "        c = tf.shape(pre_layer)[3]\n",
    "        d = tf.shape(pre_layer)[4]\n",
    "        print(a,\",\",b,\",\",c,\",\",d)\n",
    "        return tf.reshape(pre_layer, shape=(-1, N_ADV * 2560))\n",
    "        \n",
    "    def fc1(pre_layer):\n",
    "        print((pre_layer))\n",
    "        num_outputs = 512\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def decode(pre_layer):\n",
    "        return tf.reshape(pre_layer, shape=(-1, N_ADV,512))\n",
    "    \n",
    "    def lstm(pre_layer, state):\n",
    "        batch_size = tf.shape(pre_layer)[0]\n",
    "        print(pre_layer)\n",
    "        temp = tf.reduce_max(state, axis=4)\n",
    "        temp = tf.reduce_max(temp, axis=3)\n",
    "        temp = tf.reduce_max(temp, axis=2)\n",
    "        lengh = tf.cast(tf.reduce_sum(tf.sign(temp) , axis=1),dtype=tf.int32) \n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(LSTM_SIZE)\n",
    "        rnn_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        rnn_out, state_out = tf.nn.dynamic_rnn(cell, pre_layer, initial_state=rnn_state, sequence_length=lengh,dtype=tf.float32)\n",
    "        out_idx = tf.range(0, batch_size) * N_ADV + (lengh  -1)\n",
    "        output = tf.gather(tf.reshape(rnn_out, [-1, LSTM_SIZE]), out_idx)\n",
    "        return output, lengh, rnn_out\n",
    "    \n",
    "    def q_value(pre_layer):\n",
    "        num_outputs = N_ACTION\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network which be shared in global\n",
    "class ParameterServer:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,N_ADV)+RESOLUTION, name=\"state1\")\n",
    "        self.a_ = tf.placeholder(tf.int32, shape=(None,), name=\"action\")\n",
    "        self.r_ = tf.placeholder(tf.float32, shape=(None,), name=\"reward\")\n",
    "        self.r_adv = tf.placeholder(tf.float32, shape=(None,), name=\"reward_adv\")\n",
    "        self.mergin_value = tf.placeholder(tf.float32,shape=(None,N_ACTION), name=\"mergin_value\")\n",
    "#         self.s1idx_ = tf.placeholder(tf.int32, shape=(None,), name=\"lengh_of_state\")\n",
    "        \n",
    "        with tf.variable_scope(\"parameter_server\",reuse=tf.AUTO_REUSE):      # スレッド名で重み変数に名前を与え、識別します（Name Space）\n",
    "            with tf.device(\"/gpu:0\"):\n",
    "                self.model = self._build_model()            # ニューラルネットワークの形を決定\n",
    "            \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "#         self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)    # loss関数を最小化していくoptimizerの定義です\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        with tf.variable_scope(\"summary\"):\n",
    "            self._build_summary()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        print(\"-------GLOBAL-------\")\n",
    "        for w in self.weights_params:\n",
    "            print(w)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \n",
    "#         self.enco = NetworkSetting.encode(self.state1_)\n",
    "        self.conv1 = NetworkSetting.conv1(self.state1_)\n",
    "        maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        self.conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "#         self.deco = NetworkSetting.decode(fc1)\n",
    "#         rnn, l, _ = NetworkSetting.lstm(self.deco, self.state1_)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "                \n",
    "        print(\"---------MODEL SHAPE-------------\")\n",
    "        print(self.state1_.get_shape())\n",
    "        print(self.conv1.get_shape())\n",
    "        print(self.conv2.get_shape())\n",
    "        print(reshape.get_shape())\n",
    "        print(fc1.get_shape())\n",
    "        print(q_value.get_shape())\n",
    "            \n",
    "        return q_value\n",
    "                \n",
    "    def _build_summary(self):\n",
    "        \n",
    "        self.loss_one = tf.placeholder(tf.float32,shape=())\n",
    "        self.loss_n = tf.placeholder(tf.float32,shape=())\n",
    "        self.loss_c = tf.placeholder(tf.float32,shape=())\n",
    "        self.loss_l = tf.placeholder(tf.float32,shape=())\n",
    "        \n",
    "        self.reward = tf.placeholder(tf.float32,shape=())\n",
    "        self.frag = tf.placeholder(tf.int64,shape=())\n",
    "        self.death = tf.placeholder(tf.int64,shape=())\n",
    "        \n",
    "        summary_lo = tf.summary.scalar('loss_one',self.loss_one, family='loss')\n",
    "        summary_ln = tf.summary.scalar('loss_nstep', self.loss_n, family='loss')\n",
    "        summary_lc = tf.summary.scalar('loss_class', self.loss_c, family='loss')\n",
    "        summary_ll = tf.summary.scalar('loss_l2',self.loss_l, family='loss')\n",
    "\n",
    "        self.merged_loss = tf.summary.merge([summary_lo,summary_ln,summary_lc,summary_ll])\n",
    "        \n",
    "        conv1_display = tf.expand_dims(tf.transpose(self.conv1, perm=[0,1,4,2,3]), axis=5)\n",
    "        conv2_display = tf.expand_dims(tf.transpose(self.conv2, perm=[0,1,4,2,3]), axis=5)\n",
    "\n",
    "        state_shape = self.state1_.get_shape()\n",
    "        conv1_shape = conv1_display.get_shape()\n",
    "        conv2_shape = conv2_display.get_shape()\n",
    "        print(\"conv1_shape:\", conv1_shape)\n",
    "        print(\"conv2_shape:\",conv2_shape)\n",
    "        summary_state  = tf.summary.image('state',tf.reshape(self.state1_,[-1,state_shape[2], state_shape[3], state_shape[4]]),max_outputs = 1)\n",
    "        summary_conv1 = tf.summary.image('conv1',tf.reshape(conv1_display,[-1, conv1_shape[3], conv1_shape[4], conv1_shape[5]]),max_outputs = 1)\n",
    "        summary_conv2 = tf.summary.image('conv2',tf.reshape(conv2_display,[-1, conv2_shape[3], conv2_shape[4], conv2_shape[5]]),max_outputs = 1)\n",
    "\n",
    "        self.merged_image = tf.summary.merge([summary_state,summary_conv1,summary_conv2])\n",
    "        \n",
    "        summary_reward = tf.summary.scalar('reward',self.reward)\n",
    "        summary_frag = tf.summary.scalar('frag',self.frag)\n",
    "        summary_death = tf.summary.scalar('death',self.death)\n",
    "        \n",
    "        self.merged_testscore = tf.summary.merge([summary_reward,summary_frag,summary_death])\n",
    "        \n",
    "        self.merged_weights = tf.summary.merge([tf.summary.scalar(self.weights_params[i].name,tf.reduce_mean(self.weights_params[i]), family='weights') for i in range(len(self.weights_params))])\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(LOG_DIR,SESS.graph)\n",
    "\n",
    "    # write summary about LOSS and IMAGE\n",
    "    def write_loss(self,step,loss_one,loss_n,loss_class,loss_l2):\n",
    "            m = SESS.run(self.merged_loss,feed_dict= \\\n",
    "                               {self.loss_one:loss_one,self.loss_n:loss_n,self.loss_c:loss_class,self.loss_l:loss_l2})\n",
    "            self.writer.add_summary(m, step)\n",
    "            return 0\n",
    "                \n",
    "    def write_images(self, step, s1):\n",
    "        m = SESS.run(self.merged_image, {self.state1_: s1})\n",
    "        self.writer.add_summary(m, step)\n",
    "        return 0\n",
    "    \n",
    "    def write_records(self,step,r,f,d):\n",
    "        m = SESS.run(self.merged_testscore,feed_dict={self.reward:r,self.frag:f,self.death:d})\n",
    "        self.writer.add_summary(m,step)\n",
    "        \n",
    "    def write_weights(self, step):\n",
    "        m = SESS.run(self.merged_weights)\n",
    "        self.writer.add_summary(m, step)\n",
    "        return 0\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        self.saver.save(SESS, model_path+\"/model.ckpt\")\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.saver.restore(SESS, model_path+\"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name,parameter_server):\n",
    "        self.name = name\n",
    "\n",
    "        self.s1 = np.zeros(shape=(120,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.s2 = np.zeros(shape=(120,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.reward = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.reward_adv = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.action = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.isdemo = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.queue_pointer = 0\n",
    "        \n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,N_ADV,)+RESOLUTION, name=\"A\")\n",
    "        self.state2_ = tf.placeholder(tf.float32,shape=(None,N_ADV,)+RESOLUTION, name=\"B\")\n",
    "        self.a_ = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.r_ = tf.placeholder(tf.float32, shape=(None,))\n",
    "        self.r_adv = tf.placeholder(tf.float32, shape=(None,))\n",
    "        self.isdemo_ = tf.placeholder(tf.float32,shape=(None,))\n",
    "        self.mergin_value = tf.placeholder(tf.float32,shape=(None,N_ACTION))\n",
    "        self.is_weight_ = tf.placeholder(tf.float32, shape=(None,))\n",
    "#         self.s1idx_ = tf.placeholder(tf.int32, shape = (None,))\n",
    "        \n",
    "        with tf.variable_scope(self.name+\"_target\", reuse=tf.AUTO_REUSE):\n",
    "            self.model_t, self.len_s2 = self._model(self.state2_)\n",
    "        with tf.variable_scope(self.name+\"_train\"):\n",
    "            self.model_l, self.len_s1 = self._model(self.state1_)\n",
    "\n",
    "        self._build_graph(parameter_server)\n",
    "            \n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self,state):\n",
    "        \n",
    "#         enco = NetworkSetting.encode(state)\n",
    "        conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "#         deco = NetworkSetting.decode(fc1)\n",
    "#         rnn, lengh, _ = NetworkSetting.lstm(deco, state)\n",
    "#         self.deco = NetworkSetting.decode(fc1)\n",
    "#         self.rnn, lengh, _ = NetworkSetting.lstm(self.deco, state)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "        \n",
    "        return q_value, 0\n",
    "\n",
    "    def _build_graph(self,parameter_server):\n",
    "        \n",
    "#         self.best_action = tf.argmax(self.model_l, axis=0)\n",
    "        self.prob_action = tf.nn.softmax(self.model_l, axis=1)\n",
    "\n",
    "        q_model_t = tf.where(tf.equal(self.len_s2, self.len_s1) , self.model_t,tf.zeros_like(self.model_t))\n",
    "        self.test1 = q_model_t\n",
    "        \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"_train\")\n",
    "        self.weights_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"_target\")\n",
    "        self.copy_params = [t.assign(l) for l,t in zip(self.weights_params, self.weights_params_target)]\n",
    "        \n",
    "#         self.loss_one = tf.square(tf.stop_gradient(self.r_ + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "#         self.loss_adv = tf.square(tf.stop_gradient(self.r_adv + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "        self.loss_one = tf.abs(tf.stop_gradient(self.r_ + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "        self.loss_adv = LAMBDA1 * tf.abs(tf.stop_gradient(self.r_adv + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "        target = tf.stop_gradient(tf.reduce_max(self.model_l + self.mergin_value))\n",
    "        idx = tf.transpose([tf.range(tf.shape(self.model_l)[0]), self.a_])\n",
    "        self.loss_class =  LAMBDA2 * (target- tf.gather_nd(self.model_l,indices=idx)) * self.isdemo_\n",
    "        self.loss_l2 = LAMBDA3 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params])\n",
    "        \n",
    "        self.loss_total = (self.loss_one +  self.loss_adv + self.loss_class + self.loss_l2) * self.is_weight_\n",
    "        \n",
    "        self.grads = tf.gradients(self.loss_total ,self.weights_params)\n",
    "        \n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))\n",
    "\n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params,parameter_server.weights_params)]\n",
    "\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params)]\n",
    "    \n",
    "    def pull_parameter_server(self):\n",
    "        SESS.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self):\n",
    "        SESS.run(self.push_local_weight_params)\n",
    "        \n",
    "    def show_weights(self):\n",
    "        hoge = SESS.run(self.weights_params)\n",
    "        for i in range(len(hoge)):\n",
    "            print(hoge[i])\n",
    "    \n",
    "    def update_parameter_server_batch(self, s1, a, r, r_adv, s2, isdemo, is_weight):\n",
    "        if np.ndim(s1) == 4:\n",
    "            s1 = np.array([s1])\n",
    "        if np.ndim(s2) == 4:\n",
    "            s2 = np.array([s2])\n",
    "        mergin = [[0.8*(not(a[j]==i)) for i in range(N_ACTION)] for j in range(np.shape(a)[0])]\n",
    "\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_:r,self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin,self.isdemo_:isdemo, self.is_weight_: is_weight}\n",
    "        val = SESS.run([self.update_global_weight_params,self.loss_one, self.loss_adv, self.loss_class, self.loss_l2],feed_dict)\n",
    "        return val[1], val[2], val[3], val[4]\n",
    "\n",
    "        \n",
    "    def update_parameter_server(self):\n",
    "        if self.queue_pointer > 0:\n",
    "            s1 = np.ones((self.queue_pointer, N_ADV, )+RESOLUTION) * np.nan\n",
    "            s2 = np.ones((self.queue_pointer, N_ADV, )+RESOLUTION) * np.nan\n",
    "            for i in range(self.queue_pointer):\n",
    "                s1[i, 0:i] = self.s1[0:i]\n",
    "                s2[i, 0:i] = self.s2[0:i]\n",
    "            r = self.reward[0:self.queue_pointer]\n",
    "            a = self.action[0:self.queue_pointer]\n",
    "            r_adv = self.reward_adv[0:self.queue_pointer]\n",
    "            mergin = [[0.8*(not(a[j]==i)) for i in range(N_ACTION)] for j in range(self.queue_pointer)]\n",
    "            isdemo = self.isdemo[0:self.queue_pointer]\n",
    "            \n",
    "            feed_dict = {self.state1_: s1, self.a_:a, self.r_:r, self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin, self.isdemo_:isdemo}\n",
    "#             _, l, m_l, m_t = SESS.run([self.update_global_weight_params, self.loss_total, self.model_l, self.model_t],feed_dict)\n",
    "            SESS.run(self.update_global_weight_params,feed_dict)\n",
    "            self.queue_pointer = 0\n",
    "            \n",
    "    def predict_best_action(self, s1):\n",
    "        if np.ndim(s1)==4:\n",
    "            s1 = np.array([s1])\n",
    "        \n",
    "#         print(SESS.run(self.model_l, {self.state1_:s1}))\n",
    "#         return SESS.run(self.best_action,{self.state1_:s1})\n",
    "\n",
    "        probs = SESS.run(self.prob_action, {self.state1_:s1})\n",
    "#         print(probs)\n",
    "\n",
    "        return [np.random.choice(N_ACTION, p=p) for p in probs]\n",
    "\n",
    "    def get_q_value(self,s1):\n",
    "        if np.ndim(s1)==4:\n",
    "            s1 = np.array([s1])\n",
    "            \n",
    "        return SESS.run(self.model_l,{self.state1_:s1})\n",
    "    \n",
    "    def calc_loss(self, s1, a, r, r_adv, s2, isdemo):\n",
    "        mergin = [[0.8*(not(a[j]==i)) for i in range(N_ACTION)] for j in range(len(a))]\n",
    "        s\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_:r,self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin,self.isdemo_:isdemo}\n",
    "        return SESS.run([self.loss_one, self.loss_adv, self.loss_class],feed_dict)\n",
    "    \n",
    "    def copy_learn2target(self):\n",
    "        SESS.run(self.copy_params)\n",
    "\n",
    "    def train_push(self,s1,a,r,r_adv,s2,isdemo):\n",
    "        # Push obs to make batch\n",
    "        self.s1[self.queue_pointer] = s1\n",
    "        self.s2[self.queue_pointer] = s2\n",
    "        self.action[self.queue_pointer] = a\n",
    "        self.reward[self.queue_pointer] = r\n",
    "        self.reward_adv[self.queue_pointer] = r_adv\n",
    "        self.isdemo[self.queue_pointer] = isdemo\n",
    "        self.queue_pointer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning():\n",
    "    global start_time_async, start_time_pre, runout\n",
    "    \n",
    "    replay_memory = ReplayMemory(CAPACITY,permanent_data=300)\n",
    "    \n",
    "    threads = []\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        parameter_server = ParameterServer()\n",
    "\n",
    "        for i in range(N_WORKERS):            \n",
    "            threads.append(WorkerThread(\"learning_\"+str(i),parameter_server, replay_memory))\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        pre_env = Environment(\"pre_env\",parameter_server, replay_memory)\n",
    "        test_env = Environment(\"test_env\", parameter_server,replay_memory)\n",
    "\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "    threads[0].environment.summary=True\n",
    "\n",
    "    time.sleep(5.0)\n",
    "\n",
    "    print(\"---LOADING DEMO---\")\n",
    "    pre_env.load_demonstration()\n",
    "    print(\"---PRE LEARNING---\")\n",
    "    start_time_pre = datetime.datetime.now()\n",
    "    pre_env.run_pre_learning()\n",
    "    \n",
    "    if SAVE_FILE == True:\n",
    "        print(\"---SAVING_MODEL---\")\n",
    "        parameter_server.save_model(PREMODEL_PATH)\n",
    "        print(\"---SAVING GIF---\")\n",
    "        test_env.run_test(True, gif_path=PREGIF_PATH)\n",
    "\n",
    "    print(\"---MULTI THREAD LEARNING---\")\n",
    "    start_time_async = datetime.datetime.now()\n",
    "    for worker in threads:\n",
    "        job = lambda: worker.run()      # この辺は、マルチスレッドを走らせる作法だと思って良い\n",
    "        t = threading.Thread(target=job)\n",
    "        t.start()\n",
    "\n",
    "    test_frame = 0\n",
    "    while True:\n",
    "        if frames >= test_frame and frames<test_frame+1000:\n",
    "            r,f,d = test_env.run_test()\n",
    "            if SAVE_FILE == True:\n",
    "                parameter_server.write_weights(frames)\n",
    "                parameter_server.write_records(frames,r,f,d)\n",
    "            test_frame += 1000\n",
    "        elif frames >= test_frame+1000:\n",
    "            print(\"TEST at %d~%d step cant be finished\"%(test_frame, test_frame+1000-1))\n",
    "            test_frame += 1000\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if datetime.datetime.now() > TIME_LEARN + start_time_async:\n",
    "            runout = True\n",
    "            break\n",
    "    print(\"*****************************\\nTIME to PRE LEARNING:%.3f [sec]\\n*****************************\"%(datetime.datetime.now()-start_time_pre).seconds)\n",
    "    print(\"*****************************\\nTIME to ASYNC LEARNING:%.3f [sec]\\n*****************************\"%(datetime.datetime.now()-start_time_async).seconds)\n",
    "\n",
    "    print(\"---LEARNING PHASE IS FINISHED---\")\n",
    "    test_env.run_test()\n",
    "\n",
    "    if SAVE_FILE == True:\n",
    "        print(\"---SAVING_MODEL---\")\n",
    "        parameter_server.save_model(MODEL_PATH)\n",
    "        print(\"---SAVING GIF---\")\n",
    "        test_env.run_test(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        parameter_server = ParameterServer()\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        test_env = Environment(\"test_env\", parameter_server)\n",
    "\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "    parameter_server.load_model()\n",
    "\n",
    "    test_env.run_test(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = 0\n",
    "runout = False\n",
    "current_time = 0\n",
    "start_time_async = 0\n",
    "\n",
    "config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "SESS = tf.Session(config=config)\n",
    "\n",
    "learning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
