
from multiprocessing import Process, Queue, Pipe, Value, Array, Manager, Pool , TimeoutError
import os, time
import tensorflow as tf

class Environment(object):
    def __init__(self,name, parameter_server,replay_memory, summary=False):
        
        self.name = name
        self.session = tf.Session()
        self.x_ = tf.placeholder(tf.float32, (2,))
#         self.a_ = tf.placeholder(tf.float32, (2,))
#         self.A = tf.Variable([0.0,0.0])
#         self.assign = tf.assign(self.A, self.a_)
#         self.session.run(tf.global_variables_initializer())
#         self.game = GameInstance(DoomGame(), name=self.name, config_file_path=CONFIG_FILE_PATH, rewards=REWARDS,n_adv=N_ADV)
        
#         self.network = NetworkLocal(name, parameter_server)
#         self.agent = Agent(self.network)
#         self.replay_memory = replay_memory
        
#         self.local_step = 0
        
#         self.finished = False
        
#         self.summary = summary
#         self.parameter_server = parameter_server
        
#         self.log_buff = [np.zeros(shape=(N_ADV,) + RESOLUTION, dtype=np.float32), \
#                          np.zeros(shape=(N_ADV,), dtype=np.int8), \
#                          np.zeros(shape=(N_ADV,) +  RESOLUTION, dtype=np.float32), \
#                          np.zeros(shape=(N_ADV,), dtype=np.float32), \
#                          np.ones(shape=(N_ADV,), dtype=np.int8), \
#                          np.zeros(shape=(N_ADV,), dtype=np.int8)]
#         self.buff_pointer = 0
        
#     def preprocess(self,img):
#         if len(img.shape) == 3 and img.shape[0]==3:
#             img = img.transpose(1,2,0)
        
#         img = skimage.transform.resize(img, RESOLUTION, mode="constant")
#         img = img.astype(np.float32)
# #         img = (img)/255.0
#         return img

    def tes(self, q, x, y):
#         ans = self.session.run(tf.square(self.A[0]) + tf.square(self.A[1]))
        ans = self.session.run(tf.square(self.x_[0]) + tf.square(self.x_[1]), {self.x_:[1,2]})
        print(ans)
        q.put(ans)
        
    def test2(self, a):
        _,ans = self.session.run([self.assign, self.A], {self.a_:a})
        print(self.name,":", ans)

#     def run(self):
#         global current_time, frames, start_time_async, N_EPISODES
        
#         step = 0
#         while True:
#             self.network.pull_parameter_server()

#             if len(self.replay_memory) > BATCH_SIZE:
#                 tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight, a_onehot = self.make_batch()
#                 l_one, l_adv, l_cls, l_l2 = self.network.update_parameter_server_batch(s1,actions,rewards,rewards_adv,s2,isdemos, is_weight, a_onehot, isterminals)
#                 if step%RECORD_INTERVAL == 0 and SAVE_FILE == True and self.summary == True:
#                     self.parameter_server.write_eps(frames, float(self.agent.calc_eps_time()))
#                     self.parameter_server.write_weights(frames)
#                     self.parameter_server.write_loss(frames, np.mean(l_one), np.mean(l_adv), np.mean(l_cls), l_l2)
#                     if step % (RECORD_INTERVAL*10) == 0:
#                         self.parameter_server.write_images(frames, s1[0:1])

#             s1_ = self.preprocess(self.game.get_screen_buff())
#             action = self.agent.act_eps_greedy(s1_)
#             r,_ = self.game.make_action(action, FRAME_REPEAT)
# #             self.replay_memory.batch_update(tree_idx, l_cls + 0.01, isdemos)

#             if step%FREQ_COPY==0:
#                 self.network.copy_learn2target()
            
#             s2_ = self.preprocess(self.game.get_screen_buff()) if not self.game.is_episode_finished() else np.zeros_like(s1_)
            
#             self.add_buff(s1_, action.index(1), s2_, r, self.game.is_episode_finished(), False)
#             self.buff_pointer += 1
            
#             if (self.game.is_player_dead()):
#                 self.game.respawn_player()
            
#             if self.game.is_episode_finished():
#                 N_EPISODES += 1
#                 self.game.new_episode(BOTS_NUM)
#                 while(self.buff_pointer < N_ADV):
#                     self.add_buff(np.zeros_like(s1_), -1, np.zeros_like(s1_), 0, True, False)
#                     self.buff_pointer += 1
#                 self.replay_memory.store(self.log_buff)
#                 self.clear_buff()
#                 self.agent.clear_memory()
                
#             if self.buff_pointer == N_ADV:
#                 self.replay_memory.store(self.log_buff)
#                 self.clear_buff()

#             step += 1
#             frames += 1
            
#             current_time = datetime.datetime.now().timestamp() - start_time_async.timestamp()
            
#             if runout == True:
#                 self.finished = True
#                 break

#         return 0
                
#     def make_batch(self):
        
#         tree_idx, batch, is_weight = self.replay_memory.sample(BATCH_SIZE)
# #         tree_idx,batch,is_weight = self.replay_memory.sample_uniform(BATCH_SIZE)

#         s1 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)
#         s2 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)
#         actions = np.zeros((BATCH_SIZE ,),dtype=np.int8)
#         rewards = np.zeros((BATCH_SIZE, ),dtype=np.float32)
#         rewards_adv = np.zeros((BATCH_SIZE,),dtype=np.float32)
#         isterminals = np.zeros((BATCH_SIZE, ),dtype=np.int8)
#         isdemos = np.zeros((BATCH_SIZE,),dtype=np.int8)
#         a_onehot = np.zeros((BATCH_SIZE, N_AGENT_ACTION,), dtype=np.int8)
        
#         for i in range(BATCH_SIZE):
#             isterminal = (batch[i][4] == 1)
#             s1[i] = batch[i][0]
#             s2[i] = batch[i][2]
#             isterminals[i] = 1 if isterminal.any() else 0
#             actions[i] = batch[i][1][isterminal][0] if isterminal.any() else batch[i][1][-1]
#             rewards[i] = batch[i][3][isterminal][0] if isterminal.any() else batch[i][3][-1]
#             isdemos[i] = batch[i][5][-1]
#             rewards_adv[i] = sum([r * GAMMA**j for j,r in zip(range(len(batch[i][3])-1,0,-1), batch[i][3])])
#             a_onehot[i][actions[i]] = 1
            
#         return tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight, a_onehot
    
#     def run_test(self, global_step, gif_buff=None reward_buff=None, show_result=True):
        
#         self.game.new_episode(BOTS_NUM)
        
#         #Copy params from global
#         self.network.pull_parameter_server()

#         step = 0
#         gif_img = []
#         total_reward = 0
#         total_detail = {}
#         while not self.game.is_episode_finished():
#             s1_row = self.game.get_screen_buff()
#             s1 = self.preprocess(s1_row)
#             if gif_buff is not None:
#                 gif_img.append(s1_row.transpose(1,2,0))
#             action = self.agent.act_greedy(s1)
#             engine_action = self.convert_action_agent2engine(action.index(1))
#             reward,reward_detail = self.game.make_action(engine_action,FRAME_REPEAT)
#             isterminal = self.game.is_episode_finished()
#             total_reward += reward
#             for k in reward_detail.keys():
#                 if not k in total_detail.keys():
#                     total_detail[k] = reward_detail[k]
#                 else:
#                     total_detail[k] += reward_detail[k]
#             step += 1
#             if reward_buff is not None:
#                 reward_buff.append((engine_action, reward))
            
#             if (self.game.is_player_dead()):
#                 self.game.respawn_player()
        
#         save_img = []
#         if gif_buff is not None:
#             print(np.shape(gif_img))
#             for i in range(len(gif_img)):
#                 save_img.append(Image.fromarray(np.uint8(gif_img[i])))
#             gif_buff += save_img
# #             save_img[0].save(GIF_PATH,save_all=True,append_images=save_img[1:])
#         if show_result == True:
#             print("----------TEST at %d step-------------"%(global_step))
#             print("FRAG:",self.game.get_frag_count(), "DEATH:",self.game.get_death_count())
#             print("REWARD",total_reward)
#             print(total_detail)
#         return total_reward, self.game.get_frag_count(), self.game.get_death_count()

#     def convert_action_engine2agent(self,engine_action):
#         assert type(engine_action) == type(list()), print("type: ", type(engine_action))

#         ans = 0
#         for i, e_a in enumerate(engine_action):
#             ans += e_a * 2**i

#         return ans
    
#     def convert_action_agent2engine(self,agent_action):
#         assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print("type(agent_action)=",type(agent_action))
#         ans = []
#         for i in range(6):
#             ans.append(agent_action%2)
#             agent_action = int(agent_action / 2)
#         return ans
    
#     def add_buff(self, s1,a,s2,r,isterminal, isdemo):
#         self.log_buff[0][self.buff_pointer] = s1
#         self.log_buff[2][self.buff_pointer] = s2
#         self.log_buff[1][self.buff_pointer] = a
#         self.log_buff[3][self.buff_pointer] = r
#         self.log_buff[4][self.buff_pointer] = isterminal
#         self.log_buff[5][self.buff_pointer] = isdemo
#         return 0
    
#     def clear_buff(self):
#         self.log_buff = [np.zeros(shape=(N_ADV,) + RESOLUTION, dtype=np.float32), \
#                          np.zeros(shape=(N_ADV,), dtype=np.int8), \
#                          np.zeros(shape=(N_ADV,) +  RESOLUTION, dtype=np.float32), \
#                          np.zeros(shape=(N_ADV,), dtype=np.float32), \
#                          np.ones(shape=(N_ADV,), dtype=np.int8), \
#                          np.zeros(shape=(N_ADV,), dtype=np.int8)]
#         self.buff_pointer = 0
#         return 0  

workers = [Environment(name="test_%d"%i, parameter_server=None, replay_memory=None) for i in range(1)]

q = Queue()
arr = Array('d', [1.,2.])

p1 = [Process(target=w.tes, args=(q,1,2)) for w in workers]
# p2 = [Process(target=w.test2, args=(arr,)) for w in workers]

workers[0].tes(q)

q.get()

for p in p1:
    p.start()
    p.join()

p1[0].start()
print(q.get())
p1[0].join()

q.get()
