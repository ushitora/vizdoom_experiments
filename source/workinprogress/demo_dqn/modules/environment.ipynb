{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.color, skimage.transform\n",
    "from vizdoom import *\n",
    "import os, time, random, threading, h5py, math,pickle, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from global_constants import *\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from PIL import Image\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,sess,  name, game_instance, network, agent, replaymemory,start_time=None, end_time=None, n_step=None,  random_seed=0, position_data=None, parameters=None):\n",
    "#     def __init__(self,sess,  name, start_time, end_time, parameter_server):\n",
    "        self.name = name\n",
    "        self.sess = sess\n",
    "        self.parameters = parameters\n",
    "        self.game = game_instance\n",
    "        self.game.game.set_seed(random_seed)\n",
    "        self.game.game.set_render_weapon(True)\n",
    "        self.game.game.set_render_crosshair(True)\n",
    "        self.game.game.set_episode_timeout(500)\n",
    "        self.game.game.init()\n",
    "        self.network = network\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.clear_obs()\n",
    "        self.clear_batch()\n",
    "        \n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.n_step = n_step\n",
    "        self.progress = 0.0\n",
    "        self.log_server = None\n",
    "        \n",
    "        self.replay_memory = replaymemory\n",
    "        \n",
    "        self.step = 0\n",
    "        self.model_gen_count = 0\n",
    "        \n",
    "        self.times_act = None\n",
    "        self.times_update = None\n",
    "        \n",
    "        self.count_update = 0\n",
    "        self.rewards_detail = None\n",
    "        self.position_data_buff = position_data\n",
    "        \n",
    "        self.record_action = []\n",
    "        self.record_treeidx = []\n",
    "        \n",
    "        self.count_idx = np.zeros_like(replaymemory.tree.tree, dtype=np.int32)\n",
    "        print(self.name,\" initialized...\")\n",
    "        \n",
    "    def run_learning(self, coordinator):\n",
    "        print(self.name + \" start learning\")\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        self.game.new_episode()\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                self.learning_step()\n",
    "                if self.n_step is not None:\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "#                 if self.progress >= 1.0:\n",
    "#                     break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(self.name,\" ended\")\n",
    "            \n",
    "#         if self.log_server is not None:\n",
    "#             coordinator.request_stop()\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def run_prelearning(self, coordinator):\n",
    "        assert self.replay_memory is not None\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                loss_values = self.prelearning_step()\n",
    "                if self.n_step is not None:\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "        except Exception as e:\n",
    "            coordinator.request_stop(e)\n",
    "            \n",
    "        coordinator.request_stop()\n",
    "        return 0\n",
    "    \n",
    "    def run_exploring(self, coordinator):\n",
    "        print(self.name + \" start exploring\")\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        self.game.new_episode()\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                self.exploring_step()\n",
    "                if self.n_step is not None:\n",
    "                    if self.step % 1000 == 0:\n",
    "                        print(self.name,\":\", self.step)\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            coordinator.request_stop(e)\n",
    "            \n",
    "        if self.log_server is not None:\n",
    "            coordinator.request_stop()\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def run_test(self, coordinator):\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "        self.network.copy_network_learning2target(self.sess)\n",
    "        try:\n",
    "            while not coordinator.should_stop():\n",
    "                play_log = []\n",
    "                reward,frag, death,kill,total_detail,steps = self.test_agent(reward_buff =play_log)\n",
    "                if self.parameters.save_data == True:\n",
    "                    with open(os.path.join(self.parameters.play_lodir, \"playlog_step%02d.txt\"%int(self.progress*100)), 'wb') as f:\n",
    "                        pickle.dump(play_log, f)\n",
    "                if self.rewards_detail is not None:\n",
    "                    self.rewards_detail.append(total_detail)\n",
    "\n",
    "                if self.log_server is not None:\n",
    "                    if kill <= 0:\n",
    "                        steps = 100\n",
    "                    self.log_server.write_score(self.sess,self.step,  reward, frag, death ,kill, steps, time.process_time())\n",
    "                    self.log_server.write_processtime_score(self.sess, self.step, time.process_time())\n",
    "                    if self.progress >= self.model_gen_count/12:\n",
    "                        self.model_gen_count += 1\n",
    "                        if self.parameters.save_data == True:\n",
    "                            self.log_server.save_model(sess=self.sess, model_path=self.parameters.model_path, step=self.model_gen_count+1)\n",
    "                \n",
    "                self.step += 1\n",
    "                if self.n_step is not None:\n",
    "                    self.progress = self.step/self.n_step\n",
    "                else:\n",
    "                    self.progress = (datetime.now().timestamp() - self.start_time)/(self.end_time - self.start_time)\n",
    "                if self.progress >= 1.0:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(self.name, \"killed \")\n",
    "#             coordinator.request_stop(e)\n",
    "\n",
    "    def learning_step(self):\n",
    "        if self.step % self.parameters.interval_pull_params == 0:\n",
    "            self.network.pull_parameter_server(self.sess)\n",
    "        loss_values = []\n",
    "        if not self.game.is_episode_finished() and self.game.get_screen_buff() is not None:\n",
    "            \n",
    "            if self.times_act is not None:\n",
    "                start_time = datetime.now().timestamp()\n",
    "\n",
    "            s1_ = self.preprocess(self.game.get_screen_buff())\n",
    "            self.push_obs(s1_)\n",
    "            agent_action_idx = self.agent.act_eps_greedy(self.sess, self.obs['s1'], self.progress)\n",
    "            self.record_action.append(agent_action_idx)\n",
    "            \n",
    "            if self.position_data_buff is not None:\n",
    "                enemy_label = self.game.get_label(\"Cacodemon\")\n",
    "                if enemy_label is not None:\n",
    "                    center_x = enemy_label.x + enemy_label.width/2\n",
    "                    center_y = enemy_label.y + enemy_label.height/2\n",
    "#                     enemy_position_class = 2\n",
    "                else:\n",
    "                    center_x = 0\n",
    "                    center_y = 0\n",
    "#                     enemy_position_class = 0\n",
    "                player_position_x = self.game.get_pos_x()\n",
    "                player_position_y = self.game.get_pos_y()\n",
    "                player_angle = self.game.get_angle()\n",
    "                self.position_data_buff.append([center_x, center_y, player_position_x, player_position_y, player_angle])\n",
    "            \n",
    "#             engin_action = self.convert_action_agent2engine_simple(agent_action_idx)\n",
    "            engin_action = self.convert_action_agent2engine(agent_action_idx)\n",
    "            r,r_detail = self.game.make_action(self.step,engin_action , self.parameters.frame_repeat)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            if isterminal:\n",
    "                s2_ = np.zeros(self.parameters.resolution)\n",
    "            else:\n",
    "                s2_ = self.preprocess(self.game.get_screen_buff())\n",
    "            \n",
    "            self.push_batch( self.obs['s1'], agent_action_idx, s2_, r , isterminal, False)\n",
    "            \n",
    "            if self.times_act is not None:\n",
    "                self.times_act.append(datetime.now().timestamp() - start_time)\n",
    "            \n",
    "            if len(self.memory) >= self.parameters.n_adv or isterminal:\n",
    "                batch = self.make_advantage_data()\n",
    "                self.clear_batch()\n",
    "                for i,b in enumerate(batch):\n",
    "                    if len(b) == 8:\n",
    "                        self.replay_memory.store(b)\n",
    "            \n",
    "            self.step += 1\n",
    "            \n",
    "            if self.step % self.parameters.interval_update_network == 0:\n",
    "                self.network.copy_network_learning2target(self.sess)\n",
    "                \n",
    "            if self.times_update is not None:\n",
    "                start_time = datetime.now().timestamp()\n",
    "            \n",
    "            if self.step % self.parameters.interval_batch_learning == 0 and len(self.replay_memory) >= self.parameters.n_batch:\n",
    "                s1, actions, r_one, r_adv, isdemo, is_weight, tree_idx = self.make_batch()\n",
    "                self.record_treeidx.append(tree_idx)\n",
    "                if self.log_server is not None:\n",
    "                    self.count_idx[tree_idx] += 1\n",
    "                loss_values = self.network.update_parameter_server(self.sess, s1, actions, r_one, r_adv, isdemo, is_weight)\n",
    "                self.count_update += 1\n",
    "                tderror = loss_values[4]\n",
    "                l_one, l_n, l_m, l_l = loss_values[:-1]\n",
    "                if self.log_server is not None:\n",
    "                    self.log_server.write_loss(self.sess,self.step ,np.mean(l_one), np.mean(l_n), np.mean(l_m), l_l,time.process_time())\n",
    "                    self.log_server.write_weights(self.sess, self.step)\n",
    "                self.replay_memory.batch_update_new(tree_idx, np.copy(l_one),np.array(r_adv)>0)\n",
    "                    \n",
    "            if self.times_update is not None:\n",
    "                self.times_update.append(datetime.now().timestamp() - start_time)\n",
    "        else:\n",
    "            self.game.new_episode()\n",
    "            self.clear_batch()\n",
    "            self.clear_obs()\n",
    "\n",
    "        return loss_values\n",
    "    \n",
    "    def prelearning_step(self):\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "\n",
    "        s1, actions, r_one, r_adv, isdemo, is_weight, tree_idx = self.make_batch()\n",
    "        loss_values = self.network.update_parameter_server(self.sess, s1, actions, r_one, r_adv, isdemo, is_weight)\n",
    "        tderror = loss_values[4]\n",
    "        l_one, l_n, l_m, l_l = loss_values[:-1]\n",
    "        self.replay_memory.batch_update(tree_idx, tderror)\n",
    "        \n",
    "        if self.step % self.parameters.interval_update_network == 0:\n",
    "            self.network.copy_network_learning2target(self.sess)\n",
    "        \n",
    "        if self.log_server is not None:\n",
    "            if self.step % 10 == 0:\n",
    "                self.log_server.write_loss(self.sess, self.step, np.mean(l_one), np.mean(l_n), np.mean(l_m), l_l,time.process_time())\n",
    "                self.log_server.write_weights(self.sess, self.step)\n",
    "        self.step += 1\n",
    "        return loss_values\n",
    "\n",
    "    def test_agent(self, gif_buff=None, reward_buff=None, sample_imgs=None):\n",
    "        \n",
    "        self.game.new_episode()\n",
    "        self.network.pull_parameter_server(self.sess)\n",
    "\n",
    "        step = 0\n",
    "        gif_img = []\n",
    "        total_reward = 0\n",
    "        total_detail = {}\n",
    "        self.clear_obs()\n",
    "        while not self.game.is_episode_finished():\n",
    "            s1_row = self.game.get_screen_buff()\n",
    "            s1 = self.preprocess(s1_row)\n",
    "            if sample_imgs is not None:\n",
    "                sample_imgs.append(s1)\n",
    "            if gif_buff is not None:\n",
    "                gif_img.append(s1_row.transpose(1,2,0))\n",
    "            self.push_obs(s1)\n",
    "            action = self.agent.act_greedy(self.sess,self.obs['s1'])\n",
    "            engine_action = self.convert_action_agent2engine(action)\n",
    "            reward,reward_detail = self.game.make_action(step,engine_action,self.parameters.frame_repeat)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            total_reward += reward\n",
    "            for k in reward_detail.keys():\n",
    "                if not k in total_detail.keys():\n",
    "                    total_detail[k] = reward_detail[k]\n",
    "                else:\n",
    "                    total_detail[k] += reward_detail[k]\n",
    "            step += 1\n",
    "            if reward_buff is not None:\n",
    "                reward_buff.append((engine_action, reward_detail))\n",
    "        \n",
    "        save_img = []\n",
    "        if gif_buff is not None:\n",
    "            for i in range(len(gif_img)):\n",
    "                save_img.append(Image.fromarray(np.uint8(gif_img[i])))\n",
    "            gif_buff += save_img\n",
    "            \n",
    "        return total_reward, self.game.get_frag_count(), self.game.get_death_count(), self.game.get_kill_count(), total_detail, step\n",
    "        \n",
    "    def convert_action_engine2agent(self,engine_action):\n",
    "#         return engine_action.index(1)\n",
    "        assert type(engine_action) == type(list()), print(\"type: \", type(engine_action))\n",
    "        ans = 0\n",
    "        for i, e_a in enumerate(engine_action):\n",
    "            ans += e_a * 2**i\n",
    "        return ans\n",
    "    \n",
    "    def convert_action_agent2engine(self,agent_action):\n",
    "        assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print(\"type(agent_action)=\",type(agent_action))\n",
    "        ans = []\n",
    "        for i in range(self.parameters.n_action):\n",
    "            ans.append(agent_action%2)\n",
    "            agent_action = int(agent_action / 2)\n",
    "        return ans\n",
    "    \n",
    "    def convert_action_agent2engine_simple(self, agent_action):\n",
    "        assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print(\"type(agent_action)=\",type(agent_action))\n",
    "        ans = np.zeros((self.parameters.n_agent_action,))\n",
    "        ans[agent_action] = 1\n",
    "        return ans.tolist()\n",
    "    \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3 and img.shape[0]==3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, self.parameters.resolution, mode=\"constant\")\n",
    "        img = img.astype(np.float32)\n",
    "#         img = (img)/255.0\n",
    "        return img\n",
    "\n",
    "    def push_obs(self, s1):\n",
    "        self.obs['s1'] = s1\n",
    "        \n",
    "    def clear_obs(self):\n",
    "        self.obs = {}\n",
    "        self.obs['s1'] = np.zeros(self.parameters.resolution, dtype=np.float32)\n",
    "        \n",
    "    def push_batch(self, s1, action,s2,  reward, isterminal, isdemo):\n",
    "        self.memory.append([np.copy(s1), action, np.copy(s2) , reward, isterminal, isdemo])\n",
    "    \n",
    "    def clear_batch(self):\n",
    "        self.memory = []\n",
    "    \n",
    "    def make_advantage_data(self):\n",
    "        len_memory = len(self.memory)\n",
    "        ret_batch = []\n",
    "        R_adv = 0\n",
    "        _,_,s2_adv,_,_,_ = self.memory[-1]\n",
    "        for i in range(len_memory-1, -1, -1):\n",
    "            s1,a,s2,r,isterminal,isdemo = self.memory[i]\n",
    "            R_adv = r + self.parameters.gamma*R_adv\n",
    "            ret_batch.append(np.array([s1, a,s2,s2_adv,r ,R_adv ,isterminal, isdemo]))\n",
    "        \n",
    "        self.memory = []\n",
    "        return ret_batch\n",
    "    \n",
    "    def make_batch(self):\n",
    "        while True:\n",
    "            tree_idx, batch_row, is_weight = self.replay_memory.sample(self.parameters.n_batch, self.calc_beta(self.progress))\n",
    "#             tree_idx, batch_row, is_weight = self.replay_memory.sample(N_BATCH, 0.1)\n",
    "            s2_input = [ batch_row[i,2] for i in range(self.parameters.n_batch)]\n",
    "            s2_adv = [ batch_row[i,3] for i in range(self.parameters.n_batch)]\n",
    "            if (np.shape(s2_input) == ((self.parameters.n_batch,)+self.parameters.resolution) and np.shape(s2_adv) == ((self.parameters.n_batch,)+self.parameters.resolution)):\n",
    "                break\n",
    "        \n",
    "        s1, actions, s2, r_one, r_adv, isdemo = [],[],[],[],[],[]\n",
    "        \n",
    "        predicted_q_adv  = self.network.get_qvalue_max_learningaction(self.sess,s2_adv)\n",
    "        \n",
    "        predicted_q = self.network.get_qvalue_max_learningaction(self.sess,s2_input)\n",
    "        \n",
    "        for i in range(self.parameters.n_batch):\n",
    "            s1.append(batch_row[i][0])\n",
    "            actions.append(batch_row[i][1])\n",
    "            R_one = batch_row[i][4] + self.parameters.gamma * predicted_q[i] if batch_row[i][6] == False else batch_row[i][4]\n",
    "            R_adv = batch_row[i][5] + self.parameters.gamma**self.parameters.n_adv * predicted_q_adv[i] if batch_row[i][6] == False else batch_row[i][5]\n",
    "            r_one.append(R_one)\n",
    "            r_adv.append(R_adv)\n",
    "            isdemo.append(batch_row[i][7])\n",
    "\n",
    "        actions = np.array(actions)\n",
    "        return s1, actions.astype(np.int32), r_one, r_adv, isdemo, is_weight, tree_idx\n",
    "    \n",
    "    def make_batch_uniform(self):\n",
    "        while True:\n",
    "            tree_idx, batch_row, is_weight = self.replay_memory.sample_uniform(N_BATCH)\n",
    "            \n",
    "            s2_input = [ batch_row[i,2] for i in range(self.parameters.n_batch)]\n",
    "            s2_adv = [ batch_row[i,3] for i in range(self.parameters.n_batch)]\n",
    "            if (np.shape(s2_input) == (self.parameters.n_batch,5, 120,120,3) and np.shape(s2_adv) == (self.parameters.n_batch,5, 120,120,3)):\n",
    "                break\n",
    "        \n",
    "        s1, actions, s2, r_one, r_adv, isdemo = [],[],[],[],[],[]\n",
    "        \n",
    "        predicted_q_adv  = self.network.get_qvalue_max_learningaction(self.sess,s2_adv)\n",
    "        \n",
    "        predicted_q = self.network.get_qvalue_max_learningaction(self.sess,s2_input)\n",
    "        \n",
    "        for i in range(self.parameters.n_batch):\n",
    "            s1.append(batch_row[i][0])\n",
    "            actions.append(batch_row[i][1])\n",
    "            R_one = batch_row[i][4] + self.parameters.gamma * predicted_q[i] if batch_row[i][6] == False else batch_row[i][4]\n",
    "            R_adv = batch_row[i][5] + self.parameters.gamma**self.parameters.n_adv * predicted_q_adv[i] if batch_row[i][6] == False else batch_row[i][5]\n",
    "            r_one.append(R_one)\n",
    "            r_adv.append(R_adv)\n",
    "            isdemo.append(batch_row[i][7])\n",
    "\n",
    "        actions = np.array(actions)\n",
    "        return s1, actions.astype(np.int32), r_one, r_adv, isdemo, is_weight, tree_idx\n",
    "    \n",
    "    def calc_beta(self, progress):\n",
    "#         return BETA_MIN\n",
    "        return (self.parameters.beta_max - self.parameters.beta_min) * progress + self.parameters.beta_min\n",
    "    \n",
    "    def exploring_step(self):\n",
    "        if self.step % self.parameters.interval_pull_params == 0:\n",
    "            self.network.pull_parameter_server(self.sess)\n",
    "        loss_values = []\n",
    "        if not self.game.is_episode_finished() and self.game.get_screen_buff() is not None:\n",
    "\n",
    "            s1_ = self.preprocess(self.game.get_screen_buff())\n",
    "            self.push_obs(s1_)\n",
    "            agent_action_idx = self.agent.act_eps_greedy(self.sess, self.obs['s1'], self.progress)\n",
    "#             engin_action = self.convert_action_agent2engine_simple(agent_action_idx)\n",
    "            engin_action = self.convert_action_agent2engine(agent_action_idx)\n",
    "            r,r_detail = self.game.make_action(self.step,engin_action , self.parameters.frame_repeat)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            if isterminal:\n",
    "                s2_ = np.zeros(self.parameters.resolution)\n",
    "            else:\n",
    "                s2_ = self.preprocess(self.game.get_screen_buff())\n",
    "            \n",
    "            self.push_batch( self.obs['s1'], agent_action_idx, s2_, r , isterminal, False)\n",
    "            \n",
    "            if len(self.memory) >= self.parameters.n_adv or isterminal:\n",
    "                batch = self.make_advantage_data()\n",
    "                self.clear_batch()\n",
    "                for i,b in enumerate(batch):\n",
    "                    if len(b) == 8:\n",
    "                        self.replay_memory.store(b)\n",
    "            \n",
    "            self.step += 1\n",
    "        else:\n",
    "            self.game.new_episode()\n",
    "            self.clear_batch()\n",
    "            self.clear_obs()\n",
    "\n",
    "        return loss_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
