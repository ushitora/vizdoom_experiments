{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import os, time, random, threading, h5py, math,pickle, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name, parameter_server, networksetting, parameters):\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope(\"learning_network\"):\n",
    "                self.state1_ = tf.placeholder(tf.float32,shape=(None,)+self.parameters.resolution, name=\"state_1\")\n",
    "                self.q_value, self.conv1, self.conv2,self.reshape,self.fc1 = self._build_model(self.state1_,networksetting)\n",
    "            with tf.variable_scope(\"target_network\"):\n",
    "                self.state1_target_ = tf.placeholder(tf.float32,shape=(None,)+self.parameters.resolution, name=\"state_1\")\n",
    "                self.q_value_target,_,_,_,_ = self._build_model(self.state1_target_,networksetting)\n",
    "            \n",
    "            self.a_ = tf.placeholder(tf.int32, shape=(None,), name=\"action\")\n",
    "            self.target_one_ = tf.placeholder(tf.float32, shape=(None,), name=\"target_one_\")\n",
    "            self.target_n_ = tf.placeholder(tf.float32, shape=(None,), name=\"target_n_\")\n",
    "            self.isdemo_ = tf.placeholder(tf.float32,shape=(None,), name=\"isdemo_\")\n",
    "            self.mergin_ = tf.placeholder(tf.float32,shape=(None,self.parameters.n_agent_action), name=\"mergin_\")\n",
    "            self.is_weight_ = tf.placeholder(tf.float32, shape=(None,), name=\"is_weight\")\n",
    "            \n",
    "            self._build_graph()\n",
    "        \n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients([(g,w) for g, w in zip(self.grads, parameter_server.weights_params)])\n",
    "        \n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params_learning,parameter_server.weights_params)]\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params_learning)]\n",
    "\n",
    "    def _build_model(self,state,networksetting):\n",
    "        conv1 = networksetting.conv1(state)\n",
    "#         maxpool1 = networksetting.maxpool1(conv1)\n",
    "        conv2 = networksetting.conv2(conv1)\n",
    "#         maxpool2 = networksetting.maxpool2(conv2)\n",
    "        reshape = networksetting.reshape(conv2)\n",
    "        fc1 = networksetting.fc1(reshape)\n",
    "        \n",
    "        q_value = networksetting.q_value(fc1)\n",
    "        \n",
    "        return q_value, conv1, conv2,reshape,fc1\n",
    "\n",
    "    def _build_graph(self):\n",
    "\n",
    "        self.q_prob = tf.nn.softmax(self.q_value)\n",
    "        self.q_argmax = tf.argmax(self.q_value, axis=1)\n",
    "        self.q_learning_max = tf.reduce_max(self.q_value, axis=1)\n",
    "        self.q_target_max = tf.reduce_max(self.q_value_target, axis=1)\n",
    "        \n",
    "        action_idxlist = tf.transpose([tf.range(tf.shape(self.q_value)[0]), self.a_])\n",
    "        \n",
    "        self.weights_params_learning = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"/learning_network\")\n",
    "        self.weights_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"/target_network\")\n",
    "        \n",
    "        self.tderror_one = self.parameters.lambda1 * tf.abs(self.target_one_ - tf.reduce_max(self.q_value, axis=1))\n",
    "        self.loss_one = (self.parameters.lambda1 * tf.square(self.target_one_ - tf.reduce_max(self.q_value, axis=1))) * self.is_weight_\n",
    "        self.tderror_n = self.parameters.lambda2 * tf.abs(self.target_n_ - tf.reduce_max(self.q_value, axis=1))\n",
    "        self.loss_n = (self.parameters.lambda2 * tf.square(self.target_n_ - tf.reduce_max(self.q_value, axis=1)))*self.is_weight_\n",
    "                \n",
    "        self.loss_l2 = self.parameters.lambda4 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params_learning])\n",
    "        \n",
    "        self.loss_mergin = self.parameters.lambda3 * ((tf.stop_gradient(tf.reduce_max(self.q_value + self.mergin_, axis=1)) - tf.gather_nd(self.q_value,indices=action_idxlist))*self.isdemo_)\n",
    "        \n",
    "        self.tderror_total = self.tderror_one + self.tderror_n\n",
    "        self.loss_total = tf.reduce_mean(self.loss_one +  self.loss_n + self.loss_mergin + self.loss_l2)    \n",
    "        \n",
    "        self.grads = tf.gradients(self.loss_total ,self.weights_params_learning)\n",
    "        \n",
    "        self.copy_params = [t.assign(l) for l,t in zip(self.weights_params_learning, self.weights_params_target)]\n",
    "        \n",
    "    def copy_network_learning2target(self, sess):\n",
    "        return sess.run(self.copy_params)\n",
    "        \n",
    "    def pull_parameter_server(self, sess):\n",
    "        return sess.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self,sess):\n",
    "        return sess.run(self.push_local_weight_params)\n",
    "        \n",
    "    def get_weights_learngin(self, sess):\n",
    "        return sess.run(self.weights_params_learning)\n",
    "    \n",
    "    def get_weights_target(self, sess):\n",
    "        return sess.run(self.weights_params_target)\n",
    "    \n",
    "    def get_loss(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "#         l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "        l_one, tderror_total = sess.run([self.loss_n, self.tderror_n], feed_dict)\n",
    "        return l_one, 0,0,0, tderror_total\n",
    "    \n",
    "    def get_losstotal(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "        loss_total = sess.run([self.loss_total], feed_dict)\n",
    "        return loss_total[0]\n",
    "    \n",
    "    def get_grads(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "        grads = sess.run(self.grads, feed_dict)\n",
    "        return grads\n",
    "    \n",
    "    def update_parameter_server(self, sess, s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        assert np.ndim(s1) == 4\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "#         _,l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.update, self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "#         _,l_one,l_mergin,l_l2 ,tderror_total = sess.run([self.update_global_weight_params,self.loss_n,self.loss_mergin,self.loss_l2, self.tderror_total], feed_dict)\n",
    "#         return l_one, 0,l_mergin,l_l2, tderror_total\n",
    "        _,l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.update_global_weight_params, self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "        return l_one, l_n, l_mergin, l_l2, tderror_total\n",
    "    \n",
    "    def check_weights(self, sess):\n",
    "        weights = sess.run(self.weights_params_learning)\n",
    "        assert np.isnan([np.mean(w) for w in weights]).any()==False , print(weights)\n",
    "\n",
    "    def get_qvalue_learning(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_value, {self.state1_: s1})\n",
    "    \n",
    "    def get_qvalue_lerning_max(self, sess, s1):\n",
    "        return sess.run(self.q_learing_max, {self.state1_:s1})\n",
    "\n",
    "    def get_qvalue_target(self, sess ,s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_value_target, {self.state1_target_:s1})\n",
    "    \n",
    "    def get_qvalue_target_max(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_target_max, {self.state1_target_:s1})\n",
    "    \n",
    "    def get_qvalue_max_learningaction(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        action_idx, q_value = sess.run([self.q_argmax, self.q_value_target], {self.state1_:s1, self.state1_target_:s1})\n",
    "        return q_value[range(np.shape(s1)[0]), action_idx]\n",
    "    \n",
    "    def get_policy(self, sess, s1):\n",
    "        return sess.run(self.q_prob, {self.state1_: s1})\n",
    "    \n",
    "    def get_best_action(self,sess, s1):\n",
    "        return sess.run(self.q_argmax, {self.state1_:s1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocalNew(object):\n",
    "    def __init__(self,name, parameter_server, networksetting, parameters):\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope(\"learning_network\"):\n",
    "                self.state1_ = tf.placeholder(tf.float32,shape=(None,)+self.parameters.resolution, name=\"state_1\")\n",
    "                self.q_value, self.conv1, self.conv2,self.reshape,self.fc1 = self._build_model(self.state1_,networksetting)\n",
    "            with tf.variable_scope(\"target_network\"):\n",
    "                self.state1_target_ = tf.placeholder(tf.float32,shape=(None,)+self.parameters.resolution, name=\"state_1\")\n",
    "                self.q_value_target,_,_,_,_ = self._build_model(self.state1_target_,networksetting)\n",
    "            \n",
    "            self.a_ = tf.placeholder(tf.int32, shape=(None,), name=\"action\")\n",
    "            self.target_one_ = tf.placeholder(tf.float32, shape=(None,), name=\"target_one_\")\n",
    "            self.target_n_ = tf.placeholder(tf.float32, shape=(None,), name=\"target_n_\")\n",
    "            self.isdemo_ = tf.placeholder(tf.float32,shape=(None,), name=\"isdemo_\")\n",
    "            self.mergin_ = tf.placeholder(tf.float32,shape=(None,self.parameters.n_agent_action), name=\"mergin_\")\n",
    "            self.is_weight_ = tf.placeholder(tf.float32, shape=(None,), name=\"is_weight\")\n",
    "            \n",
    "            self._build_graph()\n",
    "        \n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients([(g,w) for g, w in zip(self.grads, parameter_server.weights_params)])\n",
    "        \n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params_learning,parameter_server.weights_params)]\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params_learning)]\n",
    "\n",
    "    def _build_model(self,state,networksetting):\n",
    "        conv1 = networksetting.conv1(state)\n",
    "#         maxpool1 = networksetting.maxpool1(conv1)\n",
    "        conv2 = networksetting.conv2(conv1)\n",
    "#         maxpool2 = networksetting.maxpool2(conv2)\n",
    "        reshape = networksetting.reshape(conv2)\n",
    "        fc1 = networksetting.fc1(reshape)\n",
    "        \n",
    "        q_value = networksetting.q_value(fc1)\n",
    "        \n",
    "        return q_value, conv1, conv2,reshape,fc1\n",
    "\n",
    "    def _build_graph(self):\n",
    "\n",
    "        self.q_prob = tf.nn.softmax(self.q_value)\n",
    "        self.q_argmax = tf.argmax(self.q_value, axis=1)\n",
    "        self.q_learning_max = tf.reduce_max(self.q_value, axis=1)\n",
    "        self.q_target_max = tf.reduce_max(self.q_value_target, axis=1)\n",
    "        \n",
    "        action_idxlist = tf.transpose([tf.range(tf.shape(self.q_value)[0]), self.a_])\n",
    "        \n",
    "        self.weights_params_learning = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"/learning_network\")\n",
    "        self.weights_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"/target_network\")\n",
    "        \n",
    "        self.tderror_one = self.parameters.lambda1 * tf.abs(self.target_one_ - tf.gather_nd(self.q_value, indices=action_idxlist))\n",
    "        self.loss_one = (self.parameters.lambda1 * tf.square(self.target_one_ - tf.gather_nd(self.q_value, indices=action_idxlist))) * self.is_weight_\n",
    "        self.tderror_n = self.parameters.lambda2 * tf.abs(self.target_n_ - tf.gather_nd(self.q_value, indices=action_idxlist))\n",
    "        self.loss_n = (self.parameters.lambda2 * tf.square(self.target_n_ - tf.gather_nd(self.q_value, indices=action_idxlist)))*self.is_weight_\n",
    "        \n",
    "        self.loss_l2 = self.parameters.lambda4 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params_learning])\n",
    "        \n",
    "        self.loss_mergin = self.parameters.lambda3 * ((tf.stop_gradient(tf.reduce_max(self.q_value + self.mergin_, axis=1)) - tf.gather_nd(self.q_value,indices=action_idxlist))*self.isdemo_)\n",
    "        \n",
    "        self.tderror_total = self.tderror_one + self.tderror_n\n",
    "        self.loss_total = tf.reduce_mean(self.loss_one +  self.loss_n + self.loss_mergin + self.loss_l2)    \n",
    "        \n",
    "        self.grads = tf.gradients(self.loss_total ,self.weights_params_learning)\n",
    "        \n",
    "        self.copy_params = [t.assign(l) for l,t in zip(self.weights_params_learning, self.weights_params_target)]\n",
    "        \n",
    "    def copy_network_learning2target(self, sess):\n",
    "        return sess.run(self.copy_params)\n",
    "        \n",
    "    def pull_parameter_server(self, sess):\n",
    "        return sess.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self,sess):\n",
    "        return sess.run(self.push_local_weight_params)\n",
    "        \n",
    "    def get_weights_learngin(self, sess):\n",
    "        return sess.run(self.weights_params_learning)\n",
    "    \n",
    "    def get_weights_target(self, sess):\n",
    "        return sess.run(self.weights_params_target)\n",
    "    \n",
    "    def get_loss(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "#         l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "        l_one, tderror_total = sess.run([self.loss_n, self.tderror_n], feed_dict)\n",
    "        return l_one, 0,0,0, tderror_total\n",
    "    \n",
    "    def get_losstotal(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "        loss_total = sess.run([self.loss_total], feed_dict)\n",
    "        return loss_total[0]\n",
    "    \n",
    "    def get_grads(self, sess,s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "        grads = sess.run(self.grads, feed_dict)\n",
    "        return grads\n",
    "    \n",
    "    def update_parameter_server(self, sess, s1, a, target_one,target_n, isdemo, is_weight):\n",
    "        assert np.ndim(s1) == 4\n",
    "        mergin_value = np.ones((len(s1), self.parameters.n_agent_action)) * self.parameters.mergin_value\n",
    "        mergin_value[range(len(s1)), a] = 0.0\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.target_one_:target_one, self.target_n_:target_n, self.isdemo_:isdemo, self.is_weight_:is_weight, self.mergin_:mergin_value}\n",
    "#         _,l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.update, self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "#         _,l_one,l_mergin,l_l2 ,tderror_total = sess.run([self.update_global_weight_params,self.loss_n,self.loss_mergin,self.loss_l2, self.tderror_total], feed_dict)\n",
    "#         return l_one, 0,l_mergin,l_l2, tderror_total\n",
    "        _,l_one, l_n, l_mergin, l_l2, tderror_total = sess.run([self.update_global_weight_params, self.loss_one, self.loss_n, self.loss_mergin, self.loss_l2, self.tderror_total], feed_dict)\n",
    "        return l_one, l_n, l_mergin, l_l2, tderror_total\n",
    "    \n",
    "    def check_weights(self, sess):\n",
    "        weights = sess.run(self.weights_params_learning)\n",
    "        assert np.isnan([np.mean(w) for w in weights]).any()==False , print(weights)\n",
    "\n",
    "    def get_qvalue_learning(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_value, {self.state1_: s1})\n",
    "    \n",
    "    def get_qvalue_lerning_max(self, sess, s1):\n",
    "        return sess.run(self.q_learing_max, {self.state1_:s1})\n",
    "\n",
    "    def get_qvalue_target(self, sess ,s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_value_target, {self.state1_target_:s1})\n",
    "    \n",
    "    def get_qvalue_target_max(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        return sess.run(self.q_target_max, {self.state1_target_:s1})\n",
    "    \n",
    "    def get_qvalue_max_learningaction(self, sess, s1):\n",
    "        assert np.ndim(s1) == 4\n",
    "        action_idx, q_value = sess.run([self.q_argmax, self.q_value_target], {self.state1_:s1, self.state1_target_:s1})\n",
    "        return q_value[range(np.shape(s1)[0]), action_idx]\n",
    "    \n",
    "    def get_policy(self, sess, s1):\n",
    "        return sess.run(self.q_prob, {self.state1_: s1})\n",
    "    \n",
    "    def get_best_action(self,sess, s1):\n",
    "        return sess.run(self.q_argmax, {self.state1_:s1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
