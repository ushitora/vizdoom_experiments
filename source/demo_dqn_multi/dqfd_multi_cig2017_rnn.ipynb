{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from random import sample, randint, random\n",
    "import time,random,threading,datetime\n",
    "from tqdm import tqdm\n",
    "import transition\n",
    "import tensorflow as tf\n",
    "import replay_memory\n",
    "import transition\n",
    "import h5py\n",
    "import math\n",
    "import sys, os, glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([np.array([1,2,3]), np.array([4,5,6])], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_PATH = \"./demonstration/demodata02.hdf5\"\n",
    "CONFIG_FILE_PATH = \"./config/custom_config.cfg\"\n",
    "LOG_DIR = \"./logs/logs_test\"\n",
    "MODEL_PATH = \"./models/model_test\"\n",
    "GIF_PATH = \"./gifs/gif_test.gif\"\n",
    "\n",
    "TIME_LEARN = datetime.timedelta(hours  = 3)\n",
    "TIME_PRELEARN = datetime.timedelta(minutes  = 2)\n",
    "\n",
    "SAVE_FILE = False\n",
    "\n",
    "USED_GPU = \"0\"\n",
    "\n",
    "# if SAVE_FILE:\n",
    "#     if len(glob.glob(LOG_DIR+\"/*\"))!=0 or len(glob.glob(MODEL_PATH))!=0:\n",
    "#         print(\"ERROR: Log or Model file exists already!\")\n",
    "#         sys.exit()\n",
    "        \n",
    "#     if not os.path.exists(MODEL_PATH):\n",
    "#         os.mkdir(MODEL_PATH)\n",
    "\n",
    "RESOLUTION = (120,180,3)\n",
    "\n",
    "N_ADV = 5\n",
    "\n",
    "FREQ_COPY = 10\n",
    "FREQ_TEST = 50\n",
    "\n",
    "N_WORKERS = 1\n",
    "N_PRESTEPS = 20000\n",
    "N_STEPS = 200\n",
    "TOTAL_STEPS = N_STEPS\n",
    "TOTAL_TIME = TIME_LEARN.seconds\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "LEARNING_RATE = 0.3\n",
    "RMSProbDecaly = 0.9\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "LAMBDA1 = 1.0\n",
    "LAMBDA2 = 1.0\n",
    "LAMBDA3 = 10e-5\n",
    "L_MIN = 0.8\n",
    "LSTM_SIZE = 128\n",
    "\n",
    "N_ACTION = 6\n",
    "\n",
    "BOTS_NUM = 20\n",
    "\n",
    "FRAME_REPEAT = 4\n",
    "\n",
    "N_FOLDER = 3\n",
    "\n",
    "REWARDS = {'living':-0.01, 'health_loss':-1, 'medkit':50, 'ammo':0.0, 'frag':500, 'dist':3e-2, 'suicide':-500} \n",
    "\n",
    "CAPACITY = 10000\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.0\n",
    "LINEAR_EPS_START = 0.1\n",
    "LINEAR_EPS_END = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --class for Thread　-------\n",
    "class WorkerThread:\n",
    "    # Each Thread has an Environment to run Game and Learning.\n",
    "    def __init__(self, thread_name, parameter_server, isLearning=True):\n",
    "        self.environment = Environment(thread_name, parameter_server)\n",
    "        print(thread_name,\" Initialized\")\n",
    "        self.isLearning = isLearning\n",
    "\n",
    "    def run(self):\n",
    "        if self.isLearning:\n",
    "            while True:\n",
    "                if not self.environment.finished:\n",
    "                    self.environment.run()\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            # Run Test Environment\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,name, parameter_server, summary=False):\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(CONFIG_FILE_PATH)\n",
    "        self.game.set_window_visible(False)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "#         self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        self.game.set_screen_format(ScreenFormat.CRCGCB)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "        \n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,frag,pos_x,pos_y)\n",
    "        \n",
    "        self.network = NetworkLocal(name, parameter_server)\n",
    "        self.agent = Agent(self.network)\n",
    "        \n",
    "        self.demo_buff = replay_memory.ReplayMemory(CAPACITY)\n",
    "        \n",
    "        self.local_step = 0\n",
    "        \n",
    "        self.finished = False\n",
    "        \n",
    "        self.pre_death = 0\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.summary = summary\n",
    "        self.parameter_server = parameter_server\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.game.new_episode()\n",
    "        self.agent.image_buff = []\n",
    "        for i in range(BOTS_NUM):\n",
    "            self.game.send_game_command(\"addbot\")\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION,mode='constant')\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def get_reward(self):\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        \n",
    "        r,r_detail = self.reward_gen.get_reward(health,ammo,frag,pos_x,pos_y)\n",
    "    \n",
    "        return r,r_detail\n",
    "    \n",
    "    # Method for Previous Learning\n",
    "    def run_pre_learning(self):        \n",
    "        global frames, start_time_pre\n",
    "        \n",
    "#         for step in tqdm(range(N_PRESTEPS)):\n",
    "        step = 0\n",
    "        while True:\n",
    "            \n",
    "            self.network.pull_parameter_server()\n",
    "            \n",
    "            tree_idx, batch, is_weight = self.demo_buff.sample(BATCH_SIZE)\n",
    "\n",
    "            s1 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)\n",
    "            s2 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)\n",
    "            actions = np.zeros((BATCH_SIZE ,),dtype=np.int8)\n",
    "            rewards = np.zeros((BATCH_SIZE, ),dtype=np.float32)\n",
    "            rewards_adv = np.zeros((BATCH_SIZE,),dtype=np.float32)\n",
    "            isterminals = np.zeros((BATCH_SIZE, ),dtype=np.int8)\n",
    "            isdemos = np.zeros((BATCH_SIZE,),dtype=np.int8)\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "                for j in range(N_ADV):\n",
    "                    if not type(batch[i][j].s1) == type(None):\n",
    "                        s1[i][j] = batch[i][j].s1\n",
    "                    if not type(batch[i][j].s2) == type(None):\n",
    "                        s2[i][j] = batch[i][j].s2\n",
    "#                     print(np.mean(s1[i][j]), np.std(s1[i][j]))\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "                R = 0\n",
    "                for j in range(N_ADV-1, -1, -1):\n",
    "#                     s1[i][j] = batch[i][j].s1\n",
    "#                     s2[i][j] = batch[i][j].s2\n",
    "                    if not batch[i][j].isterminal :\n",
    "                        if j == N_ADV-1:\n",
    "                            isterminals[i] = batch[i][j].isterminal\n",
    "                            actions[i] = batch[i][j].action\n",
    "                            rewards[i] = batch[i][j].reward\n",
    "#                             R = np.max(self.network.get_q_value(np.array([np.concatenate([s1[i][0:j], np.ones(shape=(N_ADV-j,)+RESOLUTION)*np.nan])]))[0])\n",
    "#                             print(s1[i].shape)\n",
    "                            R = np.max(self.network.get_q_value( np.array([s1[i]]) )[0])\n",
    "                        else:\n",
    "                            R = batch[i][j].reward + GAMMA * R\n",
    "                    else:\n",
    "                        if not(type(batch[i][j].s1) == type(None)):\n",
    "                            isterminals[i] = batch[i][j].isterminal\n",
    "                            actions[i] = batch[i][j].action\n",
    "                            rewards[i] = batch[i][j].reward\n",
    "                        else:\n",
    "                            pass\n",
    "                rewards_adv[i] = R\n",
    "                isdemos[i] = True\n",
    "            \n",
    "            l_one, l_adv, loss_class = self.network.update_parameter_server_batch(s1,actions,rewards,rewards_adv,s2,isdemos)\n",
    "#             print(SESS.run([self.network.model_l,self.network.model_t,self.network.loss_one],{self.network.state1_:s1[0:1], self.network.state2_:s2[0:1],self.network.r_:rewards[0:1]}))\n",
    "            if step%100 == 0 and SAVE_FILE == True:\n",
    "#                 lo, la, lc = self.network.calc_loss(s1[0:1],actions[0:1],rewards[0:1],rewards_adv[0:1],s2[0:1],isdemos[0:1])\n",
    "                self.parameter_server.write_weights(frames)\n",
    "                self.parameter_server.write_summary(frames,s1[0:1],l_one, l_adv, loss_class,[0.0])\n",
    "            if step%FREQ_COPY==0:\n",
    "                self.network.copy_learn2target()\n",
    "            frames += 1\n",
    "            step += 1\n",
    "            \n",
    "            if datetime.datetime.now() > TIME_PRELEARN + start_time_pre:\n",
    "                runout = True\n",
    "                break\n",
    "    \n",
    "    # Method for multi task learning\n",
    "    def run(self):\n",
    "        global frames,runout,current_time\n",
    "        \n",
    "        self.start_episode()\n",
    "        \n",
    "        train_episode = 0\n",
    "        step = 0\n",
    "        while True: \n",
    "#         for step in range(WORKER_STEPS):\n",
    "            #Copy params from global\n",
    "            self.agent.q_network.pull_parameter_server()\n",
    "\n",
    "            if not self.game.is_episode_finished():\n",
    "                \n",
    "                if step%N_ADV==0 and not step==0:\n",
    "                    self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                  self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "                s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "                action = self.agent.act_eps_greedy(s1)\n",
    "                self.game.make_action(action,FRAME_REPEAT)\n",
    "                reward,_ = self.get_reward()\n",
    "                isterminal = self.game.is_episode_finished()\n",
    "                s2 = self.preprocess(self.game.get_state().screen_buffer) if not isterminal else np.zeros(RESOLUTION)\n",
    "\n",
    "                self.agent.push_advantage(s1,action.index(1),reward,s2,isterminal,False)\n",
    "#                 l_v = self.agent.calc_loss()\n",
    "                self.agent.learn_advantage(isterminal)\n",
    "                if self.summary==True and SAVE_FILE == True:\n",
    "                    if step % 100 == 0:\n",
    "                        self.parameter_server.write_summary(frames,self.agent.s1_record, self.agent.loss_one_record, self.agent.loss_adv_record, self.agent.loss_class_record,[0.0])\n",
    "                \n",
    "                if self.game.is_player_dead():\n",
    "                    self.game.respawn_player()\n",
    "                    self.reward_gen.respawn_pos(self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                                self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                                self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "            else:\n",
    "                train_episode += 1\n",
    "                self.start_episode()\n",
    "                self.reward_gen.new_episode(health = self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                           ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                           posx = self.game.get_game_variable(GameVariable.POSITION_X), \\\n",
    "                                           posy = self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "            \n",
    "            frames += 1\n",
    "            step += 1\n",
    "            current_time = datetime.datetime.now().timestamp() - start_time_async.timestamp()\n",
    "            \n",
    "            if runout == True:\n",
    "                break\n",
    "                \n",
    "        print(self.name,\" finished\")\n",
    "        self.finished = True\n",
    "        \n",
    "    def run_test(self, save_gif=False):\n",
    "        \n",
    "        global frames\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,0,pos_x,pos_y)\n",
    "        \n",
    "        self.start_episode()\n",
    "        \n",
    "        #Copy params from global\n",
    "        self.agent.q_network.pull_parameter_server()\n",
    "\n",
    "        step = 0\n",
    "        gif_img = []\n",
    "        while not self.game.is_episode_finished():\n",
    "            \n",
    "            if step%N_ADV==0 and not step==0:\n",
    "                self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                              self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "            s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "            if save_gif==True:\n",
    "                if step%5==0:\n",
    "                    gif_img.append(s1)\n",
    "            action = self.agent.act_greedy(s1)\n",
    "            self.game.make_action(action,1)\n",
    "            reward = self.get_reward()\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "\n",
    "            if self.game.is_player_dead():\n",
    "                self.game.respawn_player()\n",
    "                self.reward_gen.respawn_pos(self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                            self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                            self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                            self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        save_img = []\n",
    "        if save_gif == True:\n",
    "            for i in range(len(gif_img)):\n",
    "                save_img.append(Image.fromarray(np.uint8(gif_img[i]*255)))\n",
    "            save_img[0].save(GIF_PATH,save_all=True,append_images=save_img[1:])\n",
    "        \n",
    "        print(\"----------TEST at %d step-------------\"%(frames))\n",
    "        ret_frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        ret_death = self.game.get_game_variable(GameVariable.DEATHCOUNT)-self.pre_death\n",
    "        ret_reward = self.reward_gen.total_reward\n",
    "        print(\"FRAG:\",ret_frag,\"DEATH:\",ret_death)\n",
    "        print(\"REWARD\",ret_reward)\n",
    "        print(\"DETAIL:\",self.reward_gen.total_reward_detail)\n",
    "        self.pre_death = self.game.get_game_variable(GameVariable.DEATHCOUNT)\n",
    "        return ret_reward,ret_frag,ret_death\n",
    "                 \n",
    "    def load_demonstration(self):\n",
    "        \n",
    "        file = h5py.File(DEMO_PATH,\"r\")\n",
    "        episode_list = list(file.keys())[1:]\n",
    "        episode_list = episode_list[1:3]\n",
    "        \n",
    "        test_data = []\n",
    "        \n",
    "        select_forward = False\n",
    "        for e in episode_list:\n",
    "            n_steps = file[e+\"/states\"].shape[0]\n",
    "            states = file[e+\"/states\"][:]\n",
    "            actions = file[e+\"/action\"][:]\n",
    "            frags = file[e+\"/frag\"][:]\n",
    "            deaths = file[e+\"/death\"][:]\n",
    "            health = file[e+\"/health\"][:]\n",
    "            ammo = file[e+\"/ammo\"][:]\n",
    "            posx = file[e+\"/posx\"][:]\n",
    "            posy = file[e+\"/posy\"][:]\n",
    "            \n",
    "            log_buffer = []\n",
    "            originx = posx[0]\n",
    "            originy = posy[0]\n",
    "            for i in range(n_steps):\n",
    "                \n",
    "                if i % N_ADV == 0:\n",
    "                    originx = posx[i]\n",
    "                    originy = posy[i]\n",
    "                if not i == n_steps - 1:\n",
    "                    m_frag = frags[i+1] - frags[i]\n",
    "                    m_death = deaths[i+1] - deaths[i]\n",
    "                    m_health = health[i+1] - health[i]\n",
    "                    m_ammo = ammo[i+1] - ammo[i]\n",
    "                    m_posx = abs(posx[i] - originx)\n",
    "                    m_posy = abs(posy[i] - originy)\n",
    "                    r_d = self.reward_gen.calc_reward(m_frag,m_death,m_health,m_ammo,m_posx,m_posy)\n",
    "                    r = sum(r_d.values())\n",
    "                    \n",
    "                    if sum(actions[i]) == 1:\n",
    "                        a_idx = np.where(actions[i] == 1)[0]\n",
    "                    else:\n",
    "                        if actions[i][5] == 1:\n",
    "                            a_idx = 5\n",
    "                        elif (np.where(actions[i]==1)[0] == [2,4]).all():\n",
    "                            if select_forward == True:\n",
    "                                a_idx = 4\n",
    "                            else:\n",
    "                                a_idx = 2\n",
    "                            select_forward = not select_forward\n",
    "                        elif (np.where(actions[i]==1)[0] == [3,4]).all():\n",
    "                            if select_forward == True:\n",
    "                                a_idx = 4\n",
    "                            else:\n",
    "                                a_idx = 3\n",
    "                            select_forward = not select_forward\n",
    "                        else:\n",
    "                            a_idx = -1\n",
    "                            \n",
    "                    \n",
    "                    log_buffer.append(transition.Transition(states[i],a_idx,states[i+1],r,False,True))\n",
    "                else:\n",
    "                    log_buffer.append(transition.Transition(states[i],5, None, 0 ,True, True))\n",
    "                    log_buffer = log_buffer + [transition.Transition(None,None,None,None,True,True) for _ in range(N_ADV - len(log_buffer))]\n",
    "                    \n",
    "                    \n",
    "                if len(log_buffer) == N_ADV:\n",
    "#                     self.demo_buff.store(np.copy(log_buffer))\n",
    "                    test_data.append(np.copy(log_buffer))\n",
    "                    log_buffer = []\n",
    "        \n",
    "        return test_data\n",
    "            \n",
    "        \n",
    "#         hdf5file = h5py.File(DEMO_PATH,\"r\")\n",
    "#         folder = \"demodata_\"+str(0)\n",
    "#         state1 = hdf5file[folder+\"/state1\"].value\n",
    "#         state2 = hdf5file[folder+\"/state2\"].value\n",
    "#         actions = hdf5file[folder+\"/actions\"].value\n",
    "#         isterminals = hdf5file[folder+\"/isterminals\"].value\n",
    "#         health = hdf5file[folder+\"/healths\"].value\n",
    "#         ammo = hdf5file[folder+\"/ammos\"].value\n",
    "#         posx = hdf5file[folder+\"/posxs\"].value\n",
    "#         posy = hdf5file[folder+\"/posys\"].value\n",
    "#         death = hdf5file[folder+\"/deaths\"].value\n",
    "#         frag = hdf5file[folder+\"/frags\"].value\n",
    "\n",
    "#         for i in range(1,N_FOLDER):\n",
    "#             folder = \"demodata_\" +str(i)\n",
    "#             state1 = np.concatenate((state1,hdf5file[folder+\"/state1\"].value),axis=0)\n",
    "#             state2 = np.concatenate((state2,hdf5file[folder+\"/state2\"].value),axis=0)\n",
    "#             actions = np.concatenate((actions,hdf5file[folder+\"/actions\"].value),axis=0)\n",
    "#             isterminals = np.concatenate((isterminals,hdf5file[folder+\"/isterminals\"].value),axis=0)\n",
    "#             health = np.concatenate((health,hdf5file[folder+\"/healths\"].value),axis=0)\n",
    "#             ammo = np.concatenate((ammo,hdf5file[folder+\"/ammos\"].value),axis=0)\n",
    "#             posx = np.concatenate((posx,hdf5file[folder+\"/posxs\"].value),axis=0)\n",
    "#             posy = np.concatenate((posy,hdf5file[folder+\"/posys\"].value),axis=0)\n",
    "#             death = np.concatenate((death,hdf5file[folder+\"/deaths\"].value),axis=0)\n",
    "#             frag = np.concatenate((frag,hdf5file[folder+\"/frags\"].value),axis=0)\n",
    "\n",
    "#         n_transit, n_step, _ = actions.shape\n",
    "\n",
    "#         print(\"SIZE of DEMO:\",actions.shape)\n",
    "\n",
    "#         transit = np.empty((n_step,),dtype=object)\n",
    "\n",
    "#         is_dead = False\n",
    "#         is_finished = False\n",
    "\n",
    "#         pre_health = 100\n",
    "#         pre_ammo = 15\n",
    "#         pre_frag = 0\n",
    "#         pre_death = 0\n",
    "#         pre_posx = 0.0\n",
    "#         pre_posy = 0.0\n",
    "\n",
    "\n",
    "#         for i in range(n_transit):\n",
    "\n",
    "#             if i % 2 == 0:\n",
    "#                 pre_posx = posx[i][0]\n",
    "#                 pre_posy = posy[i][0]\n",
    "\n",
    "#             for j in range(n_step):\n",
    "#                 if not is_finished:\n",
    "#                     if is_dead :\n",
    "#                         pre_posx = posx[i][j]\n",
    "#                         pre_posy = posy[i][j]\n",
    "#                         is_dead = False\n",
    "\n",
    "#                     m_frag = frag[i][j] - pre_frag\n",
    "#                     m_death = death[i][j] - pre_death\n",
    "#                     m_health = health[i][j] - pre_health\n",
    "#                     m_ammo = ammo[i][j] - pre_ammo\n",
    "#                     m_posx = posx[i][j] - pre_posx\n",
    "#                     m_posy = posy[i][j] - pre_posy\n",
    "\n",
    "#                     if m_death >= 1:\n",
    "#                         is_dead = True \n",
    "\n",
    "#                     if isterminals[i][j] == True:\n",
    "#                         is_finished = True\n",
    "\n",
    "#                     r_d = self.reward_gen.calc_reward(m_frag,m_death,m_health,m_ammo,m_posx,m_posy)\n",
    "#                     r = sum(r_d.values())\n",
    "#                     transit[j] = transition.Transition(state1[i][j],actions[i][j],state2[i][j],r,isterminals[i][j],True)\n",
    "\n",
    "#                     pre_frag = frag[i][j]\n",
    "#                     pre_death = death[i][j]\n",
    "#                     pre_health = health[i][j]\n",
    "#                     pre_ammo = ammo[i][j]\n",
    "#                 else:\n",
    "#                     transit[j] = transition.Transition(None,None,None,None,True,True)\n",
    "\n",
    "#             is_finished = False\n",
    "\n",
    "#             self.demo_buff.store(np.copy(transit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardGenerater(object):\n",
    "    def __init__(self,health,ammo,frag,pos_x,pos_y):\n",
    "\n",
    "        # Reward\n",
    "        self.rewards = REWARDS\n",
    "        self.dist_unit = 6.0\n",
    "        \n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y\n",
    "        \n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "        self.pre_frag = frag\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self.total_reward_detail = {'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "\n",
    "    \n",
    "    def get_reward(self,health,ammo,frag,pos_x,pos_y):\n",
    "        \n",
    "        if abs(health) > 10000:\n",
    "            health = 100.0\n",
    "\n",
    "        if self.origin_x == 0 and self.origin_y == 0:\n",
    "            self.origin_x = pos_x\n",
    "            self.origin_y = pos_y\n",
    "        \n",
    "        self.reward_detail = self.calc_reward(frag-self.pre_frag,0.0, \\\n",
    "                                              health-self.pre_health,\\\n",
    "                                              ammo-self.pre_ammo, \\\n",
    "                                              pos_x-self.origin_x, \\\n",
    "                                              pos_y-self.origin_y)\n",
    "        self.reward = sum(self.reward_detail.values())\n",
    "\n",
    "        for k,v in self.reward_detail.items():\n",
    "            self.total_reward_detail[k] += v\n",
    "        self.total_reward = sum(self.total_reward_detail.values())\n",
    "\n",
    "        self.pre_frag = frag\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "                    \n",
    "        return (self.reward, self.reward_detail)\n",
    "    \n",
    "    def calc_reward(self,m_frag,m_death,m_health,m_ammo,m_posx,m_posy):\n",
    "\n",
    "        ret_detail = {}\n",
    "\n",
    "        ret_detail['living'] = self.rewards['living']\n",
    "\n",
    "        if m_frag >= 0:\n",
    "            ret_detail['frag'] = (m_frag)*self.rewards['frag']\n",
    "            ret_detail['suicide'] = 0.0\n",
    "        else:\n",
    "            ret_detail['suicide'] = (m_frag*-1)*(self.rewards['suicide'])\n",
    "            ret_detail['frag'] = 0.0\n",
    "        \n",
    "        ret_detail['dist'] = int((math.sqrt((m_posx)**2 + (m_posy)**2))/self.dist_unit) * (self.rewards['dist'] * self.dist_unit)\n",
    "        \n",
    "        if m_health > 0:\n",
    "            ret_detail['medkit'] = self.rewards['medkit']\n",
    "            ret_detail['health_loss'] = 0.0\n",
    "        else:\n",
    "            ret_detail['medkit'] = 0.0\n",
    "            ret_detail['health_loss'] = (m_health)*self.rewards['health_loss'] * (-1)\n",
    "\n",
    "        ret_detail['ammo'] = (m_ammo)*self.rewards['ammo'] if m_ammo>0 else 0.0\n",
    "        \n",
    "        return ret_detail \n",
    "    \n",
    "    def respawn_pos(self,health,ammo,posx, posy):\n",
    "        self.origin_x = posx\n",
    "        self.origin_y = posy\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "\n",
    "    def new_episode(self,health,ammo,posx,posy):\n",
    "        self.respawn_pos(health,ammo,posx,posy)\n",
    "        self.pre_frag = 0\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.total_reward_detail={'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "    \n",
    "    def update_origin(self,pos_x, pos_y):\n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,q_network):\n",
    "        \n",
    "        self.q_network = q_network\n",
    "        \n",
    "#         self.image_buff = np.zeros(shape=(N_ADV,)+RESOLUTION)\n",
    "        self.image_buff = []\n",
    "        self.memory = []\n",
    "        self.batch = {'s1':[], 'action':[], 's2':[] ,'reward':[], 'reward_adv':[], 'isdemo':[]}\n",
    "        self.R = 0\n",
    "        \n",
    "        self.s1_record = np.zeros((1,N_ADV,)+RESOLUTION)\n",
    "        self.loss_one_record = 0\n",
    "        self.loss_adv_record = 0\n",
    "        self.loss_class_record = 0\n",
    "        \n",
    "    def calc_eps_step(self):\n",
    "            global frames\n",
    "\n",
    "            if frames<TOTAL_STEPS*LINEAR_EPS_START:\n",
    "                eps = EPS_START\n",
    "            elif frames>=TOTAL_STEPS*LINEAR_EPS_START and frames<TOTAL_STEPS*LINEAR_EPS_END:\n",
    "                eps = EPS_START + frames*(EPS_END-EPS_START)/(TOTAL_STEPS)\n",
    "            else:\n",
    "                eps = EPS_END\n",
    "            return eps\n",
    "        \n",
    "    def calc_eps_time(self):\n",
    "        if current_time < TOTAL_TIME * LINEAR_EPS_START:\n",
    "            eps = EPS_START\n",
    "        elif current_time >= TOTAL_TIME * LINEAR_EPS_START and current_time < TOTAL_TIME*LINEAR_EPS_END:\n",
    "            eps = EPS_START + current_time*(EPS_END-EPS_START)/(TOTAL_TIME)\n",
    "        else:\n",
    "            eps = EPS_END\n",
    "            \n",
    "        return eps\n",
    "\n",
    "    def act_eps_greedy(self,s1):\n",
    "\n",
    "        self.image_buff.append(s1)\n",
    "        ret_action = np.zeros((N_ACTION,))\n",
    "        if len(self.image_buff) == N_ADV + 1:\n",
    "            eps = self.calc_eps_time()\n",
    "\n",
    "            self.image_buff.pop(0)\n",
    "            \n",
    "            if random.random() > eps:\n",
    "                a_idx = self.q_network.predict_best_action(self.image_buff)\n",
    "            else:\n",
    "                a_idx = randint(0,N_ACTION-1)\n",
    "        else:\n",
    "            a_idx = randint(0,N_ACTION-1)\n",
    "                \n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "    \n",
    "    def act_greedy(self,s1):\n",
    "\n",
    "        self.image_buff.append(s1)\n",
    "        ret_action = np.zeros((N_ACTION,))\n",
    "        if len(self.image_buff) == N_ADV + 1:\n",
    "            eps = self.calc_eps_time()\n",
    "\n",
    "            self.image_buff.pop(0)\n",
    "            \n",
    "            a_idx = self.q_network.predict_best_action(self.image_buff)\n",
    "        else:\n",
    "            a_idx = randint(0,N_ACTION-1)\n",
    "\n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "    \n",
    "    def push_advantage(self,s1_,a_,r_,s2_,isterminal,isdemo):\n",
    "        self.memory.append((s1_,a_,r_,s2_,isdemo))\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.memory = []\n",
    "    \n",
    "    def push_to_batch(self, s1, action, s2, reward, reward_adv, isdemo):\n",
    "        self.batch['s1'].append(s1)\n",
    "        self.batch['action'].append(action)\n",
    "        self.batch['s2'].append(s2)\n",
    "        self.batch['reward'].append(reward)\n",
    "        self.batch['reward_adv'].append(reward_adv)\n",
    "        self.batch['isdemo'].append(isdemo)\n",
    "        return 0\n",
    "    \n",
    "    def clear_batch(self):\n",
    "        self.batch = {'s1':[], 'action':[], 's2':[] ,'reward':[], 'reward_adv':[], 'isdemo':[]}\n",
    "        return 0\n",
    "    \n",
    "    def make_batch_learn(self):\n",
    "        n = len(self.batch['action'])\n",
    "        s1 = np.zeros((n, N_ADV,)+RESOLUTION)\n",
    "        s2 = np.zeros((n, N_ADV,)+RESOLUTION)\n",
    "        for i in range(n):\n",
    "            s1[i, :i+1] = self.batch['s1'][:i+1]\n",
    "            s2[i, :i+1] = self.batch['s2'][:i+1]\n",
    "        \n",
    "        self.s1_record = s1[0:1]\n",
    "        self.loss_one_record, self.loss_adv_record, self.loss_class_record = \\\n",
    "        self.q_network.update_parameter_server_batch(s1, self.batch['action'], self.batch['reward'], \\\n",
    "                                                         self.batch['reward_adv'], s2, self.batch['isdemo']) \n",
    "        return 0\n",
    "    \n",
    "    def learn_advantage(self, isterminal):\n",
    "        \n",
    "        if len(self.memory)==N_ADV or isterminal:\n",
    "            tail_idx = len(self.memory)-1\n",
    "            \n",
    "            s1_buff = np.zeros((N_ADV, )+RESOLUTION)\n",
    "            for i in range(tail_idx+1):\n",
    "                s1_buff[i] = self.memory[i][0]\n",
    "            \n",
    "            for i in range(tail_idx,-1,-1):\n",
    "                s1,a,r,s2,d = self.memory[i]\n",
    "                if i==tail_idx:\n",
    "                    if not isterminal:\n",
    "#                         print(np.max(self.q_network.get_q_value(s1)[0]))\n",
    "                        self.R = np.max(self.q_network.get_q_value(s1_buff)[0])\n",
    "                        \n",
    "                    else:\n",
    "                        self.R = 0\n",
    "                else:\n",
    "                    self.R =  r + GAMMA*self.R\n",
    "            \n",
    "#                 self.q_network.train_push(s1,a,r,self.R,s2,d)\n",
    "                self.push_to_batch(s1,a,s2,r,self.R,d)\n",
    "            \n",
    "#             self.q_network.update_parameter_server()\n",
    "#             self.q_network.update_parameter_server_batch(self.batch['s1'], self.batch['action'], self.batch['reward'], \\\n",
    "#                                                          self.batch['reward_adv'], self.batch['s2'], self.batch['isdemo'])\n",
    "\n",
    "#             print(np.shape(self.batch['s1']))\n",
    "#             print(np.shape(self.batch['s2']))\n",
    "#             print(np.shape(self.batch['s2']))\n",
    "            self.make_batch_learn()\n",
    "            self.q_network.copy_learn2target()\n",
    "            self.R = 0\n",
    "            self.clear_memory()\n",
    "            self.clear_batch()\n",
    "            \n",
    "#             return self.q_network.calc_loss([s1],[a],[r],[self.R],[s2],[d])\n",
    "#         return 0.0,0.0,0.0\n",
    "    \n",
    "    def calc_loss(self):\n",
    "        \n",
    "        if len(self.memory) == N_ADV :\n",
    "            tail_idx = len(self.memory) - 1\n",
    "            s1_buff = np.ones((1, tail_idx+1, )+RESOLUTION) * np.nan\n",
    "            s2_buff = np.ones((1, tail_idx+1, )+RESOLUTION) * np.nan\n",
    "            for i in range(tail_idx+1):\n",
    "                s1_buff[0, i] = self.memory[i][0]\n",
    "                s2_buff[0, i] = self.memory[i][3]\n",
    "            \n",
    "            for i in range(tail_idx, -1, -1):\n",
    "                s1 , a, r, s2, d = self.memory[i]\n",
    "                if i == tail_idx :\n",
    "                    R = np.max(self.q_network.get_q_value(s1_buff)[0])\n",
    "                else:\n",
    "                    R = r * GAMMA * R\n",
    "                \n",
    "                _, last_action, last_r, _, last_d = self.memory[tail_idx]\n",
    "                \n",
    "                return [s1_buff] + self.q_network.calc_loss(s1_buff, [last_action], [last_r], [R] ,s2_buff ,[last_d])\n",
    "        \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def encode(pre_layer):\n",
    "        s = tf.shape(pre_layer)\n",
    "        return tf.reshape(pre_layer, shape=(-1,)+RESOLUTION)\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [1,6,6]\n",
    "        stride = [1,3,3]\n",
    "#         kernel_size = [6,6]\n",
    "#         stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "#         weights_init = tf.constant_initializer(2.0)\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,\\\n",
    "                                        biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "#         return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 64\n",
    "        kernel_size = [1,3,3]\n",
    "        stride = [1,2,2]\n",
    "#         kernel_size = [3,3]\n",
    "#         stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "#         return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        print(pre_layer)\n",
    "#         return tf.contrib.layers.flatten(pre_layer)\n",
    "        a = tf.shape(pre_layer)[1]\n",
    "        b = tf.shape(pre_layer)[2]\n",
    "        c = tf.shape(pre_layer)[3]\n",
    "        d = tf.shape(pre_layer)[4]\n",
    "        print(a,\",\",b,\",\",c,\",\",d)\n",
    "        return tf.reshape(pre_layer, shape=(-1, N_ADV * 2560))\n",
    "        \n",
    "    def fc1(pre_layer):\n",
    "        print((pre_layer))\n",
    "        num_outputs = 512\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def decode(pre_layer):\n",
    "        return tf.reshape(pre_layer, shape=(-1, N_ADV,512))\n",
    "    \n",
    "    def lstm(pre_layer, state):\n",
    "        batch_size = tf.shape(pre_layer)[0]\n",
    "        print(pre_layer)\n",
    "        temp = tf.reduce_max(state, axis=4)\n",
    "        temp = tf.reduce_max(temp, axis=3)\n",
    "        temp = tf.reduce_max(temp, axis=2)\n",
    "        lengh = tf.cast(tf.reduce_sum(tf.sign(temp) , axis=1),dtype=tf.int32) \n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(LSTM_SIZE)\n",
    "        rnn_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        rnn_out, state_out = tf.nn.dynamic_rnn(cell, pre_layer, initial_state=rnn_state, sequence_length=lengh,dtype=tf.float32)\n",
    "        out_idx = tf.range(0, batch_size) * N_ADV + (lengh  -1)\n",
    "        output = tf.gather(tf.reshape(rnn_out, [-1, LSTM_SIZE]), out_idx)\n",
    "        return output, lengh, rnn_out\n",
    "    \n",
    "    def q_value(pre_layer):\n",
    "        num_outputs = N_ACTION\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network which be shared in global\n",
    "class ParameterServer:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,N_ADV)+RESOLUTION, name=\"state1\")\n",
    "        self.a_ = tf.placeholder(tf.int32, shape=(None,), name=\"action\")\n",
    "        self.r_ = tf.placeholder(tf.float32, shape=(None,), name=\"reward\")\n",
    "        self.r_adv = tf.placeholder(tf.float32, shape=(None,), name=\"reward_adv\")\n",
    "        self.mergin_value = tf.placeholder(tf.float32,shape=(None,N_ACTION), name=\"mergin_value\")\n",
    "#         self.s1idx_ = tf.placeholder(tf.int32, shape=(None,), name=\"lengh_of_state\")\n",
    "        \n",
    "        with tf.variable_scope(\"parameter_server\",reuse=tf.AUTO_REUSE):      # スレッド名で重み変数に名前を与え、識別します（Name Space）\n",
    "            with tf.device(\"/gpu:0\"):\n",
    "                self.model = self._build_model()            # ニューラルネットワークの形を決定\n",
    "            \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "#         self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)    # loss関数を最小化していくoptimizerの定義です\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        with tf.variable_scope(\"summary\"):\n",
    "            self._build_summary()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        print(\"-------GLOBAL-------\")\n",
    "        for w in self.weights_params:\n",
    "            print(w)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \n",
    "#         self.enco = NetworkSetting.encode(self.state1_)\n",
    "        self.conv1 = NetworkSetting.conv1(self.state1_)\n",
    "        maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        self.conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "#         self.deco = NetworkSetting.decode(fc1)\n",
    "#         rnn, l, _ = NetworkSetting.lstm(self.deco, self.state1_)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "                \n",
    "        print(\"---------MODEL SHAPE-------------\")\n",
    "        print(self.state1_.get_shape())\n",
    "        print(self.conv1.get_shape())\n",
    "        print(self.conv2.get_shape())\n",
    "        print(reshape.get_shape())\n",
    "        print(fc1.get_shape())\n",
    "        print(q_value.get_shape())\n",
    "            \n",
    "        return q_value\n",
    "                \n",
    "    def _build_summary(self):\n",
    "        \n",
    "        self.loss_one = tf.placeholder(tf.float32,shape=(1,))\n",
    "        self.loss_n = tf.placeholder(tf.float32,shape=(1,))\n",
    "        self.loss_c = tf.placeholder(tf.float32,shape=(1,))\n",
    "        self.loss_l = tf.placeholder(tf.float32,shape=(1,))\n",
    "        \n",
    "        self.reward = tf.placeholder(tf.float32,shape=(1,))\n",
    "        self.frag = tf.placeholder(tf.int64,shape=(1,))\n",
    "        self.death = tf.placeholder(tf.int64,shape=(1,))\n",
    "        \n",
    "        summary_lo = tf.summary.scalar('loss_one',self.loss_one[0])\n",
    "        summary_ln = tf.summary.scalar('loss_nstep', self.loss_n[0])\n",
    "        summary_lc = tf.summary.scalar('loss_class', self.loss_c[0])\n",
    "        summary_ll = tf.summary.scalar('loss_l2',self.loss_l[0])\n",
    "\n",
    "        self.merged_loss = tf.summary.merge([summary_lo,summary_ln,summary_lc,summary_ll])\n",
    "        \n",
    "        conv1_display = tf.expand_dims(tf.transpose(self.conv1, perm=[0,1,4,2,3]), axis=5)\n",
    "        conv2_display = tf.expand_dims(tf.transpose(self.conv2, perm=[0,1,4,2,3]), axis=5)\n",
    "\n",
    "        state_shape = self.state1_.get_shape()\n",
    "        conv1_shape = conv1_display.get_shape()\n",
    "        conv2_shape = conv2_display.get_shape()\n",
    "        print(\"conv1_shape:\", conv1_shape)\n",
    "        print(\"conv2_shape:\",conv2_shape)\n",
    "        summary_state  = tf.summary.image('state',tf.reshape(self.state1_,[-1,state_shape[2], state_shape[3], state_shape[4]]),max_outputs = 1)\n",
    "        summary_conv1 = tf.summary.image('conv1',tf.reshape(conv1_display,[-1, conv1_shape[3], conv1_shape[4], conv1_shape[5]]),max_outputs = 1)\n",
    "        summary_conv2 = tf.summary.image('conv2',tf.reshape(conv2_display,[-1, conv2_shape[3], conv2_shape[4], conv2_shape[5]]),max_outputs = 1)\n",
    "\n",
    "        self.merged_image = tf.summary.merge([summary_state,summary_conv1,summary_conv2])\n",
    "        \n",
    "        summary_reward = tf.summary.scalar('reward',self.reward[0])\n",
    "        summary_frag = tf.summary.scalar('frag',self.frag[0])\n",
    "        summary_death = tf.summary.scalar('death',self.death[0])\n",
    "        \n",
    "        self.merged_testscore = tf.summary.merge([summary_reward,summary_frag,summary_death])\n",
    "        \n",
    "        self.merged_weights = tf.summary.merge([tf.summary.scalar('weights'+str(i),tf.reduce_mean(self.weights_params[i])) for i in range(len(self.weights_params))])\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(LOG_DIR,SESS.graph)\n",
    "\n",
    "    # write summary about LOSS and IMAGE\n",
    "    def write_summary(self,step,s1,loss_one,loss_n,loss_class,loss_l2):\n",
    "#         print(s1.shape)\n",
    "#         print(\"step:\",step,\"loss_one:\",loss_one, \"loss_n:\",loss_n, \"loss_class\",loss_class, \"loss_l2\",loss_l2)\n",
    "        if step%1 == 0:\n",
    "            m_s,m_i = SESS.run([self.merged_loss,self.merged_image],feed_dict= \\\n",
    "                               {self.state1_:s1,self.loss_one:[loss_one],self.loss_n:[loss_n],self.loss_c:[loss_class],self.loss_l:loss_l2})\n",
    "            self.writer.add_summary(m_s,step)\n",
    "            if step%1 == 0:\n",
    "                self.writer.add_summary(m_i,step)\n",
    "    \n",
    "    def write_records(self,step,r,f,d):\n",
    "#         r = np.array([[r]])\n",
    "#         f = np.array([[f]])\n",
    "#         d = np.array([[d]])\n",
    "        m = SESS.run(self.merged_testscore,feed_dict={self.reward:[r],self.frag:[f],self.death:[d]})\n",
    "        self.writer.add_summary(m,step)\n",
    "        \n",
    "    def write_weights(self, step):\n",
    "        m = SESS.run(self.merged_weights)\n",
    "        self.writer.add_summary(m, step)\n",
    "        return 0\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.saver.save(SESS, MODEL_PATH+\"/model.ckpt\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.saver.restore(SESS, MODEL_PATH+\"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name,parameter_server):\n",
    "        self.name = name\n",
    "\n",
    "        self.s1 = np.zeros(shape=(120,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.s2 = np.zeros(shape=(120,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]),dtype=np.float32)\n",
    "        self.reward = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.reward_adv = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.action = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.isdemo = np.empty(shape=(120,),dtype=np.float32)\n",
    "        self.queue_pointer = 0\n",
    "        \n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,N_ADV,)+RESOLUTION, name=\"A\")\n",
    "        self.state2_ = tf.placeholder(tf.float32,shape=(None,N_ADV,)+RESOLUTION, name=\"B\")\n",
    "        self.a_ = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.r_ = tf.placeholder(tf.float32, shape=(None,))\n",
    "        self.r_adv = tf.placeholder(tf.float32, shape=(None,))\n",
    "        self.isdemo_ = tf.placeholder(tf.float32,shape=(None,))\n",
    "        self.mergin_value = tf.placeholder(tf.float32,shape=(None,N_ACTION))\n",
    "#         self.s1idx_ = tf.placeholder(tf.int32, shape = (None,))\n",
    "        \n",
    "        with tf.variable_scope(self.name+\"_target\", reuse=tf.AUTO_REUSE):\n",
    "            self.model_t, self.len_s2 = self._model(self.state2_)\n",
    "        with tf.variable_scope(self.name+\"_train\"):\n",
    "            self.model_l, self.len_s1 = self._model(self.state1_)\n",
    "\n",
    "        self._build_graph(parameter_server)\n",
    "            \n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self,state):\n",
    "        \n",
    "#         enco = NetworkSetting.encode(state)\n",
    "        conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "#         deco = NetworkSetting.decode(fc1)\n",
    "#         rnn, lengh, _ = NetworkSetting.lstm(deco, state)\n",
    "#         self.deco = NetworkSetting.decode(fc1)\n",
    "#         self.rnn, lengh, _ = NetworkSetting.lstm(self.deco, state)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "        \n",
    "        return q_value, 0\n",
    "\n",
    "    def _build_graph(self,parameter_server):\n",
    "        \n",
    "#         self.best_action = tf.argmax(self.model_l, axis=0)\n",
    "        self.prob_action = tf.nn.softmax(self.model_l, axis=1)\n",
    "\n",
    "        q_model_t = tf.where(tf.equal(self.len_s2, self.len_s1) , self.model_t,tf.zeros_like(self.model_t))\n",
    "        self.test1 = q_model_t\n",
    "        \n",
    "#         self.loss_one = tf.square(tf.stop_gradient(self.r_ + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "#         self.loss_adv = tf.square(tf.stop_gradient(self.r_adv + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "        self.loss_one = tf.reduce_mean(tf.abs(tf.stop_gradient(self.r_ + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1)))\n",
    "        self.loss_adv = tf.reduce_mean(tf.abs(tf.stop_gradient(self.r_adv + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1)))\n",
    "        target = tf.stop_gradient(tf.reduce_max(self.model_l + self.mergin_value))\n",
    "        idx = tf.transpose([tf.range(tf.shape(self.model_l)[0]), self.a_])\n",
    "        self.loss_class =  tf.reduce_mean((target- tf.gather_nd(self.model_l,indices=idx)) * self.isdemo_)\n",
    "        \n",
    "        self.loss_total = self.loss_one + LAMBDA1 * self.loss_adv + LAMBDA2 * self.loss_class\n",
    "        \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"_train\")\n",
    "        self.weights_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"_target\")\n",
    "        self.grads = tf.gradients(self.loss_total,self.weights_params)\n",
    "        self.copy_params = [t.assign(l) for l,t in zip(self.weights_params, self.weights_params_target)]\n",
    "        \n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))\n",
    "\n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params,parameter_server.weights_params)]\n",
    "\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params)]\n",
    "    \n",
    "    def pull_parameter_server(self):\n",
    "        SESS.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self):\n",
    "        SESS.run(self.push_local_weight_params)\n",
    "        \n",
    "    def show_weights(self):\n",
    "        hoge = SESS.run(self.weights_params)\n",
    "        for i in range(len(hoge)):\n",
    "            print(hoge[i])\n",
    "    \n",
    "    def update_parameter_server_batch(self, s1, a, r, r_adv, s2, isdemo):\n",
    "        if np.ndim(s1) == 4:\n",
    "            s1 = np.array([s1])\n",
    "        if np.ndim(s2) == 4:\n",
    "            s2 = np.array([s2])\n",
    "        mergin = [[0.8*(not(a[j]==i)) for i in range(N_ACTION)] for j in range(np.shape(a)[0])]\n",
    "\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_:r,self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin,self.isdemo_:isdemo}\n",
    "        val = SESS.run([self.update_global_weight_params, self.loss_one, self.loss_adv, self.loss_class],feed_dict)\n",
    "#         val = SESS.run([self.update_global_weight_params, self.loss_one, self.loss_adv, self.loss_class,self.rnn, self.grads, self.deco],feed_dict)\n",
    "#         print(\"--------------------------\")\n",
    "#         print(\"deco:\",[np.mean(v) for v in val[6]])\n",
    "#         print(\"rnn:\",[np.mean(v) for v in val[4]])\n",
    "#         print(\"grad\", [np.mean(v) for v in val[5]])\n",
    "        return val[1], val[2], val[3]\n",
    "\n",
    "        \n",
    "    def update_parameter_server(self):\n",
    "        if self.queue_pointer > 0:\n",
    "            s1 = np.ones((self.queue_pointer, N_ADV, )+RESOLUTION) * np.nan\n",
    "            s2 = np.ones((self.queue_pointer, N_ADV, )+RESOLUTION) * np.nan\n",
    "            for i in range(self.queue_pointer):\n",
    "                s1[i, 0:i] = self.s1[0:i]\n",
    "                s2[i, 0:i] = self.s2[0:i]\n",
    "            r = self.reward[0:self.queue_pointer]\n",
    "            a = self.action[0:self.queue_pointer]\n",
    "            r_adv = self.reward_adv[0:self.queue_pointer]\n",
    "            mergin = [[0.8*(not(a[j]==i)) for i in range(N_ACTION)] for j in range(self.queue_pointer)]\n",
    "            isdemo = self.isdemo[0:self.queue_pointer]\n",
    "            \n",
    "            feed_dict = {self.state1_: s1, self.a_:a, self.r_:r, self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin, self.isdemo_:isdemo}\n",
    "#             _, l, m_l, m_t = SESS.run([self.update_global_weight_params, self.loss_total, self.model_l, self.model_t],feed_dict)\n",
    "            SESS.run(self.update_global_weight_params,feed_dict)\n",
    "            self.queue_pointer = 0\n",
    "            \n",
    "    def predict_best_action(self, s1):\n",
    "        if np.ndim(s1)==4:\n",
    "            s1 = np.array([s1])\n",
    "        \n",
    "#         print(SESS.run(self.model_l, {self.state1_:s1}))\n",
    "#         return SESS.run(self.best_action,{self.state1_:s1})\n",
    "\n",
    "        probs = SESS.run(self.prob_action, {self.state1_:s1})\n",
    "#         print(probs)\n",
    "\n",
    "        return [np.random.choice(N_ACTION, p=p) for p in probs]\n",
    "\n",
    "    def get_q_value(self,s1):\n",
    "        if np.ndim(s1)==4:\n",
    "            s1 = np.array([s1])\n",
    "            \n",
    "        return SESS.run(self.model_l,{self.state1_:s1})\n",
    "    \n",
    "    def calc_loss(self, s1, a, r, r_adv, s2, isdemo):\n",
    "        mergin = [[0.8*(not(a[j]==i)) for i in range(N_ACTION)] for j in range(len(a))]\n",
    "        \n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_:r,self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin,self.isdemo_:isdemo}\n",
    "        return SESS.run([self.loss_one, self.loss_adv, self.loss_class],feed_dict)\n",
    "    \n",
    "    def copy_learn2target(self):\n",
    "        SESS.run(self.copy_params)\n",
    "\n",
    "    def train_push(self,s1,a,r,r_adv,s2,isdemo):\n",
    "        # Push obs to make batch\n",
    "        self.s1[self.queue_pointer] = s1\n",
    "        self.s2[self.queue_pointer] = s2\n",
    "        self.action[self.queue_pointer] = a\n",
    "        self.reward[self.queue_pointer] = r\n",
    "        self.reward_adv[self.queue_pointer] = r_adv\n",
    "        self.isdemo[self.queue_pointer] = isdemo\n",
    "        self.queue_pointer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"learning\":\n",
    "    \n",
    "    frames = 0\n",
    "    runout = False\n",
    "    current_time = 0\n",
    "    start_time_async = 0\n",
    "\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = True\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "    \n",
    "    threads = []\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        parameter_server = ParameterServer()\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        for i in range(N_WORKERS):            \n",
    "            threads.append(WorkerThread(\"learning_\"+str(i),parameter_server))\n",
    "\n",
    "        pre_env = Environment(\"pre_env\",parameter_server)\n",
    "        test_env = Environment(\"test_env\", parameter_server)\n",
    "\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "    threads[0].environment.summary=True\n",
    "\n",
    "    time.sleep(5.0)\n",
    "\n",
    "    print(\"---LOADING DEMO---\")\n",
    "    pre_env.load_demonstration()\n",
    "    print(\"---PRE LEARNING---\")\n",
    "    start_time_pre = datetime.datetime.now()\n",
    "    pre_env.run_pre_learning()\n",
    "    \n",
    "#     if SAVE_FILE == True:\n",
    "#         print(\"---SAVING GIF---\")\n",
    "#         test_env.run_test(True)\n",
    "\n",
    "#     print(\"---MULTI THREAD LEARNING---\")\n",
    "#     start_time_async = datetime.datetime.now()\n",
    "#     for worker in threads:\n",
    "#         job = lambda: worker.run()      # この辺は、マルチスレッドを走らせる作法だと思って良い\n",
    "#         t = threading.Thread(target=job)\n",
    "#         t.start()\n",
    "\n",
    "#     test_frame = 0\n",
    "#     while True:\n",
    "#         if frames >= test_frame and frames<test_frame+1000:\n",
    "#             r,f,d = test_env.run_test()\n",
    "#             if SAVE_FILE == True:\n",
    "#                 parameter_server.write_weights(frames)\n",
    "#                 parameter_server.write_records(frames,r,f,d)\n",
    "#             test_frame += 1000\n",
    "#         elif frames >= test_frame+1000:\n",
    "#             print(\"TEST at %d~%d step cant be finished\"%(test_frame, test_frame+1000-1))\n",
    "#             test_frame += 1000\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "#         if datetime.datetime.now() > TIME_LEARN + start_time_async:\n",
    "#             runout = True\n",
    "#             break\n",
    "#     print(\"*****************************\\nTIME to PRE LEARNING:%.3f [sec]\\n*****************************\"%(datetime.datetime.now()-start_time_pre).seconds)\n",
    "#     print(\"*****************************\\nTIME to ASYNC LEARNING:%.3f [sec]\\n*****************************\"%(datetime.datetime.now()-start_time_async).seconds)\n",
    "\n",
    "#     print(\"---LEARNING PHASE IS FINISHED---\")\n",
    "#     test_env.run_test()\n",
    "\n",
    "    if SAVE_FILE == True:\n",
    "        print(\"---SAVING_MODEL---\")\n",
    "        parameter_server.save_model()\n",
    "        print(\"---SAVING GIF---\")\n",
    "        test_env.run_test(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"test\":\n",
    "    \n",
    "    frames = 0\n",
    "    runout = False\n",
    "    current_time = 0\n",
    "    start_time_async = 0\n",
    "\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = True\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        parameter_server = ParameterServer()\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        test_env = Environment(\"test_env\", parameter_server)\n",
    "\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "    parameter_server.load_model()\n",
    "\n",
    "    test_env.run_test(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    frames = 0\n",
    "    runout = False\n",
    "    current_time = 0\n",
    "    start_time_async = 0\n",
    "\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = True\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "    \n",
    "    test_env = Environment(\"test\", ParameterServer())\n",
    "    h = test_env.load_demonstration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(h))\n",
    "for t in h:\n",
    "    print(t[0].reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
