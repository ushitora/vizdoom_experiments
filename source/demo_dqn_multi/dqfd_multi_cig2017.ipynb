{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from random import sample, randint, random\n",
    "import time,random,threading,datetime\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import sys, os, glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from game_instance import GameInstance\n",
    "from global_constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"./logs/logs_test\"\n",
    "MODEL_PATH = \"./models/model_20180918\"\n",
    "PREMODEL_PATH = \"./models/premodel_test\"\n",
    "GIF_PATH = \"./gifs/test.gif\"\n",
    "PREGIF_PATH = \"./gifs/pre_v13.gif\"\n",
    "\n",
    "__name__ = \"test\"\n",
    "\n",
    "DEMO_PATH = DEMO_PATH[:3]\n",
    "\n",
    "TIME_LEARN = datetime.timedelta(minutes = 20)\n",
    "TIME_PRELEARN = datetime.timedelta(minutes = 1)\n",
    "\n",
    "SAVE_FILE = True\n",
    "\n",
    "TOTAL_STEPS = 10000\n",
    "TOTAL_TIME = TIME_LEARN.seconds\n",
    "TOTAL_TIME_PRE = TIME_PRELEARN.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --class for Thread　-------\n",
    "class WorkerThread:\n",
    "    # Each Thread has an Environment to run Game and Learning.\n",
    "    def __init__(self, thread_name, parameter_server, replay_memory, isLearning=True):\n",
    "        self.environment = Environment(thread_name, parameter_server, replay_memory)\n",
    "        print(thread_name,\" Initialized\")\n",
    "        self.isLearning = isLearning\n",
    "\n",
    "    def run(self):\n",
    "        if self.isLearning:\n",
    "            while True:\n",
    "                if not self.environment.finished:\n",
    "                    self.environment.run()\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            # Run Test Environment\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,name, parameter_server,replay_memory, summary=False):\n",
    "        \n",
    "        self.name = name\n",
    "        self.game = GameInstance(DoomGame(), name=self.name, config_file_path=CONFIG_FILE_PATH, rewards=REWARDS,n_adv=N_ADV)\n",
    "        \n",
    "        self.network = NetworkLocal(name, parameter_server)\n",
    "        self.agent = Agent(self.network)\n",
    "        self.replay_memory = replay_memory\n",
    "        \n",
    "        self.local_step = 0\n",
    "        \n",
    "        self.finished = False\n",
    "        \n",
    "        self.summary = summary\n",
    "        self.parameter_server = parameter_server\n",
    "        \n",
    "        self.log_buff = [np.zeros(shape=(N_ADV,) + RESOLUTION, dtype=np.float32), \\\n",
    "                         np.zeros(shape=(N_ADV,), dtype=np.int8), \\\n",
    "                         np.zeros(shape=(N_ADV,) +  RESOLUTION, dtype=np.float32), \\\n",
    "                         np.zeros(shape=(N_ADV,), dtype=np.float32), \\\n",
    "                         np.ones(shape=(N_ADV,), dtype=np.int8), \\\n",
    "                         np.zeros(shape=(N_ADV,), dtype=np.int8)]\n",
    "        self.buff_pointer = 0\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3 and img.shape[0]==3:\n",
    "            img = img.transpose(1,2,0)\n",
    "        \n",
    "        img = skimage.transform.resize(img, RESOLUTION, mode=\"constant\")\n",
    "        img = img.astype(np.float32)\n",
    "#         img = (img)/255.0\n",
    "        return img\n",
    "    \n",
    "    def run_pre_learning(self):\n",
    "        global frames, start_time_pre,current_time\n",
    "        \n",
    "        step = 0\n",
    "        while True:\n",
    "            \n",
    "            self.network.pull_parameter_server()\n",
    "            tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight, a_onehot = self.make_batch()\n",
    "#             print(is_weight)\n",
    "            l_one, l_adv, l_cls, l_l2 = self.network.update_parameter_server_batch(s1,actions,rewards,rewards_adv,s2,isdemos, is_weight, a_onehot, isterminals)\n",
    "\n",
    "#             print(\"l_one:{} l_adv:{}, l_cls:{}, l_l2:{}\".format(l_one, l_adv, l_cls, l_l2))\n",
    "            self.replay_memory.batch_update(tree_idx, l_one+l_cls+l_l2, isdemos)\n",
    "            if step%RECORD_INTERVAL == 0 and SAVE_FILE == True:\n",
    "#                 lo, la, lc = self.network.calc_loss(s1[0:1],actions[0:1],rewards[0:1],rewards_adv[0:1],s2[0:1],isdemos[0:1])\n",
    "                self.parameter_server.write_weights(frames)\n",
    "                self.parameter_server.write_loss(frames, np.mean(l_one), np.mean(l_adv), np.mean(l_cls), l_l2)\n",
    "                if step % (RECORD_INTERVAL*10) == 0:\n",
    "                    self.parameter_server.write_images(frames, s1[0:1])\n",
    "            if step%FREQ_COPY==0:\n",
    "                self.network.copy_learn2target()\n",
    "                \n",
    "            \n",
    "            if step % TEST_INTERVAL == 0 and SAVE_FILE==True:\n",
    "                r,frag,death = self.run_test(frames)\n",
    "                self.parameter_server.write_records(frames, r, frag,death)\n",
    "            \n",
    "            step += 1\n",
    "            frames += 1\n",
    "            \n",
    "            current_time = datetime.datetime.now().timestamp() - start_time_pre.timestamp()\n",
    "            \n",
    "            if datetime.datetime.now() > TIME_PRELEARN + start_time_pre:\n",
    "                runout = True\n",
    "                break\n",
    "\n",
    "    def run(self):\n",
    "        global current_time, frames, start_time_async, N_EPISODES\n",
    "        \n",
    "        step = 0\n",
    "        while True:\n",
    "            self.network.pull_parameter_server()\n",
    "\n",
    "            if len(self.replay_memory) > BATCH_SIZE:\n",
    "                tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight, a_onehot = self.make_batch()\n",
    "                l_one, l_adv, l_cls, l_l2 = self.network.update_parameter_server_batch(s1,actions,rewards,rewards_adv,s2,isdemos, is_weight, a_onehot, isterminals)\n",
    "                if step%RECORD_INTERVAL == 0 and SAVE_FILE == True and self.summary == True:\n",
    "                    self.parameter_server.write_eps(frames, float(self.agent.calc_eps_time()))\n",
    "                    self.parameter_server.write_weights(frames)\n",
    "                    self.parameter_server.write_loss(frames, np.mean(l_one), np.mean(l_adv), np.mean(l_cls), l_l2)\n",
    "                    if step % (RECORD_INTERVAL*10) == 0:\n",
    "                        self.parameter_server.write_images(frames, s1[0:1])\n",
    "\n",
    "            s1_ = self.preprocess(self.game.get_screen_buff())\n",
    "            action = self.agent.act_eps_greedy(s1_)\n",
    "            r,_ = self.game.make_action(action, FRAME_REPEAT)\n",
    "#             self.replay_memory.batch_update(tree_idx, l_cls + 0.01, isdemos)\n",
    "\n",
    "            if step%FREQ_COPY==0:\n",
    "                self.network.copy_learn2target()\n",
    "            \n",
    "            s2_ = self.preprocess(self.game.get_screen_buff()) if not self.game.is_episode_finished() else np.zeros_like(s1_)\n",
    "            \n",
    "            self.add_buff(s1_, action.index(1), s2_, r, self.game.is_episode_finished(), False)\n",
    "            self.buff_pointer += 1\n",
    "            \n",
    "            if (self.game.is_player_dead()):\n",
    "                self.game.respawn_player()\n",
    "            \n",
    "            if self.game.is_episode_finished():\n",
    "                N_EPISODES += 1\n",
    "                self.game.new_episode(BOTS_NUM)\n",
    "                while(self.buff_pointer < N_ADV):\n",
    "                    self.add_buff(np.zeros_like(s1_), -1, np.zeros_like(s1_), 0, True, False)\n",
    "                    self.buff_pointer += 1\n",
    "                self.replay_memory.store(self.log_buff)\n",
    "                self.clear_buff()\n",
    "                self.agent.clear_memory()\n",
    "                \n",
    "            if self.buff_pointer == N_ADV:\n",
    "                self.replay_memory.store(self.log_buff)\n",
    "                self.clear_buff()\n",
    "\n",
    "            step += 1\n",
    "            frames += 1\n",
    "            \n",
    "            current_time = datetime.datetime.now().timestamp() - start_time_async.timestamp()\n",
    "            \n",
    "            if runout == True:\n",
    "                self.finished = True\n",
    "                break\n",
    "\n",
    "        return 0\n",
    "                \n",
    "    def make_batch(self):\n",
    "        \n",
    "        tree_idx, batch, is_weight = self.replay_memory.sample(BATCH_SIZE)\n",
    "#         tree_idx,batch,is_weight = self.replay_memory.sample_uniform(BATCH_SIZE)\n",
    "\n",
    "        s1 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)\n",
    "        s2 = np.zeros((BATCH_SIZE , N_ADV,)+RESOLUTION,dtype=np.float32)\n",
    "        actions = np.zeros((BATCH_SIZE ,),dtype=np.int8)\n",
    "        rewards = np.zeros((BATCH_SIZE, ),dtype=np.float32)\n",
    "        rewards_adv = np.zeros((BATCH_SIZE,),dtype=np.float32)\n",
    "        isterminals = np.zeros((BATCH_SIZE, ),dtype=np.int8)\n",
    "        isdemos = np.zeros((BATCH_SIZE,),dtype=np.int8)\n",
    "        a_onehot = np.zeros((BATCH_SIZE, N_AGENT_ACTION,), dtype=np.int8)\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            isterminal = (batch[i][4] == 1)\n",
    "            s1[i] = batch[i][0]\n",
    "            s2[i] = batch[i][2]\n",
    "            isterminals[i] = 1 if isterminal.any() else 0\n",
    "            actions[i] = batch[i][1][isterminal][0] if isterminal.any() else batch[i][1][-1]\n",
    "            rewards[i] = batch[i][3][isterminal][0] if isterminal.any() else batch[i][3][-1]\n",
    "            isdemos[i] = batch[i][5][-1]\n",
    "            rewards_adv[i] = sum([r * GAMMA**j for j,r in zip(range(len(batch[i][3])-1,0,-1), batch[i][3])])\n",
    "            a_onehot[i][actions[i]] = 1\n",
    "            \n",
    "        return tree_idx, s1, actions, s2, rewards, rewards_adv, isterminals, isdemos, is_weight, a_onehot\n",
    "    \n",
    "    def run_test(self, global_step, gif_buff=None reward_buff=None, show_result=True):\n",
    "        \n",
    "        self.game.new_episode(BOTS_NUM)\n",
    "        \n",
    "        #Copy params from global\n",
    "        self.network.pull_parameter_server()\n",
    "\n",
    "        step = 0\n",
    "        gif_img = []\n",
    "        total_reward = 0\n",
    "        total_detail = {}\n",
    "        while not self.game.is_episode_finished():\n",
    "            s1_row = self.game.get_screen_buff()\n",
    "            s1 = self.preprocess(s1_row)\n",
    "            if gif_buff is not None:\n",
    "                gif_img.append(s1_row.transpose(1,2,0))\n",
    "            action = self.agent.act_greedy(s1)\n",
    "            engine_action = self.convert_action_agent2engine(action.index(1))\n",
    "            reward,reward_detail = self.game.make_action(engine_action,FRAME_REPEAT)\n",
    "            isterminal = self.game.is_episode_finished()\n",
    "            total_reward += reward\n",
    "            for k in reward_detail.keys():\n",
    "                if not k in total_detail.keys():\n",
    "                    total_detail[k] = reward_detail[k]\n",
    "                else:\n",
    "                    total_detail[k] += reward_detail[k]\n",
    "            step += 1\n",
    "            if reward_buff is not None:\n",
    "                reward_buff.append((engine_action, reward))\n",
    "            \n",
    "            if (self.game.is_player_dead()):\n",
    "                self.game.respawn_player()\n",
    "        \n",
    "        save_img = []\n",
    "        if gif_buff is not None:\n",
    "            print(np.shape(gif_img))\n",
    "            for i in range(len(gif_img)):\n",
    "                save_img.append(Image.fromarray(np.uint8(gif_img[i])))\n",
    "            gif_buff += save_img\n",
    "#             save_img[0].save(GIF_PATH,save_all=True,append_images=save_img[1:])\n",
    "        if show_result == True:\n",
    "            print(\"----------TEST at %d step-------------\"%(global_step))\n",
    "            print(\"FRAG:\",self.game.get_frag_count(), \"DEATH:\",self.game.get_death_count())\n",
    "            print(\"REWARD\",total_reward)\n",
    "            print(total_detail)\n",
    "        return total_reward, self.game.get_frag_count(), self.game.get_death_count()\n",
    "    \n",
    "    def load_demonstration(self):\n",
    "        \n",
    "        for d in DEMO_PATH[:]:\n",
    "            print(\"Loading \", d)\n",
    "            file = h5py.File(d,\"r\")\n",
    "            episode_list = list(file.keys())[1:]\n",
    "    #         episode_list = episode_list[:1]\n",
    "            self.clear_buff()\n",
    "\n",
    "            for e in episode_list:\n",
    "                n_steps = file[e+\"/states\"].shape[0]\n",
    "                states = file[e+\"/states\"][:]\n",
    "                actions = file[e+\"/action\"][:]\n",
    "                frags = file[e+\"/frag\"][:]\n",
    "                deaths = file[e+\"/death\"][:]\n",
    "                health = file[e+\"/health\"][:]\n",
    "                ammo = file[e+\"/ammo\"][:]\n",
    "                posx = file[e+\"/posx\"][:]\n",
    "                posy = file[e+\"/posy\"][:]\n",
    "\n",
    "                originx = 0\n",
    "                originy = 0\n",
    "\n",
    "                for i in range(n_steps):\n",
    "\n",
    "                    if i % N_ADV == 0:\n",
    "                        originx = posx[i]\n",
    "                        originy = posy[i]\n",
    "\n",
    "                    if not i == n_steps - 1:\n",
    "\n",
    "                        m_frag = frags[i+1] - frags[i]\n",
    "                        m_death = deaths[i+1] - deaths[i]\n",
    "                        m_health = health[i+1] - health[i]\n",
    "                        m_ammo = ammo[i+1] - ammo[i]\n",
    "                        m_posx = abs(posx[i] - originx)\n",
    "                        m_posy = abs(posy[i] - originy)\n",
    "\n",
    "                        r,_ = self.game.get_reward(m_frag, m_death, m_health, m_ammo, m_posx, m_posy)\n",
    "\n",
    "                        s1 = self.preprocess(states[i])\n",
    "                        s2 = self.preprocess(states[i+1])\n",
    "                        agent_action_idx = self.convert_action_engine2agent(actions[i].tolist())\n",
    "\n",
    "                        self.add_buff(s1, agent_action_idx, s2, r, False, True)\n",
    "                        self.buff_pointer += 1\n",
    "                    else:\n",
    "\n",
    "                        r,_ = self.game.get_reward(m_frag,m_death,m_health,m_ammo,m_posx,m_posy) \n",
    "\n",
    "                        s1 = self.preprocess(states[i])\n",
    "                        agent_action_idx = self.convert_action_engine2agent(actions[i].tolist())\n",
    "                        self.add_buff(s1,agent_action_idx, np.zeros_like(s1), r,True, True)\n",
    "                        self.buff_pointer += 1\n",
    "                        while(self.buff_pointer < N_ADV):\n",
    "                            self.add_buff(np.zeros_like(s1), -1, np.zeros_like(s1), 0, True, True)\n",
    "                            self.buff_pointer += 1\n",
    "\n",
    "                    if self.buff_pointer == N_ADV:\n",
    "                        self.replay_memory.store(self.log_buff)\n",
    "                        self.clear_buff()\n",
    "                        \n",
    "        self.replay_memory.set_n_permanent_data(len(self.replay_memory))\n",
    "        print(\"number of demondtration: %d\"%(self.replay_memory.permanent_data))\n",
    "        return 0\n",
    "\n",
    "    def convert_action_engine2agent(self,engine_action):\n",
    "        assert type(engine_action) == type(list()), print(\"type: \", type(engine_action))\n",
    "\n",
    "        ans = 0\n",
    "        for i, e_a in enumerate(engine_action):\n",
    "            ans += e_a * 2**i\n",
    "\n",
    "        return ans\n",
    "    \n",
    "    def convert_action_agent2engine(self,agent_action):\n",
    "        assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print(\"type(agent_action)=\",type(agent_action))\n",
    "        ans = []\n",
    "        for i in range(6):\n",
    "            ans.append(agent_action%2)\n",
    "            agent_action = int(agent_action / 2)\n",
    "        return ans\n",
    "    \n",
    "    def add_buff(self, s1,a,s2,r,isterminal, isdemo):\n",
    "        self.log_buff[0][self.buff_pointer] = s1\n",
    "        self.log_buff[2][self.buff_pointer] = s2\n",
    "        self.log_buff[1][self.buff_pointer] = a\n",
    "        self.log_buff[3][self.buff_pointer] = r\n",
    "        self.log_buff[4][self.buff_pointer] = isterminal\n",
    "        self.log_buff[5][self.buff_pointer] = isdemo\n",
    "        return 0\n",
    "    \n",
    "    def clear_buff(self):\n",
    "        self.log_buff = [np.zeros(shape=(N_ADV,) + RESOLUTION, dtype=np.float32), \\\n",
    "                         np.zeros(shape=(N_ADV,), dtype=np.int8), \\\n",
    "                         np.zeros(shape=(N_ADV,) +  RESOLUTION, dtype=np.float32), \\\n",
    "                         np.zeros(shape=(N_ADV,), dtype=np.float32), \\\n",
    "                         np.ones(shape=(N_ADV,), dtype=np.int8), \\\n",
    "                         np.zeros(shape=(N_ADV,), dtype=np.int8)]\n",
    "        self.buff_pointer = 0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling should not execute when the tree is not full !!!\n",
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity, permanent_data=0):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # stores not probabilities but priorities !!!\n",
    "        self.data = np.zeros(capacity, dtype=object)  # stores transitions\n",
    "        self.permanent_data = permanent_data  # numbers of data which never be replaced, for demo data protection\n",
    "        assert 0 <= self.permanent_data <= self.capacity  # equal is also illegal\n",
    "        self.full = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.data_pointer\n",
    "\n",
    "    def set_n_permanent_data(self,n_permanent_data):\n",
    "        self.permanent_data = n_permanent_data\n",
    "        self.data_pointer = self.permanent_data\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, p)\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.full = True\n",
    "            self.data_pointer = self.data_pointer % self.capacity + self.permanent_data  # make sure demo data permanent\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if v <= self.tree[left_child_idx]:\n",
    "                parent_idx = left_child_idx\n",
    "            else:\n",
    "                v -= self.tree[left_child_idx]\n",
    "                parent_idx = right_child_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    epsilon = 0.001  # small amount to avoid zero priority\n",
    "    demo_epsilon = 1.0  # 1.0  # extra\n",
    "    alpha = 0.4  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.6  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.00001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity, permanent_data=0):\n",
    "        self.permanent_data = permanent_data\n",
    "        self.tree = SumTree(capacity, permanent_data)\n",
    "#         self.data_name = data_name\n",
    "\n",
    "    def set_n_permanent_data(self, n):\n",
    "        self.permanent_data = n\n",
    "        self.tree.set_n_permanent_data(self.permanent_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree)\n",
    "\n",
    "    def full(self):\n",
    "        return self.tree.full\n",
    "\n",
    "    def store(self, transition):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        self.tree.add(max_p, transition)  # set the max_p for new transition\n",
    "        \n",
    "    def sample_uniform(self, n):\n",
    "        num_data = len(self.tree)\n",
    "        assert num_data > n, print(\"num_data:{}\".format(num_data))\n",
    "        idx = np.random.randint(0, num_data, (n,))\n",
    "        b_memory = np.empty((n, ), dtype=object)\n",
    "        b_memory = self.tree.data[idx]\n",
    "        ISWeights = np.ones((n,))\n",
    "        return idx,b_memory,ISWeights\n",
    "            \n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx = np.empty((n,), dtype=np.int32)\n",
    "#         b_memory = np.empty((n, self.tree.data[0].size), dtype=object)\n",
    "        b_memory = np.empty((n,), dtype=object)\n",
    "        ISWeights = np.empty((n,))\n",
    "        pri_seg = self.tree.total_p / n\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "        size_replay = len(self.tree)\n",
    "\n",
    "        if self.tree.full:\n",
    "            min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p\n",
    "            assert min_prob > 0, \"min_prob={}\".format(min_prob)\n",
    "\n",
    "            for i in range(n):\n",
    "                v = np.random.uniform(pri_seg * i, pri_seg * (i + 1))\n",
    "                idx, p, data = self.tree.get_leaf(v)  # note: idx is the index in self.tree.tree\n",
    "                prob = p / self.tree.total_p\n",
    "                ISWeights[i] = np.power(prob*size_replay, -self.beta)\n",
    "#                 ISWeights[i] = np.power(prob/min_prob, -self.beta)\n",
    "#                 ISWeights[i] = prob/min_prob\n",
    "                b_idx[i], b_memory[i] = idx, data\n",
    "        else:\n",
    "            min_prob = np.min(self.tree.tree[self.tree.capacity-1:self.tree.capacity+self.tree.data_pointer-1]) / self.tree.total_p\n",
    "            assert min_prob > 0, \"min_prob={}\".format(min_prob)\n",
    "\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    v = np.random.uniform(self.abs_err_upper, pri_seg * (i + 1))\n",
    "                else:\n",
    "                    v = np.random.uniform(pri_seg * i, pri_seg * (i + 1))\n",
    "                idx, p, data = self.tree.get_leaf(v)  # note: idx is the index in self.tree.tree\n",
    "                prob = p / self.tree.total_p\n",
    "#                 ISWeights[i] = np.power(prob/min_prob, -self.beta)\n",
    "                ISWeights[i] = np.power(prob*size_replay, -self.beta)\n",
    "#                 ISWeights[i] = prob/min_prob]\n",
    "                b_idx[i], b_memory[i] = idx, data\n",
    "\n",
    "        return b_idx, b_memory, ISWeights  # note: b_idx stores indexes in self.tree.tree, not in self.tree.data !!!\n",
    "\n",
    "    # update priority\n",
    "    def batch_update(self, tree_idxes, abs_errors ,is_demo):\n",
    "        for i, d in enumerate(is_demo):\n",
    "            if d == True:\n",
    "                abs_errors[i] += self.demo_epsilon\n",
    "            else:\n",
    "                abs_errors[i] += self.epsilon\n",
    "        \n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idxes, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,q_network):\n",
    "        \n",
    "        self.q_network = q_network\n",
    "        \n",
    "#         self.image_buff = np.zeros(shape=(N_ADV,)+RESOLUTION)\n",
    "        self.image_buff = []\n",
    "        self.memory = []\n",
    "        self.batch = {'s1':[], 'action':[], 's2':[] ,'reward':[], 'reward_adv':[], 'isdemo':[]}\n",
    "        self.R = 0\n",
    "        \n",
    "        self.s1_record = np.zeros((1,N_ADV,)+RESOLUTION)\n",
    "        self.loss_one_record = 0\n",
    "        self.loss_adv_record = 0\n",
    "        self.loss_class_record = 0\n",
    "        self.loss_l2_record = 0\n",
    "        \n",
    "    def calc_eps_step(self):\n",
    "            global frames\n",
    "\n",
    "            if frames<TOTAL_STEPS*LINEAR_EPS_START:\n",
    "                eps = EPS_START\n",
    "            elif frames>=TOTAL_STEPS*LINEAR_EPS_START and frames<TOTAL_STEPS*LINEAR_EPS_END:\n",
    "                eps = EPS_START + frames*(EPS_END-EPS_START)/(TOTAL_STEPS)\n",
    "            else:\n",
    "                eps = EPS_END\n",
    "            return eps\n",
    "        \n",
    "    def calc_eps_time(self):\n",
    "        \n",
    "        current_time_async = current_time\n",
    "#         print(\"%f, %f, %f\"%(current_time_async, current_time, TOTAL_TIME_PRE))\n",
    "        if current_time_async < TOTAL_TIME * LINEAR_EPS_START:\n",
    "            eps = EPS_START\n",
    "        elif current_time_async >= TOTAL_TIME * LINEAR_EPS_START and current_time_async < TOTAL_TIME*LINEAR_EPS_END:\n",
    "            eps = EPS_START + current_time_async*(EPS_END-EPS_START)/(TOTAL_TIME)\n",
    "        else:\n",
    "            eps = EPS_END\n",
    "            \n",
    "        return eps\n",
    "\n",
    "    def act_eps_greedy(self,s1):\n",
    "        \n",
    "        assert np.ndim(s1) == 3, print(\"np.ndim(s1)=\",np.ndim(s1))\n",
    "\n",
    "        self.image_buff.append(s1)\n",
    "        ret_action = np.zeros((N_AGENT_ACTION,))\n",
    "        \n",
    "        if not len(self.image_buff) == N_ADV + 1:\n",
    "            buff = self.image_buff + [np.zeros_like(s1) for _ in range(N_ADV - len(self.image_buff))]\n",
    "        else:\n",
    "            self.image_buff.pop(0)\n",
    "            buff = self.image_buff\n",
    "\n",
    "        eps = self.calc_eps_time()\n",
    "\n",
    "#         print(\"np.shape(buff)\",np.shape(buff))\n",
    "        if random.random() > eps:\n",
    "            a_idx = self.q_network.predict_best_action(buff)[0]\n",
    "#             print(a_idx.shape)\n",
    "        else:\n",
    "            a_idx = random.randint(0,N_AGENT_ACTION-1)\n",
    "\n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "    \n",
    "    def act_greedy(self,s1):\n",
    "\n",
    "        self.image_buff.append(s1)\n",
    "        ret_action = np.zeros((N_AGENT_ACTION,))\n",
    "        if len(self.image_buff) == N_ADV + 1:\n",
    "            self.image_buff.pop(0)\n",
    "            q = self.q_network.get_q_value(self.image_buff)[0]\n",
    "            a_idx = self.q_network.predict_best_action(self.image_buff)[0]\n",
    "        else:\n",
    "            a_idx = self.q_network.predict_best_action(self.image_buff + [np.zeros(RESOLUTION) for _ in range(N_ADV - len(self.image_buff))])[0]\n",
    "#             a_idx = randint(0,N_AGENT_ACTION-1)\n",
    "\n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "    \n",
    "    def push_advantage(self,s1_,a_,r_,s2_,isterminal,isdemo):\n",
    "        self.memory.append((s1_,a_,r_,s2_,isdemo))\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.memory = []\n",
    "    \n",
    "    def push_to_batch(self, s1, action, s2, reward, reward_adv, isdemo):\n",
    "        self.batch['s1'].append(s1)\n",
    "        self.batch['action'].append(action)\n",
    "        self.batch['s2'].append(s2)\n",
    "        self.batch['reward'].append(reward)\n",
    "        self.batch['reward_adv'].append(reward_adv)\n",
    "        self.batch['isdemo'].append(isdemo)\n",
    "        return 0\n",
    "    \n",
    "    def clear_batch(self):\n",
    "        self.batch = {'s1':[], 'action':[], 's2':[] ,'reward':[], 'reward_adv':[], 'isdemo':[]}\n",
    "        return 0\n",
    "    \n",
    "    def make_batch_learn(self):\n",
    "        n = len(self.batch['action'])\n",
    "        s1 = np.zeros((n, N_ADV,)+RESOLUTION)\n",
    "        s2 = np.zeros((n, N_ADV,)+RESOLUTION)\n",
    "        for i in range(n):\n",
    "            s1[i, :n - i] = self.batch['s1'][:n - i]\n",
    "            s2[i, :n - i] = self.batch['s2'][:n - i]\n",
    "        \n",
    "        self.s1_record = s1[0:1]\n",
    "        \n",
    "        self.loss_one_record, self.loss_adv_record, self.loss_class_record = \\\n",
    "        self.q_network.update_parameter_server_batch(s1, self.batch['action'], self.batch['reward'], \\\n",
    "                                                         self.batch['reward_adv'], s2, self.batch['isdemo'])\n",
    "        return 0\n",
    "    \n",
    "    def get_q_value(self):\n",
    "        \n",
    "        if len(self.image_buff) == N_ADV:\n",
    "            q = self.q_network.get_q_value(self.image_buff)[0]\n",
    "        else:\n",
    "            q = self.q_network.get_q_value(self.image_buff + [np.zeros(RESOLUTION) for _ in range(N_ADV - len(self.image_buff))])[0]\n",
    "        \n",
    "        return q\n",
    "        \n",
    "    \n",
    "    def learn_advantage(self, isterminal):\n",
    "        \n",
    "        if len(self.memory)==N_ADV or isterminal:\n",
    "            tail_idx = len(self.memory)-1\n",
    "            \n",
    "            s1_buff = np.zeros((N_ADV, )+RESOLUTION)\n",
    "            for i in range(tail_idx+1):\n",
    "                s1_buff[i] = self.memory[i][0]\n",
    "            \n",
    "            for i in range(tail_idx,-1,-1):\n",
    "                s1,a,r,s2,d = self.memory[i]\n",
    "                if i==tail_idx:\n",
    "                    if not isterminal:\n",
    "#                         print(np.max(self.q_network.get_q_value(s1)[0]))\n",
    "                        self.R = np.max(self.q_network.get_q_value(s1_buff)[0])\n",
    "                        \n",
    "                    else:\n",
    "                        self.R = 0\n",
    "                else:\n",
    "                    self.R =  r + GAMMA*self.R\n",
    "            \n",
    "#                 self.q_network.train_push(s1,a,r,self.R,s2,d)\n",
    "                self.push_to_batch(s1,a,s2,r,self.R,d)\n",
    "            \n",
    "#             self.q_network.update_parameter_server()\n",
    "#             self.q_network.update_parameter_server_batch(self.batch['s1'], self.batch['action'], self.batch['reward'], \\\n",
    "#                                                          self.batch['reward_adv'], self.batch['s2'], self.batch['isdemo'])\n",
    "\n",
    "#             print(np.shape(self.batch['s1']))\n",
    "#             print(np.shape(self.batch['s2']))\n",
    "#             print(np.shape(self.batch['s2']))\n",
    "            self.make_batch_learn()\n",
    "            self.q_network.copy_learn2target()\n",
    "            self.R = 0\n",
    "            self.clear_memory()\n",
    "            self.clear_batch()\n",
    "            \n",
    "#             return self.q_network.calc_loss([s1],[a],[r],[self.R],[s2],[d])\n",
    "#         return 0.0,0.0,0.0\n",
    "    \n",
    "    def calc_loss(self):\n",
    "        \n",
    "        if len(self.memory) == N_ADV :\n",
    "            tail_idx = len(self.memory) - 1\n",
    "            s1_buff = np.ones((1, tail_idx+1, )+RESOLUTION) * np.nan\n",
    "            s2_buff = np.ones((1, tail_idx+1, )+RESOLUTION) * np.nan\n",
    "            for i in range(tail_idx+1):\n",
    "                s1_buff[0, i] = self.memory[i][0]\n",
    "                s2_buff[0, i] = self.memory[i][3]\n",
    "            \n",
    "            for i in range(tail_idx, -1, -1):\n",
    "                s1 , a, r, s2, d = self.memory[i]\n",
    "                if i == tail_idx :\n",
    "                    R = np.max(self.q_network.get_q_value(s1_buff)[0])\n",
    "                else:\n",
    "                    R = r * GAMMA * R\n",
    "                \n",
    "                _, last_action, last_r, _, last_d = self.memory[tail_idx]\n",
    "                \n",
    "                return [s1_buff] + self.q_network.calc_loss(s1_buff, [last_action], [last_r], [R] ,s2_buff ,[last_d])\n",
    "        \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [1,6,6]\n",
    "        stride = [1,3,3]\n",
    "#         kernel_size = [6,6]\n",
    "#         stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "#         weights_init = tf.constant_initializer(2.0)\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "#         print(pre_layer.get_shape())\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,\\\n",
    "                                        biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "#         return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [1,3,3]\n",
    "        stride = [1,2,2]\n",
    "#         kernel_size = [3,3]\n",
    "#         stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "#         return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        return tf.nn.max_pool3d(pre_layer,[1,1,3,3,1],[1,1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        shape = pre_layer.get_shape()\n",
    "        return tf.reshape(pre_layer, shape=(-1, shape[1],shape[2]*shape[3]*shape[4]))\n",
    "#         return tf.reshape(pre_layer, shape=(-1, N_ADV * 2240))\n",
    "    \n",
    "    def lstm(pre_layer, state):\n",
    "        batch_size = tf.shape(pre_layer)[0]\n",
    "        temp = tf.reduce_max(state, axis=4)\n",
    "        temp = tf.reduce_max(temp, axis=3)\n",
    "        temp = tf.reduce_max(temp, axis=2)\n",
    "        lengh = tf.cast(tf.reduce_sum(tf.sign(temp) , axis=1),dtype=tf.int32)\n",
    "        lengh = tf.where(tf.equal(lengh, tf.zeros_like(lengh)), tf.ones_like(lengh), lengh)\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(LSTM_SIZE)\n",
    "        rnn_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        rnn_out, state_out = tf.nn.dynamic_rnn(cell, pre_layer, initial_state=rnn_state, sequence_length=lengh,dtype=tf.float32)\n",
    "        out_idx = tf.range(0, batch_size) * N_ADV + (lengh  -1)\n",
    "        output = tf.gather(tf.reshape(rnn_out, [-1, LSTM_SIZE]), out_idx)\n",
    "        return output, lengh, rnn_out\n",
    "    \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs = 512\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def q_value(pre_layer):\n",
    "        num_outputs = N_AGENT_ACTION\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network which be shared in global\n",
    "class ParameterServer:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,N_ADV)+RESOLUTION, name=\"state1\")\n",
    "        self.a_ = tf.placeholder(tf.int32, shape=(None,), name=\"action\")\n",
    "        self.r_ = tf.placeholder(tf.float32, shape=(None,), name=\"reward\")\n",
    "        self.r_adv = tf.placeholder(tf.float32, shape=(None,), name=\"reward_adv\")\n",
    "        self.mergin_value = tf.placeholder(tf.float32,shape=(None,N_AGENT_ACTION), name=\"mergin_value\")\n",
    "#         self.s1idx_ = tf.placeholder(tf.int32, shape=(None,), name=\"lengh_of_state\")\n",
    "        \n",
    "        with tf.variable_scope(\"parameter_server\",reuse=tf.AUTO_REUSE):      # スレッド名で重み変数に名前を与え、識別します（Name Space）\n",
    "            self.model = self._build_model()            # ニューラルネットワークの形を決定\n",
    "            \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"parameter_server\")\n",
    "#         self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, RMSProbDecaly)    # loss関数を最小化していくoptimizerの定義です\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            with tf.variable_scope(\"summary\"):\n",
    "                self._build_summary()\n",
    "\n",
    "            self.saver = tf.train.Saver()\n",
    "        \n",
    "        print(\"-------GLOBAL-------\")\n",
    "        for w in self.weights_params:\n",
    "            print(w)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.conv1 = NetworkSetting.conv1(self.state1_)\n",
    "        self.maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        self.conv2 = NetworkSetting.conv2(self.maxpool1)\n",
    "        self.maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        self.reshape = NetworkSetting.reshape(self.maxpool2)\n",
    "        self.rnn, self.length, self.rnn_raw = NetworkSetting.lstm(self.reshape, self.state1_)\n",
    "        self.fc1 = NetworkSetting.fc1(self.rnn)\n",
    "\n",
    "        q_value = NetworkSetting.q_value(self.fc1)\n",
    "\n",
    "        print(\"---------MODEL SHAPE-------------\")\n",
    "        print(self.state1_.get_shape())\n",
    "        print(self.conv1.get_shape())\n",
    "        print(self.conv2.get_shape())\n",
    "        print(self.reshape.get_shape())\n",
    "        print(self.fc1.get_shape())\n",
    "        print(q_value.get_shape())\n",
    "\n",
    "        return q_value\n",
    "\n",
    "    def _build_summary(self):\n",
    "        \n",
    "        self.loss_one = tf.placeholder(tf.float32,shape=())\n",
    "        self.loss_n = tf.placeholder(tf.float32,shape=())\n",
    "        self.loss_c = tf.placeholder(tf.float32,shape=())\n",
    "        self.loss_l = tf.placeholder(tf.float32,shape=())\n",
    "        \n",
    "        self.reward = tf.placeholder(tf.float32,shape=())\n",
    "        self.frag = tf.placeholder(tf.int64,shape=())\n",
    "        self.death = tf.placeholder(tf.int64,shape=())\n",
    "        \n",
    "        summary_lo = tf.summary.scalar('loss_one',self.loss_one, family='loss')\n",
    "        summary_ln = tf.summary.scalar('loss_nstep', self.loss_n, family='loss')\n",
    "        summary_lc = tf.summary.scalar('loss_class', self.loss_c, family='loss')\n",
    "        summary_ll = tf.summary.scalar('loss_l2',self.loss_l, family='loss')\n",
    "\n",
    "        self.merged_loss = tf.summary.merge([summary_lo,summary_ln,summary_lc,summary_ll])\n",
    "        \n",
    "        conv1_display = tf.expand_dims(tf.transpose(self.conv1, perm=[0,1,4,2,3]), axis=5)\n",
    "        conv2_display = tf.expand_dims(tf.transpose(self.conv2, perm=[0,1,4,2,3]), axis=5)\n",
    "\n",
    "        state_shape = self.state1_.get_shape()\n",
    "        conv1_shape = conv1_display.get_shape()\n",
    "        conv2_shape = conv2_display.get_shape()\n",
    "        print(\"state1_shape\",state_shape)\n",
    "        print(\"conv1_shape:\", conv1_shape)\n",
    "        print(\"conv2_shape:\",conv2_shape)\n",
    "        summary_state  = tf.summary.image('state',tf.reshape(self.state1_,[-1,state_shape[2], state_shape[3], state_shape[4]]),max_outputs = 1)\n",
    "        summary_conv1 = tf.summary.image('conv1',tf.reshape(conv1_display,[-1, conv1_shape[3], conv1_shape[4], conv1_shape[5]]),max_outputs = 1)\n",
    "        summary_conv2 = tf.summary.image('conv2',tf.reshape(conv2_display,[-1, conv2_shape[3], conv2_shape[4], conv2_shape[5]]),max_outputs = 1)\n",
    "\n",
    "        self.merged_image = tf.summary.merge([summary_state,summary_conv1,summary_conv2])\n",
    "        \n",
    "        summary_reward = tf.summary.scalar('reward',self.reward)\n",
    "        summary_frag = tf.summary.scalar('frag',self.frag)\n",
    "        summary_death = tf.summary.scalar('death',self.death)\n",
    "        \n",
    "        self.merged_testscore = tf.summary.merge([summary_reward,summary_frag,summary_death])\n",
    "        \n",
    "        self.merged_weights = tf.summary.merge([tf.summary.scalar(self.weights_params[i].name,tf.reduce_mean(self.weights_params[i]), family='weights') for i in range(len(self.weights_params))])\n",
    "        \n",
    "        self.eps_ = tf.placeholder(tf.float32, shape=())\n",
    "        summary_eps = tf.summary.scalar('epsilon', self.eps_, family='epsilon')\n",
    "        self.merged_eps = tf.summary.merge([summary_eps])\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(LOG_DIR,SESS.graph)\n",
    "\n",
    "    # write summary about LOSS and IMAGE\n",
    "    def write_loss(self,step,loss_one,loss_n,loss_class,loss_l2):\n",
    "            m = SESS.run(self.merged_loss,feed_dict= \\\n",
    "                               {self.loss_one:loss_one,self.loss_n:loss_n,self.loss_c:loss_class,self.loss_l:loss_l2})\n",
    "            self.writer.add_summary(m, step)\n",
    "            return 0\n",
    "                \n",
    "    def write_images(self, step, s1):\n",
    "        m = SESS.run(self.merged_image, {self.state1_: s1})\n",
    "        self.writer.add_summary(m, step)\n",
    "        return 0\n",
    "    \n",
    "    def write_records(self,step,r,f,d):\n",
    "        m = SESS.run(self.merged_testscore,feed_dict={self.reward:r,self.frag:f,self.death:d})\n",
    "        self.writer.add_summary(m,step)\n",
    "        \n",
    "    def write_weights(self, step):\n",
    "        m = SESS.run(self.merged_weights)\n",
    "        self.writer.add_summary(m, step)\n",
    "        return 0\n",
    "    \n",
    "    def write_eps(self, step, eps):\n",
    "        m = SESS.run(self.merged_eps, feed_dict={self.eps_:eps})\n",
    "        self.writer.add_summary(m, step)\n",
    "        return 0\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        self.saver.save(SESS, model_path+\"/model.ckpt\")\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.saver.restore(SESS, model_path+\"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name,parameter_server):\n",
    "        self.name = name\n",
    "        \n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,N_ADV,)+RESOLUTION, name=\"state_1\")\n",
    "        self.state2_ = tf.placeholder(tf.float32,shape=(None,N_ADV,)+RESOLUTION, name=\"state_2\")\n",
    "        self.a_ = tf.placeholder(tf.int32, shape=(None,), name=\"action\")\n",
    "        self.a_onehot_ = tf.placeholder(tf.int32, shape=(None, N_AGENT_ACTION,), name=\"action_onehot\")\n",
    "        self.r_ = tf.placeholder(tf.float32, shape=(None,), name=\"rewrad\")\n",
    "        self.r_adv = tf.placeholder(tf.float32, shape=(None,), name=\"reward_advantage\")\n",
    "        self.isdemo_ = tf.placeholder(tf.float32,shape=(None,), name=\"isdemo\")\n",
    "        self.isterminal_ = tf.placeholder(tf.float32, shape=(None,), name=\"isterminal\")\n",
    "        self.mergin_value = tf.placeholder(tf.float32,shape=(None,N_AGENT_ACTION), name=\"mergin\")\n",
    "        self.is_weight_ = tf.placeholder(tf.float32, shape=(None,), name=\"is_weight\")\n",
    "        self.loss_weights_ = tf.placeholder(tf.float32, shape =(4,), name=\"loss_weights\")\n",
    "        \n",
    "        with tf.variable_scope(self.name+\"_train\", reuse=tf.AUTO_REUSE):\n",
    "            self.model_l, self.len_s1 = self._model(self.state1_)\n",
    "        with tf.variable_scope(self.name+\"_target\", reuse=tf.AUTO_REUSE):\n",
    "            self.model_t, self.len_s2 = self._model(self.state2_)\n",
    "\n",
    "        self._build_graph(parameter_server)\n",
    "\n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self,state):\n",
    "\n",
    "        conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        rnn ,l ,_ = NetworkSetting.lstm(reshape, state)\n",
    "        fc1 = NetworkSetting.fc1(rnn)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "        \n",
    "        return q_value, 0\n",
    "\n",
    "    def _build_graph(self,parameter_server):\n",
    "        \n",
    "#         self.best_action = tf.argmax(self.model_l, axis=1)\n",
    "        self.prob_action = tf.nn.softmax(self.model_l, axis=1)\n",
    "        \n",
    "#         q_model_t = tf.where(tf.equal(self.len_s2, self.len_s1) , self.model_t,tf.zeros_like(self.model_t))\n",
    "        q_model_t = tf.where(tf.equal(self.isterminal_, tf.zeros_like(self.isterminal_)) ,self.model_t,tf.zeros_like(self.model_t))\n",
    "        self.q_model_t = q_model_t\n",
    "        \n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"_train\")\n",
    "        self.weights_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+\"_target\")\n",
    "        self.copy_params = [t.assign(l) for l,t in zip(self.weights_params, self.weights_params_target)]\n",
    "        \n",
    "#         self.loss_one = tf.square(tf.stop_gradient(self.r_ + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "#         self.loss_adv = tf.square(tf.stop_gradient(self.r_adv + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1))\n",
    "        self.loss_one = (tf.square(tf.stop_gradient(self.r_ + tf.reduce_max(q_model_t,axis=1)) - tf.reduce_max(self.model_l,axis=1)))\n",
    "        self.loss_adv = self.loss_weights_[1] * (tf.square(tf.stop_gradient(self.r_adv + np.power(GAMMA, N_ADV) * tf.reduce_max(q_model_t,axis=1)) - \\\n",
    "                                             tf.reduce_max(self.model_l,axis=1)))\n",
    "        target = tf.stop_gradient(tf.reduce_max(self.model_l + self.mergin_value, axis=1))\n",
    "        idx = tf.transpose([tf.range(tf.shape(self.model_l)[0]), self.a_])\n",
    "        self.loss_class =  self.loss_weights_[2] * ((target- tf.gather_nd(self.model_l,indices=idx)) * self.isdemo_)\n",
    "        self.loss_l2 = self.loss_weights_[3] * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params])\n",
    "        \n",
    "        self.target_test = target\n",
    "        self.idx_test = idx\n",
    "        self.gather_test = tf.gather_nd(self.model_l,indices=idx)\n",
    "        \n",
    "        self.loss_total = (tf.reduce_mean(self.loss_adv) +  tf.reduce_mean(self.loss_class) + self.loss_l2) * self.is_weight_\n",
    "#         self.loss_total = tf.reduce_mean(self.loss_class) + self.loss_l2\n",
    "        \n",
    "        self.grads = tf.gradients(self.loss_total ,self.weights_params)\n",
    "        \n",
    "        self.update_global_weight_params = \\\n",
    "            parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))\n",
    "\n",
    "        self.pull_global_weight_params = [l_p.assign(g_p) for l_p,g_p in zip(self.weights_params,parameter_server.weights_params)]\n",
    "\n",
    "        self.push_local_weight_params = [g_p.assign(l_p) for g_p,l_p in zip(parameter_server.weights_params,self.weights_params)]\n",
    "        \n",
    "    def test(self, s1, a, r, r_adv, s2, isdemo,isterminal):\n",
    "        mergin = [[MERGIN_BASE*(not(a[j]==i)) for i in range(N_AGENT_ACTION)] for j in range(len(a))]\n",
    "        \n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_:r,self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin,self.isdemo_:isdemo, self.isterminal_:isterminal}\n",
    "        print(SESS.run([self.q_model_t, self.target_test, self.idx_test, self.gather_test], feed_dict))\n",
    "        return 0\n",
    "    \n",
    "    def pull_parameter_server(self):\n",
    "        SESS.run(self.pull_global_weight_params)\n",
    "    \n",
    "    def push_parameter_server(self):\n",
    "        SESS.run(self.push_local_weight_params)\n",
    "        \n",
    "    def show_weights(self):\n",
    "        hoge = SESS.run(self.weights_params)\n",
    "        for i in range(len(hoge)):\n",
    "            print(hoge[i])\n",
    "    \n",
    "    def update_parameter_server_batch(self, s1, a, r, r_adv, s2, isdemo, is_weight, a_onehot, isterminal):\n",
    "        if np.ndim(s1) == 4:\n",
    "            s1 = np.array([s1])            \n",
    "            \n",
    "        if np.ndim(s2) == 4:\n",
    "            s2 = np.array([s2])\n",
    "        mergin = [[MERGIN_BASE*(not(a[j]==i)) for i in range(N_AGENT_ACTION)] for j in range(np.shape(a)[0])]\n",
    "        \n",
    "        if np.shape(s1) != (BATCH_SIZE, N_ADV)+RESOLUTION:\n",
    "            print(np.shape(s1))\n",
    "            return 0, 0, 0, 0\n",
    "        weights = SESS.run(self.weights_params)\n",
    "        assert np.isnan([np.mean(w) for w in weights]).any()==False , print(weights)\n",
    "#         print(\"s1.shape:\",s1.shape, \"np.mean(s1)\", np.mean(s1))\n",
    "#         print(\"s2.shape:\",s2.shape, \"np.mean(s2)\", np.mean(s2))\n",
    "#         print(\"action:\", a)\n",
    "#         print(\"reward:\", r)\n",
    "#         print(\"reward_adv\", r_adv)\n",
    "\n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_:r,self.r_adv:r_adv, self.state2_:s2, self.mergin_value:mergin,self.isdemo_:isdemo, \\\n",
    "                     self.is_weight_: is_weight, self.a_onehot_:a_onehot, self.isterminal_:isterminal,self.loss_weights_:[1.0, LAMBDA1, LAMBDA2, LAMBDA3]}\n",
    "        val = SESS.run([self.loss_one, self.loss_adv, self.loss_class, self.loss_l2, self.model_l,self.q_model_t,  self.grads, self.model_l],feed_dict)\n",
    "#         val = SESS.run([self.update_global_weight_params,self.loss_one, self.loss_adv, self.loss_class, self.loss_l2],feed_dict)\n",
    "        SESS.run([self.update_global_weight_params],feed_dict)\n",
    "    \n",
    "#         TEST_VALUES.append(val)\n",
    "        \n",
    "#         print(\"{}+{}={}\".format(val[2],val[3],val[2] + val[3]))\n",
    "#         print(\"model_l\",val[4])\n",
    "#         print(\"model_t\", val[5])\n",
    "#         print(\"max(Q(s_t) + l(a, a_E)) = \", val[6])\n",
    "#         print(\"a_E = \", val[7])\n",
    "#         print(\"Q(s_t)[a_E] = \", val[8])\n",
    "#         print(\"l_one:{} l_adv:{}, l_cls:{}, l_l2:{}\".format(val[0], val[1], val[2], val[3]))\n",
    "#         print(\"np.mean(grads)=\", [np.mean(g) for g in val[9]])\n",
    "        \n",
    "        return val[0], val[1], val[2], val[3]\n",
    "\n",
    "    def predict_best_action(self, s1):\n",
    "        if np.ndim(s1)==4:\n",
    "            s1 = np.array([s1])\n",
    "        \n",
    "#         print(np.shape(s1))\n",
    "#         print(SESS.run(self.model_l, {self.state1_ : s1}))\n",
    "#         return SESS.run(self.best_action,{self.state1_ : s1})\n",
    "\n",
    "        probs = SESS.run(self.prob_action, {self.state1_:s1})\n",
    "        return [np.random.choice(N_AGENT_ACTION, p=p) for p in probs]\n",
    "\n",
    "    def get_q_value(self,s1):\n",
    "        if np.ndim(s1)==4:\n",
    "            s1 = np.array([s1])\n",
    "            \n",
    "        return SESS.run(self.model_l,{self.state1_:s1})\n",
    "    \n",
    "    def calc_loss(self, s1, a, r, r_adv, s2, isdemo,isterminal):\n",
    "        mergin = [[MERGIN_BASE*(not(a[j]==i)) for i in range(N_AGENT_ACTION)] for j in range(len(a))]\n",
    "        \n",
    "        feed_dict = {self.state1_: s1,self.a_:a, self.r_:r,self.r_adv:r_adv, self.state2_:s2, \\\n",
    "                     self.mergin_value:mergin,self.isdemo_:isdemo, self.isterminal_:isterminal, self.loss_weights_:[1.0, LAMBDA1, LAMBDA2, LAMBDA3]}\n",
    "        return SESS.run([self.loss_one, self.loss_adv, self.loss_class, self.loss_l2],feed_dict)\n",
    "    \n",
    "    def copy_learn2target(self):\n",
    "        SESS.run(self.copy_params)\n",
    "\n",
    "    def train_push(self,s1,a,r,r_adv,s2,isdemo):\n",
    "        # Push obs to make batch\n",
    "        self.s1[self.queue_pointer] = s1\n",
    "        self.s2[self.queue_pointer] = s2\n",
    "        self.action[self.queue_pointer] = a\n",
    "        self.reward[self.queue_pointer] = r\n",
    "        self.reward_adv[self.queue_pointer] = r_adv\n",
    "        self.isdemo[self.queue_pointer] = isdemo\n",
    "        self.queue_pointer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TensorRecorder(object):\n",
    "#     def __init__(self):\n",
    "#         self.build_qvalue()\n",
    "        \n",
    "#     def build_qvalue(self):\n",
    "#         self.q_value = tf.placeholder(tf.float32,shape=(N_AGENT_ACTION,), name=\"q_value\")\n",
    "#         summary_q = tf.summary.histogram('q_value',self.q_value, family='q_value')\n",
    "#         self.merge_q = tf.summary.merge([summary_q])\n",
    "        \n",
    "#     def write_qvalue(self,step, q_value, writer):\n",
    "#         m = SESS.run(self.merge_q, {self.q_value: q_value})\n",
    "#         writer.add_summary(m, step)\n",
    "#         return 0\n",
    "    \n",
    "#     def build_score(self):\n",
    "#         self.reward = tf.placeholder(tf.float32,shape=(), name=\"reward\")\n",
    "#         summary_reward = tf.summary.scalar('reward',self.reward)\n",
    "#         self.merged_testscore = tf.summary.merge([summary_reward])\n",
    "    \n",
    "#     def write_score(self,step,reward, writer):\n",
    "#         m = SESS.run(self.merged_testscore,{self.reward:reward})\n",
    "#         writer.add_summary(m,step)\n",
    "#         return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"learning\":\n",
    "#     recorder = TensorRecorder()\n",
    "    frames = 0\n",
    "    runout = False\n",
    "    current_time = 0\n",
    "    start_time_async = 0\n",
    "    \n",
    "    GIF_BUFF = []\n",
    "    N_EPISODES = 0\n",
    "\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "    \n",
    "    replay_memory = ReplayMemory(20000,permanent_data=0)\n",
    "\n",
    "    threads = []\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        parameter_server = ParameterServer()\n",
    "\n",
    "        for i in range(N_WORKERS):\n",
    "            threads.append(WorkerThread(\"learning_\"+str(i),parameter_server, replay_memory))\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        pre_env = Environment(\"pre_env\",parameter_server, replay_memory)\n",
    "        test_env = Environment(\"test_env\", parameter_server,replay_memory)\n",
    "\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "    threads[0].environment.summary=True\n",
    "\n",
    "    print(\"---LOADING DEMO---\")\n",
    "    pre_env.load_demonstration()\n",
    "    print(\"---PRE LEARNING---\")\n",
    "    start_time_pre = datetime.datetime.now()\n",
    "    pre_env.run_pre_learning()\n",
    "    if SAVE_FILE == True:\n",
    "        print(\"---SAVING_MODEL---\")\n",
    "        parameter_server.save_model(PREMODEL_PATH)\n",
    "\n",
    "    print(\"---ASYNC_LEARNING---\")\n",
    "    start_time_async = datetime.datetime.now()\n",
    "    current_time = 0\n",
    "    for worker in threads:\n",
    "        job = lambda: worker.run()\n",
    "        t = threading.Thread(target=job)\n",
    "        t.start()\n",
    "\n",
    "    test_frame = 0\n",
    "    while True:\n",
    "        if frames >= test_frame and frames<test_frame+TEST_INTERVAL:\n",
    "            parameter_server.write_weights(frames)\n",
    "            r,frag,death = test_env.run_test(frames)\n",
    "            if SAVE_FILE == True:\n",
    "                parameter_server.write_records(frames,r,frag,death)\n",
    "            test_frame += TEST_INTERVAL\n",
    "        elif frames >= test_frame+TEST_INTERVAL:\n",
    "            print(\"TEST at %d~%d step cant be finished\"%(test_frame, test_frame+TEST_INTERVAL-1))\n",
    "            test_frame += TEST_INTERVAL\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if datetime.datetime.now() > TIME_LEARN + start_time_async:\n",
    "            runout = True\n",
    "            break\n",
    "    \n",
    "    if (SAVE_FILE==True):\n",
    "        test_env.run_test(global_step = frames, gif_buff=GIF_BUFF)\n",
    "        GIF_BUFF[0].save(GIF_PATH,save_all=True,append_images=GIF_BUFF[1:])\n",
    "    \n",
    "        parameter_server.save_model(MODEL_PATH) \n",
    "        \n",
    "    print(N_EPISODES, \"episodes is passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='test':\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=USED_GPU))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "    \n",
    "    replay_memory = ReplayMemory(CAPACITY,permanent_data=0)\n",
    "    \n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        parameter_server = ParameterServer()\n",
    "        pre_env = Environment(\"pre_env\",parameter_server, replay_memory)\n",
    "        \n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "    parameter_server.load_model(PREMODEL_PATH)\n",
    "    pre_env.load_demonstration()\n",
    "    pre_env.network.pull_parameter_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"test\":\n",
    "    def show_command(action):\n",
    "        buff = \"\"\n",
    "        action_names = [\"TURN_LEFT\",\"TURN_RIGHT\",\"MOVE_RIGHT\",\"MOVE_FORWARD\",\"MOVE_LEFT\",\"ATTACK\"]\n",
    "        if (type(action) == type(list())):\n",
    "            for i,a in enumerate(action):\n",
    "                if a==1:\n",
    "                    buff += action_names[i] + \",\"\n",
    "        else:\n",
    "            for i in range(6):\n",
    "                if action %2 == 1:\n",
    "                    buff +=action_names[i] + \",\"\n",
    "                action = int(action/2)\n",
    "        \n",
    "        return buff + \"\\n\"\n",
    "    \n",
    "    def get_q_values(data, env):\n",
    "        n_data = data.shape[0]\n",
    "        ans = []\n",
    "        for i in range(n_data):\n",
    "            d = data[i]\n",
    "            q_s =  env.network.get_q_value(d)[0]\n",
    "            ans.append(q_s)\n",
    "\n",
    "        return np.array(ans)\n",
    "\n",
    "    def predict_action(data, env):\n",
    "        n_data = data.shape[0]\n",
    "        ans = []\n",
    "        for i in range(n_data):\n",
    "            d = data[i]\n",
    "            q_s =  env.network.get_q_value(d)[0]\n",
    "            ans.append(np.argmax(q_s))\n",
    "\n",
    "        return np.array(ans)\n",
    "    \n",
    "    def softmax(a):\n",
    "        c = np.max(a)\n",
    "\n",
    "        exp_a = np.exp(a - c)\n",
    "        sum_exp_a = np.sum(exp_a)\n",
    "\n",
    "        y = exp_a / sum_exp_a\n",
    "\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"test\":\n",
    "    pre_env.network.copy_learn2target()\n",
    "    lengh = len(replay_memory)\n",
    "    data = [replay_memory.tree.data[i][0] for i in range(lengh)]\n",
    "    target_action = np.array([replay_memory.tree.data[i][1][-1] for i in range(lengh)])\n",
    "\n",
    "    q_values = get_q_values(np.array(data)[:200], pre_env)\n",
    "    predicted_action = predict_action(np.array(data)[:200], pre_env)\n",
    "    print(target_action[:200] == predicted_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
