{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from random import sample, randint, random\n",
    "import time,random,threading,datetime\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import sys, os, glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from game_instance import GameInstance\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import  RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__ = \"learning\"\n",
    "\n",
    "DEMO_PATH = [\"./demonstration/predict_enemyposition/demodata%02d.hdf5\"%(i) for i in range(1,3)]\n",
    "MODEL_DIR = \"./models/predict_enemyposition/model_test/\"\n",
    "LOG_DIR = \"./logs/predict_enemyposition/log_test\"\n",
    "\n",
    "LABELS = os.path.join(os.getcwd(), \"label_256.tsv\")\n",
    "SPRITES = os.path.join(os.getcwd(), \"sprite_256.png\")\n",
    "SPRITES_DATA = os.path.join(os.getcwd(), 'sprite_img_256.npy')\n",
    "\n",
    "RESOLUTION = (120, 120, 3)\n",
    "\n",
    "THRESHOLD = 800\n",
    "KEEP_PROB = 0.7\n",
    "N_AREA = 6\n",
    "LEN_AREA = 680/N_AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for f in os.listdir(LOG_DIR):\n",
    "        print(\"removed  \", f)\n",
    "        os.remove(os.path.join(LOG_DIR, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    batch_img = []\n",
    "    batch_label = []\n",
    "    for d in DEMO_PATH:\n",
    "        print(\"loading \"+d)\n",
    "        file = h5py.File(d, \"r\")\n",
    "        episode_list = list(file.keys())[1:]\n",
    "\n",
    "        for e in episode_list[:]:\n",
    "            n_steps = file[e+\"/states\"].shape[0]\n",
    "            states = file[e+\"/states\"][:]\n",
    "            enemy_x = file[e+\"/enemy_x\"][:]\n",
    "            enemy_w = file[e+\"/enemy_w\"][:]\n",
    "            enemy_h = file[e+\"/enemy_h\"][:]\n",
    "\n",
    "            for img in states:\n",
    "                batch_img.append(img)\n",
    "\n",
    "            buff = []\n",
    "            for e_x, e_w,e_h in zip(enemy_x, enemy_w, enemy_h):\n",
    "                if e_w*e_h > THRESHOLD :\n",
    "                    center = e_x + e_w/2\n",
    "                    batch_label.append(int(center / LEN_AREA)+1)\n",
    "                else:\n",
    "                    batch_label.append(0)\n",
    "\n",
    "        file.close()\n",
    "    return np.array(batch_img), np.array(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            self.state1_ = tf.placeholder(tf.float32,shape=(None, )+RESOLUTION, name=\"state_1\")\n",
    "            self.target_ = tf.placeholder(tf.int32, shape=(None,), name=\"area\")\n",
    "            self.keep_prob_ = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "            self.q_model = self._model(self.state1_, self.keep_prob_)\n",
    "            self._build_graph()\n",
    "            self.saver = tf.train.Saver(self.weights_params)\n",
    "\n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self,state, keep_prob):\n",
    "\n",
    "        self.conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        self.conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "#         rnn ,l ,_ = NetworkSetting.lstm(reshape, state)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "        drop = NetworkSetting.dropout(fc1, keep_prob)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(drop)\n",
    "        \n",
    "        return q_value\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "        \n",
    "        self.prob = tf.nn.softmax(self.q_model, axis=1)\n",
    "        \n",
    "        self.onehot = tf.one_hot(self.target_, depth=7)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.q_model, labels=self.onehot) + 1e-5 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params])\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        with tf.variable_scope(\"trainer\"):\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.update_step = optimizer.minimize(self.loss)\n",
    "        return 0\n",
    "    \n",
    "    def update_parameter_server_batch(self, s1, target):\n",
    "\n",
    "        weights = SESS.run(self.weights_params)\n",
    "        assert np.isnan([np.mean(w) for w in weights]).any()==False , print(weights)\n",
    "        feed_dict = {self.state1_: s1, self.target_:target, self.keep_prob_:KEEP_PROB}\n",
    "        l,_ = SESS.run([self.loss, self.update_step], feed_dict)\n",
    "        return l\n",
    "\n",
    "    def predict_enemyposition(self, s1):\n",
    "        \n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            probs = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return [np.random.choice(2, p=p) for p in probs][0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            probs = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return [np.random.choice(7, p=p) for p in probs]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_q_values(self, s1):\n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            q = SESS.run(self.q_model, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return q[0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            q = SESS.run(self.q_model, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return q\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_probability(self, s1):\n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            p = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return p[0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            p = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return p\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_loss(self, s1, target):\n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            q = SESS.run(self.loss, {self.state1_:s1,  self.target_:target, self.keep_prob_:1.0})\n",
    "            return q[0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            q = SESS.run(self.loss, {self.state1_:s1,  self.target_:target, self.keep_prob_:1.0})\n",
    "            return q\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_score(self, s1, target):\n",
    "        pred = self.predict_enemyposition(s1)\n",
    "        return sum(pred==target) / len(target)\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        return self.saver.save(SESS, model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        return self.saver.restore(SESS, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "#         kernel_size = [1,6,6]\n",
    "#         stride = [1,3,3]\n",
    "        kernel_size = [6,6]\n",
    "        stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,\\\n",
    "                                        biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 32\n",
    "#         kernel_size = [1,3,3]\n",
    "#         stride = [1,2,2]\n",
    "        kernel_size = [3,3]\n",
    "        stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        shape = pre_layer.get_shape()\n",
    "        return tf.reshape(pre_layer, shape=(-1, shape[1]*shape[2]*shape[3]))\n",
    "    \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs =1024\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def dropout(pre_layer, keep_prob):\n",
    "        return tf.nn.dropout(pre_layer, keep_prob)\n",
    "    \n",
    "    def q_value(pre_layer):\n",
    "        num_outputs =7\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRecorder(object):\n",
    "    def __init__(self,log_dir, sprites = None, labels=None):\n",
    "        \n",
    "        # Place holder\n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,)+RESOLUTION, name=\"state1\")\n",
    "        with tf.variable_scope(\"log_recorder\", reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope(\"model\"):\n",
    "                self.conv1, self.conv2,self.embedding_input, self.model = self._build_model(self.state1_)\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Images\"):\n",
    "                conv1_display = tf.reshape(tf.transpose(self.conv1, [0,3,1,2]), (-1, self.conv1.get_shape()[1],self.conv1.get_shape()[2]))\n",
    "                conv2_display = tf.reshape(tf.transpose(self.conv2, [0,3,1,2]), (-1, self.conv2.get_shape()[1],self.conv2.get_shape()[2]))\n",
    "                conv1_display = tf.expand_dims(conv1_display, -1)\n",
    "                conv2_display = tf.expand_dims(conv2_display, -1)\n",
    "                self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"log_recorder\")\n",
    "                \n",
    "\n",
    "            with tf.variable_scope(\"Summary_Loss\"):\n",
    "                # Summary for LOSS\n",
    "                self.loss_ = [tf.placeholder(tf.float32,shape=()), tf.placeholder(tf.float32, shape=())]\n",
    "                loss_name = [\"loss_test\",\"loss_train\" ]\n",
    "                self.merged_loss = self._build_scalar_summary(self.loss_, loss_name, \"loss\")\n",
    "\n",
    "            # Summary for  SCORE\n",
    "            with tf.variable_scope(\"Summary_Score\"):\n",
    "                self.accuracy_ = [tf.placeholder(tf.float32, shape=()), tf.placeholder(tf.float32, shape=())]\n",
    "                accuracy_name = [\"accuracy_test\", \"accuracy_train\"]\n",
    "                self.merged_accuracy = self._build_scalar_summary(self.accuracy_, accuracy_name, \"score\")\n",
    "\n",
    "            # Summary for SCREEN\n",
    "            with tf.variable_scope(\"Summary_Images\"):\n",
    "                image_name = [\"state1\", \"conv1\", \"conv2\"] \n",
    "                self.merged_images = self._build_image_summary([self.state1_, conv1_display,conv2_display],n_output=10, names=image_name, family=\"states\")\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Filter\"):\n",
    "                #Sumamry for FILTER\n",
    "                for w in self.weights_params:\n",
    "                    print(w.get_shape())\n",
    "                display_filter_conv1 = tf.reshape(tf.transpose(self.weights_params[0], [2,3,0,1]), (-1,self.weights_params[0].get_shape()[0], self.weights_params[0].get_shape()[1]))\n",
    "                display_filter_conv2 = tf.reshape(tf.transpose(self.weights_params[2], [2,3,0,1]), (-1,self.weights_params[2].get_shape()[0], self.weights_params[2].get_shape()[1]))\n",
    "                display_filter_conv1 = tf.expand_dims(display_filter_conv1, -1)\n",
    "                display_filter_conv2 = tf.expand_dims(display_filter_conv2, -1)\n",
    "                print(display_filter_conv1.get_shape())\n",
    "                print(display_filter_conv2.get_shape())\n",
    "                filter_shapes = [display_filter_conv1.get_shape(), display_filter_conv2.get_shape()]\n",
    "                filter_names = [\"conv1\", \"conv2\"]\n",
    "                self.merged_filters = self._build_image_summary([display_filter_conv1, display_filter_conv2], 10, filter_names, \"filters\")\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Histogram\"):\n",
    "                # Summary WEIGHT HISTOGRAM\n",
    "                self.merged_weight_histograms = self._build_weight_histogram(self.weights_params, \"weights\")\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Embedding\"):\n",
    "                # Embedding\n",
    "                if sprites is not None and labels is not None:\n",
    "                    self.embedding = tf.Variable(tf.zeros([256, 1024]), name=\"test_embedding\")\n",
    "                    self.assignment = self.embedding.assign(self.embedding_input)\n",
    "                    self.saver = tf.train.Saver([self.embedding])\n",
    "                \n",
    "            self.writer = tf.summary.FileWriter(log_dir)\n",
    "            self.writer.add_graph(SESS.graph)\n",
    "            \n",
    "            if sprites is not None and labels is not None:\n",
    "                conf = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "                embedding_config = conf.embeddings.add()\n",
    "                embedding_config.tensor_name = self.embedding.name\n",
    "                embedding_config.sprite.image_path = sprites\n",
    "                embedding_config.metadata_path = labels\n",
    "                embedding_config.sprite.single_image_dim.extend([120,120,3])\n",
    "                tf.contrib.tensorboard.plugins.projector.visualize_embeddings(self.writer, conf)\n",
    "    \n",
    "    def _build_model(self,state):\n",
    "        conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "        \n",
    "        return conv1, conv2,fc1,q_value\n",
    "    \n",
    "    def _build_scalar_summary(self, placeholders, names, family):\n",
    "        return tf.summary.merge([tf.summary.scalar(n, i, family=family) for n,i in zip(names, placeholders)])\n",
    "    \n",
    "    def _build_image_summary(self, placeholders,n_output, names, family):\n",
    "        summaries = []\n",
    "        for i, p in enumerate(placeholders):\n",
    "            shape = p.get_shape().as_list()\n",
    "            summ = tf.summary.image(names[i], p, n_output, family=family)\n",
    "            summaries.append(summ)\n",
    "        return tf.summary.merge(summaries)\n",
    "    \n",
    "    def _build_weight_histogram(self, weights, family):\n",
    "        print([w.name for w in weights])\n",
    "        s = [tf.summary.histogram(values=w, name=w.name, family=family) for w in weights]\n",
    "        return tf.summary.merge(s)\n",
    "        \n",
    "    def write_loss(self, step, loss_test, loss_train):\n",
    "        m = SESS.run(self.merged_loss, {self.loss_[0]:loss_test, self.loss_[1]:loss_train})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_accuracy(self, step, acc_test, acc_train):\n",
    "        m = SESS.run(self.merged_accuracy, {self.accuracy_[0]:acc_test, self.accuracy_[1]:acc_train})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_images(self, step, s1):\n",
    "        feed_dict = {self.state1_:s1}\n",
    "        m = SESS.run(self.merged_images, feed_dict)\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_filters(self, step):\n",
    "        m = SESS.run(self.merged_filters)\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_weights(self, step):\n",
    "        m = SESS.run(self.merged_weight_histograms)\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_embedding(self, step, model_path,  img):\n",
    "        SESS.run(self.assignment, feed_dict={self.state1_: img})\n",
    "        return self.saver.save(SESS, model_path, step)\n",
    "    \n",
    "    def copy_weights(self, network):\n",
    "        SESS.run([i.assign(j) for i,j in zip(self.weights_params, network.weights_params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"learning\":\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=\"0\"))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        network = NetworkLocal(\"test\")\n",
    "    log_rec = LogRecorder(log_dir=LOG_DIR, sprites=SPRITES, labels=LABELS)\n",
    "\n",
    "\n",
    "    img, label = load_data()\n",
    "#     label = np.array([l[-1] for l in label_row])\n",
    "    label = label.astype(np.int32)\n",
    "    \n",
    "    train_img, test_img , train_label, test_label = train_test_split(img, label, train_size=0.8, random_state=1)\n",
    "\n",
    "    n_train = np.shape(train_img)[0]\n",
    "    n_test = np.shape(test_label)[0]\n",
    "    print(\"n_train:\",n_train)\n",
    "    print(\"n_test:\", n_test)\n",
    "\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "    \n",
    "    TRAIN_ACC = []\n",
    "    TRAIN_LOSS = []\n",
    "    TEST_ACC = []\n",
    "    TEST_LOSS = []\n",
    "    \n",
    "    sprite_img = np.load(SPRITES_DATA)\n",
    "    \n",
    "    for i in tqdm(range(2000)):\n",
    "        batch_idx = np.random.randint(n_train, size=BATCH_SIZE)\n",
    "        batch_img = train_img[batch_idx]\n",
    "        batch_label = train_label[batch_idx]\n",
    "        l = network.update_parameter_server_batch(batch_img, batch_label)\n",
    "        if (i+1) % 10 == 0:\n",
    "            log_rec.copy_weights(network)\n",
    "\n",
    "            batch_idx = np.random.randint(n_train, size=50)\n",
    "            train_acc = network.get_score(train_img[batch_idx], train_label[batch_idx])\n",
    "            train_loss = network.get_loss(train_img[batch_idx], train_label[batch_idx])\n",
    "            \n",
    "            batch_idx = np.random.randint(n_test, size=50)\n",
    "            test_loss = network.get_loss(test_img[batch_idx], test_label[batch_idx])\n",
    "            test_acc = network.get_score(test_img[batch_idx], test_label[batch_idx])\n",
    "\n",
    "            log_rec.write_loss(i,test_loss, train_loss)\n",
    "            log_rec.write_accuracy(i,test_acc, train_acc)\n",
    "        \n",
    "            TRAIN_LOSS.append(train_loss)\n",
    "            TRAIN_ACC.append(train_acc)\n",
    "            TEST_ACC.append(test_acc)\n",
    "            TEST_LOSS.append(test_loss)\n",
    "            \n",
    "            log_rec.write_weights(i)\n",
    "            log_rec.write_filters(i)\n",
    "            log_rec.write_images(i,img[100:120])\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                log_rec.write_embedding(i, os.path.join(LOG_DIR, 'embedded.ckpt'), sprite_img)\n",
    "\n",
    "            \n",
    "#     network.save_model(os.path.join(MODEL_DIR, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"analysis\":\n",
    "#     network.load_model(MODEL_PATH)\n",
    "    predict_label = []\n",
    "    for s in test_img:\n",
    "        predict_label.append(network.predict_enemyposition([s])[0])\n",
    "\n",
    "    print(n_test)\n",
    "    confusion_mat = confusion_matrix(test_label, predict_label)\n",
    "    print(confusion_mat)\n",
    "    scores = precision_recall_fscore_support(test_label, predict_label)\n",
    "    print(\"---PRECISION---\\n\",  scores[0])\n",
    "    print(\"---RECALL---\\n\", scores[1])\n",
    "    print(\"---FSCORE---\\n\", scores[2])\n",
    "    print(\"---NUMBER of LABELS---\\n\", scores[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"learning\":\n",
    "    x = range(len(TEST_ACC))\n",
    "    plt.plot(x, np.convolve(TEST_ACC, np.ones(5)/5,mode=\"same\"), \"r\",label=\"Test Accuracy\")\n",
    "    plt.plot(x, np.convolve(TRAIN_ACC, np.ones(5)/5,mode=\"same\"),\"b\", label=\"Train Accuracy\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"learning\":\n",
    "    x = range(len(TRAIN_LOSS))\n",
    "    plt.plot(x, np.convolve(TEST_LOSS, np.ones(5)/5,mode=\"same\"), \"r\",label=\"Test Loss\")\n",
    "    plt.plot(x, np.convolve(TRAIN_LOSS, np.ones(5)/5,mode=\"same\"),\"b\", label=\"Train Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__ = \"analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"analysis\":\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=\"0\"))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        network = NetworkLocal(\"test\")\n",
    "    network.load_model(os.path.join(MODEL_DIR, 'model.ckpt'))\n",
    "    \n",
    "    test_img = np.load(\"./logs/predict_enemyposition/sprite_img_256.npy\")\n",
    "    test_label = pd.read_csv(\"./logs/predict_enemyposition/label_256.tsv\", decimal=\"\\t\", header=None).values\n",
    "    \n",
    "    probs = network.get_probability(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    IDX_testlabel = np.where(test_label==3)[0]\n",
    "    IDX_testprobs = np.where(np.argmax(probs, axis=1)==0)[0]\n",
    "    IDX = list(set(IDX_testlabel) & set(IDX_testprobs))\n",
    "    idx = 0\n",
    "    print(len(IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2.bar(range(7),probs[IDX[idx]])\n",
    "    ax1.imshow(test_img[IDX[idx]])\n",
    "    print(probs[IDX[idx]])\n",
    "    print(test_label[IDX[idx]])\n",
    "    print(IDX[idx])\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"make_sprite\":\n",
    "    img, label = load_data()\n",
    "\n",
    "    idx_1 = np.where(label==1)[0][:36]\n",
    "    idx_2 = np.where(label==2)[0][:36]\n",
    "    idx_3 = np.where(label==3)[0][:36]\n",
    "    idx_4 = np.where(label==4)[0][1:37]\n",
    "    idx_5 = np.where(label==5)[0][:36]\n",
    "    idx_6 = np.where(label==6)[0][:36]\n",
    "    idx_0 = np.where(label==0)[0][:40]\n",
    "\n",
    "    idx = np.concatenate([idx_1, idx_2, idx_3, idx_4, idx_5, idx_6, idx_0], axis=0)\n",
    "    idx.sort()\n",
    "\n",
    "    sprite_img = img[idx]\n",
    "    sprite_label = label[idx]\n",
    "    \n",
    "    np.save( 'sprite_img_256.npy', sprite_img)\n",
    "\n",
    "    sprite_img = np.reshape(sprite_img, (16,16,120,120,3))\n",
    "\n",
    "    img_over = np.concatenate([ np.concatenate([sprite_img[i,j] for j in range(16)] , axis=1) for i in range(16)] , axis=0)\n",
    "    img_over *= 255\n",
    "    img_over = img_over.astype(np.int32)\n",
    "    img_over = Image.fromarray(np.uint8(img_over))\n",
    "    img_over.save('sprite_256.png')\n",
    "\n",
    "    df = pd.DataFrame({'label':sprite_label})\n",
    "    df.to_csv('label_256.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"save_weights\":\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=\"0\"))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        network = NetworkLocal(\"test\")\n",
    "\n",
    "    network.load_model(MODEL_DIR+\"model.ckpt\")\n",
    "    weights = SESS.run(network.weights_params)\n",
    "\n",
    "    for w, name in zip(weights[:4], [\"conv1_kernel.npy\", \"conv1_bias.npy\", \"conv2_kernel.npy\", \"conv2_bias.npy\"]):\n",
    "        np.save(w, file=\"./weights_enemypos/\"+name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
