{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from random import sample, randint, random\n",
    "import time,random,threading,datetime\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import sys, os, glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from game_instance import GameInstance\n",
    "# from global_constants import *\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import  RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__ = \"learning\"\n",
    "\n",
    "DEMO_PATH = [\"./demonstration/predict_risksuicide/data_risksuicide_simple%02d.hdf5\"%(i) for i in [1,2]]\n",
    "MODEL_DIR = \"./models/predict_risksuicide/model_test/\"\n",
    "LOG_DIR = \"./logs/log_predict_risksuicide/log_test\"\n",
    "\n",
    "LABELS = os.path.join(os.getcwd(), \"./logs/log_predict_risksuicide/label_simple_256.tsv\")\n",
    "SPRITES = os.path.join(os.getcwd(), \"./logs/log_predict_risksuicide/sprite_simple_256.png\")\n",
    "SPRITES_DATA = os.path.join(os.getcwd(),'./logs/log_predict_risksuicide/sprite_img_simple_256.npy')\n",
    "\n",
    "RESOLUTION = (120, 120, 3)\n",
    "\n",
    "THRESHOLD = 30\n",
    "KEEP_PROB = 0.7\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for f in os.listdir(LOG_DIR):\n",
    "        print(\"removed  \", f)\n",
    "        os.remove(os.path.join(LOG_DIR, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sampling(x,y):\n",
    "    n_positive = sum(y)\n",
    "    x_indice = np.array(range(len(x))).reshape((-1,1))\n",
    "    rus = RandomUnderSampler(ratio={0:n_positive*3, 1:n_positive})\n",
    "    x_indice_resampled, y_resampled = rus.fit_sample(x_indice,y)\n",
    "    x_resampled = x[x_indice_resampled.reshape((-1,))]\n",
    "    return x_resampled, y_resampled, x_indice_resampled.reshape((-1,))\n",
    "#     return x_resampled, y_resampled\n",
    "\n",
    "def over_sampling(x,y):\n",
    "    n_positive = sum(y)\n",
    "    n_negative = len(y) - n_positive\n",
    "    x_indice = np.arange(0,len(x)).reshape((-1,1))\n",
    "    ros = RandomOverSampler(ratio={0:n_negative, 1:n_negative})\n",
    "    x_indice_resampled, y_resampled = ros.fit_sample(x_indice, y)\n",
    "    x_resampled = x[x_indice_resampled.reshape((-1))]\n",
    "    return x_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    batch_img = []\n",
    "    batch_label = []\n",
    "    for d in DEMO_PATH:\n",
    "        print(\"loading \"+d)\n",
    "        file = h5py.File(d, \"r\")\n",
    "        episode_list = list(file.keys())[1:]\n",
    "\n",
    "        for e in episode_list[:]:\n",
    "            n_steps = file[e+\"/states\"].shape[0]\n",
    "            states = file[e+\"/states\"][:]\n",
    "            damages = file[e+\"/damages\"][:]\n",
    "\n",
    "            for img in states:\n",
    "                batch_img.append(img)\n",
    "\n",
    "            for damage in damages:\n",
    "                if damage > THRESHOLD:\n",
    "                    batch_label.append(1)\n",
    "                else:\n",
    "                    batch_label.append(0)\n",
    "\n",
    "        file.close()\n",
    "    return np.array(batch_img), np.array(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLocal(object):\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            self.state1_ = tf.placeholder(tf.float32,shape=(None, )+RESOLUTION, name=\"state_1\")\n",
    "            self.target_ = tf.placeholder(tf.int32, shape=(None,), name=\"area\")\n",
    "            self.keep_prob_ = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "            self.q_model = self._model(self.state1_, self.keep_prob_)\n",
    "            self._build_graph()\n",
    "            self.saver = tf.train.Saver(self.weights_params)\n",
    "\n",
    "#         print(\"-----LOCAL weights---\")\n",
    "#         for w in self.weights_params:\n",
    "#             print(w)\n",
    "            \n",
    "#         print(\"-----LOCAL grads---\")\n",
    "#         for w in self.grads:\n",
    "#             print(w)\n",
    "    \n",
    "    def _model(self,state, keep_prob):\n",
    "\n",
    "        self.conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        self.conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "#         rnn ,l ,_ = NetworkSetting.lstm(reshape, state)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "        drop = NetworkSetting.dropout(fc1, keep_prob)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(drop)\n",
    "        \n",
    "        return q_value\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
    "        \n",
    "        self.prob = tf.nn.softmax(self.q_model, axis=1)\n",
    "        \n",
    "        self.onehot = tf.one_hot(self.target_, depth=2)\n",
    "        self.loss_batch = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.q_model, labels=self.onehot) + 1e-5 * tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights_params])\n",
    "        self.loss = tf.reduce_mean(self.loss_batch)\n",
    "        with tf.variable_scope(\"trainer\"):\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.update_step = optimizer.minimize(self.loss)\n",
    "            \n",
    "        with tf.variable_scope(\"Grad-CAM\"): \n",
    "            cost = self.loss_batch\n",
    "#             cost = tf.reduce_sum((network.prob - onehot) ** 2)\n",
    "#             cost = (-1) * tf.reduce_sum(tf.multiply(onehot, tf.log(network.prob)), axis=1)\n",
    "            y_c = tf.reduce_sum(tf.multiply(self.onehot, self.q_model), axis=1)\n",
    "\n",
    "            self.target_conv_layer_grad = tf.gradients(y_c, self.conv1)[0]\n",
    "            self.gb_grad = tf.gradients(y_c, self.state1_)[0]\n",
    "        return 0\n",
    "    \n",
    "    def update_parameter_server_batch(self, s1, target):\n",
    "\n",
    "        weights = SESS.run(self.weights_params)\n",
    "        assert np.isnan([np.mean(w) for w in weights]).any()==False , print(weights)\n",
    "        feed_dict = {self.state1_: s1, self.target_:target, self.keep_prob_:KEEP_PROB}\n",
    "        l,_ = SESS.run([self.loss, self.update_step], feed_dict)\n",
    "        return l\n",
    "\n",
    "    def predict_enemyposition(self, s1):\n",
    "        \n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            probs = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return [np.random.choice(2, p=p) for p in probs][0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            probs = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return [np.random.choice(2, p=p) for p in probs]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_q_values(self, s1):\n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            q = SESS.run(self.q_model, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return q[0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            q = SESS.run(self.q_model, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return q\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_probability(self, s1):\n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            p = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return p[0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            p = SESS.run(self.prob, {self.state1_:s1, self.keep_prob_:1.0})\n",
    "            return p\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_loss(self, s1, target):\n",
    "        if np.ndim(s1) == 3:\n",
    "            s1 = np.array([s1])\n",
    "            q = SESS.run(self.loss, {self.state1_:s1,  self.target_:target, self.keep_prob_:1.0})\n",
    "            return q[0]\n",
    "        elif np.ndim(s1) == 4:\n",
    "            q = SESS.run(self.loss, {self.state1_:s1,  self.target_:target, self.keep_prob_:1.0})\n",
    "            return q\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_gradient_image(self, img, target):\n",
    "        return SESS.run([self.gb_grad, self.target_conv_layer_grad], {self.state1_:img, self.target_:target, self.keep_prob_:1.0})\n",
    "        \n",
    "    def get_score(self, s1, target):\n",
    "        pred = self.predict_enemyposition(s1)\n",
    "        return sum(pred==target) / len(target)\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        return self.saver.save(SESS, model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        return self.saver.restore(SESS, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "#         kernel_size = [1,6,6]\n",
    "#         stride = [1,3,3]\n",
    "        kernel_size = [6,6]\n",
    "        stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,\\\n",
    "                                        biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 32\n",
    "#         kernel_size = [1,3,3]\n",
    "#         stride = [1,2,2]\n",
    "        kernel_size = [3,3]\n",
    "        stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        shape = pre_layer.get_shape()\n",
    "        return tf.reshape(pre_layer, shape=(-1, shape[1]*shape[2]*shape[3]))\n",
    "    \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs =1024\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def dropout(pre_layer, keep_prob):\n",
    "        return tf.nn.dropout(pre_layer, keep_prob)\n",
    "    \n",
    "    def q_value(pre_layer):\n",
    "        num_outputs =2\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRecorder(object):\n",
    "    def __init__(self,log_dir, sprites = None, labels=None):\n",
    "        \n",
    "        # Place holder\n",
    "        self.state1_ = tf.placeholder(tf.float32,shape=(None,)+RESOLUTION, name=\"state1\")\n",
    "        with tf.variable_scope(\"log_recorder\", reuse=tf.AUTO_REUSE):\n",
    "            with tf.variable_scope(\"model\"):\n",
    "                self.conv1, self.conv2,self.embedding_input, self.model = self._build_model(self.state1_)\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Images\"):\n",
    "                conv1_display = tf.reshape(tf.transpose(self.conv1, [0,3,1,2]), (-1, self.conv1.get_shape()[1],self.conv1.get_shape()[2]))\n",
    "                conv2_display = tf.reshape(tf.transpose(self.conv2, [0,3,1,2]), (-1, self.conv2.get_shape()[1],self.conv2.get_shape()[2]))\n",
    "                conv1_display = tf.expand_dims(conv1_display, -1)\n",
    "                conv2_display = tf.expand_dims(conv2_display, -1)\n",
    "                self.weights_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"log_recorder\")\n",
    "                \n",
    "\n",
    "            with tf.variable_scope(\"Summary_Loss\"):\n",
    "                # Summary for LOSS\n",
    "                self.loss_ = [tf.placeholder(tf.float32,shape=()), tf.placeholder(tf.float32, shape=())]\n",
    "                loss_name = [\"loss_test\",\"loss_train\" ]\n",
    "                self.merged_loss = self._build_scalar_summary(self.loss_, loss_name, \"loss\")\n",
    "\n",
    "            # Summary for  SCORE\n",
    "            with tf.variable_scope(\"Summary_Score\"):\n",
    "                self.accuracy_ = [tf.placeholder(tf.float32, shape=()), tf.placeholder(tf.float32, shape=())]\n",
    "                accuracy_name = [\"accuracy_test\", \"accuracy_train\"]\n",
    "                self.merged_accuracy = self._build_scalar_summary(self.accuracy_, accuracy_name, \"score\")\n",
    "\n",
    "            # Summary for SCREEN\n",
    "            with tf.variable_scope(\"Summary_Images\"):\n",
    "                image_name = [\"state1\", \"conv1\", \"conv2\"] \n",
    "                self.merged_images = self._build_image_summary([self.state1_, conv1_display,conv2_display],n_output=10, names=image_name, family=\"states\")\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Filter\"):\n",
    "                #Sumamry for FILTER\n",
    "                for w in self.weights_params:\n",
    "                    print(w.get_shape())\n",
    "                display_filter_conv1 = tf.reshape(tf.transpose(self.weights_params[0], [2,3,0,1]), (-1,self.weights_params[0].get_shape()[0], self.weights_params[0].get_shape()[1]))\n",
    "                display_filter_conv2 = tf.reshape(tf.transpose(self.weights_params[2], [2,3,0,1]), (-1,self.weights_params[2].get_shape()[0], self.weights_params[2].get_shape()[1]))\n",
    "                display_filter_conv1 = tf.expand_dims(display_filter_conv1, -1)\n",
    "                display_filter_conv2 = tf.expand_dims(display_filter_conv2, -1)\n",
    "                print(display_filter_conv1.get_shape())\n",
    "                print(display_filter_conv2.get_shape())\n",
    "                filter_shapes = [display_filter_conv1.get_shape(), display_filter_conv2.get_shape()]\n",
    "                filter_names = [\"conv1\", \"conv2\"]\n",
    "                self.merged_filters = self._build_image_summary([display_filter_conv1, display_filter_conv2], 10, filter_names, \"filters\")\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Histogram\"):\n",
    "                # Summary WEIGHT HISTOGRAM\n",
    "                self.merged_weight_histograms = self._build_weight_histogram(self.weights_params, \"weights\")\n",
    "\n",
    "            with tf.variable_scope(\"Summary_Embedding\"):\n",
    "                # Embedding\n",
    "                if sprites is not None and labels is not None:\n",
    "                    self.embedding = tf.Variable(tf.zeros([256, self.embedding_input.get_shape()[1]]), name=\"test_embedding\")\n",
    "                    self.assignment = self.embedding.assign(self.embedding_input)\n",
    "                    self.saver = tf.train.Saver([self.embedding])\n",
    "                \n",
    "            self.writer = tf.summary.FileWriter(log_dir)\n",
    "            self.writer.add_graph(SESS.graph)\n",
    "            \n",
    "            if sprites is not None and labels is not None:\n",
    "                conf = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "                embedding_config = conf.embeddings.add()\n",
    "                embedding_config.tensor_name = self.embedding.name\n",
    "                embedding_config.sprite.image_path = sprites\n",
    "                embedding_config.metadata_path = labels\n",
    "                embedding_config.sprite.single_image_dim.extend([120,120,3])\n",
    "                tf.contrib.tensorboard.plugins.projector.visualize_embeddings(self.writer, conf)\n",
    "    \n",
    "    def _build_model(self,state):\n",
    "        conv1 = NetworkSetting.conv1(state)\n",
    "        maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        conv2 = NetworkSetting.conv2(maxpool1)\n",
    "        maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        fc1 = NetworkSetting.fc1(reshape)\n",
    "        \n",
    "        q_value = NetworkSetting.q_value(fc1)\n",
    "        \n",
    "        return conv1, conv2,fc1,q_value\n",
    "    \n",
    "    def _build_scalar_summary(self, placeholders, names, family):\n",
    "        return tf.summary.merge([tf.summary.scalar(n, i, family=family) for n,i in zip(names, placeholders)])\n",
    "    \n",
    "    def _build_image_summary(self, placeholders,n_output, names, family):\n",
    "        summaries = []\n",
    "        for i, p in enumerate(placeholders):\n",
    "            shape = p.get_shape().as_list()\n",
    "            summ = tf.summary.image(names[i], p, n_output, family=family)\n",
    "            summaries.append(summ)\n",
    "        return tf.summary.merge(summaries)\n",
    "    \n",
    "    def _build_weight_histogram(self, weights, family):\n",
    "        print([w.name for w in weights])\n",
    "        s = [tf.summary.histogram(values=w, name=w.name, family=family) for w in weights]\n",
    "        return tf.summary.merge(s)\n",
    "        \n",
    "    def write_loss(self, step, loss_test, loss_train):\n",
    "        m = SESS.run(self.merged_loss, {self.loss_[0]:loss_test, self.loss_[1]:loss_train})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_accuracy(self, step, acc_test, acc_train):\n",
    "        m = SESS.run(self.merged_accuracy, {self.accuracy_[0]:acc_test, self.accuracy_[1]:acc_train})\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_images(self, step, s1):\n",
    "        feed_dict = {self.state1_:s1}\n",
    "        m = SESS.run(self.merged_images, feed_dict)\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_filters(self, step):\n",
    "        m = SESS.run(self.merged_filters)\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_weights(self, step):\n",
    "        m = SESS.run(self.merged_weight_histograms)\n",
    "        return self.writer.add_summary(m, step)\n",
    "    \n",
    "    def write_embedding(self, step, model_path,  img):\n",
    "        SESS.run(self.assignment, feed_dict={self.state1_: img})\n",
    "        return self.saver.save(SESS, model_path, step)\n",
    "    \n",
    "    def copy_weights(self, network):\n",
    "        SESS.run([i.assign(j) for i,j in zip(self.weights_params, network.weights_params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"learning\":\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=\"0\"))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        network = NetworkLocal(\"test\")\n",
    "\n",
    "    log_rec = LogRecorder(log_dir=LOG_DIR, sprites=SPRITES, labels=LABELS)\n",
    "\n",
    "\n",
    "    images, labels = load_data()\n",
    "#     label = np.array([l[-1] for l in label_row])\n",
    "    labels = labels.astype(np.int32)\n",
    "    \n",
    "    images , labels, images_indices = under_sampling(images, labels)\n",
    "    \n",
    "    train_img, test_img , train_label, test_label = train_test_split(images, labels, train_size=0.8, random_state=1)\n",
    "\n",
    "    n_train = np.shape(train_img)[0]\n",
    "    n_test = np.shape(test_label)[0]\n",
    "    print(\"n_train:\",n_train, \"n_positive:\", sum(train_label))\n",
    "    print(\"n_test:\", n_test, \"n_positive:\", sum(test_label))\n",
    "\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "    \n",
    "    TRAIN_ACC = []\n",
    "    TRAIN_LOSS = []\n",
    "    TEST_ACC = []\n",
    "    TEST_LOSS = []\n",
    "    \n",
    "    sprite_img = np.load(SPRITES_DATA)\n",
    "    \n",
    "    for i in tqdm(range(2000)):\n",
    "        batch_idx = np.random.randint(n_train, size=BATCH_SIZE)\n",
    "        batch_img = train_img[batch_idx]\n",
    "        batch_label = train_label[batch_idx]\n",
    "        l = network.update_parameter_server_batch(batch_img, batch_label)\n",
    "        if (i+1) % 10 == 0:\n",
    "            log_rec.copy_weights(network)\n",
    "\n",
    "            batch_idx = np.random.randint(n_train, size=50)\n",
    "            train_acc = network.get_score(train_img[batch_idx], train_label[batch_idx])\n",
    "            train_loss = network.get_loss(train_img[batch_idx], train_label[batch_idx])\n",
    "            \n",
    "            batch_idx = np.random.randint(n_test, size=50)\n",
    "            test_loss = network.get_loss(test_img[batch_idx], test_label[batch_idx])\n",
    "            test_acc = network.get_score(test_img[batch_idx], test_label[batch_idx])\n",
    "\n",
    "            log_rec.write_loss(i,test_loss, train_loss)\n",
    "            log_rec.write_accuracy(i,test_acc, train_acc)\n",
    "        \n",
    "            TRAIN_LOSS.append(train_loss)\n",
    "            TRAIN_ACC.append(train_acc)\n",
    "            TEST_ACC.append(test_acc)\n",
    "            TEST_LOSS.append(test_loss)\n",
    "            \n",
    "            log_rec.write_weights(i)\n",
    "            log_rec.write_filters(i)\n",
    "            log_rec.write_images(i,images[100:120])\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                log_rec.write_embedding(i, os.path.join(LOG_DIR, 'embedded.ckpt'), sprite_img)\n",
    "\n",
    "            \n",
    "    network.save_model(os.path.join(MODEL_DIR, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"learning\":\n",
    "#     network.load_model(MODEL_PATH)\n",
    "    predict_label = []\n",
    "    for s in test_img:\n",
    "        predict_label.append(network.predict_enemyposition([s])[0])\n",
    "\n",
    "    print(n_test)\n",
    "    confusion_mat = confusion_matrix(test_label, predict_label)\n",
    "    print(confusion_mat)\n",
    "    scores = precision_recall_fscore_support(test_label, predict_label)\n",
    "    print(\"---PRECISION---\\n\",  scores[0])\n",
    "    print(\"---RECALL---\\n\", scores[1])\n",
    "    print(\"---FSCORE---\\n\", scores[2])\n",
    "    print(\"---NUMBER of LABELS---\\n\", scores[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"learning\":\n",
    "    x = range(len(TEST_ACC))\n",
    "    plt.plot(x, np.convolve(TEST_ACC, np.ones(5)/5,mode=\"same\"), \"r\",label=\"Test Accuracy\")\n",
    "    plt.plot(x, np.convolve(TRAIN_ACC, np.ones(5)/5,mode=\"same\"),\"b\", label=\"Train Accuracy\")\n",
    "    plt.legend()\n",
    "    x = range(len(TRAIN_LOSS))\n",
    "    plt.plot(x, np.convolve(TEST_LOSS, np.ones(5)/5,mode=\"same\"), \"r\",label=\"Test Loss\")\n",
    "    plt.plot(x, np.convolve(TRAIN_LOSS, np.ones(5)/5,mode=\"same\"),\"b\", label=\"Train Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__=\"analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"analysis\":\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=\"0\"))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        network = NetworkLocal(\"test\")\n",
    "    network.load_model(os.path.join(MODEL_DIR, 'model.ckpt'))\n",
    "    \n",
    "    \n",
    "    test_img = np.load(SPRITES_DATA)\n",
    "    test_label = pd.read_csv(LABELS, delimiter=\"\\t\", header=None).values\n",
    "    test_label = np.ravel(test_label)\n",
    "    predict_label = network.predict_enemyposition(test_img)\n",
    "    confusion_mat = confusion_matrix(test_label, predict_label)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    print(test_img.shape)\n",
    "    print(test_label.shape)\n",
    "    \n",
    "    gb_grad_values,  target_conv_layer_grad_values = network.get_gradient_image(test_img, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    IDX = []\n",
    "    for i,l in enumerate(test_label):\n",
    "        if(l==1):\n",
    "            IDX.append(i)\n",
    "    idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(test_label[IDX[idx]])\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(gb_grad_values[IDX[idx]])\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(target_conv_layer_grad_values[IDX[idx], :, :, 0])\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(test_img[IDX[idx]])\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"make_sprite\":\n",
    "    \n",
    "    idx_0 = np.where(test_label==0)[0][:128]\n",
    "    idx_1 = np.where(test_label==1)[0][:128]\n",
    "\n",
    "    idx = np.concatenate([idx_0, idx_1], axis=0)\n",
    "    idx.sort()\n",
    "\n",
    "    sprite_img = test_img[idx]\n",
    "    sprite_label = test_label[idx]\n",
    "    \n",
    "    np.save( './logs/log_predict_risksuicide/sprite_img_simple_256.npy', sprite_img)\n",
    "\n",
    "    sprite_img = np.reshape(sprite_img, (16,16,120,120,3))\n",
    "\n",
    "    img_over = np.concatenate([ np.concatenate([sprite_img[i,j] for j in range(16)] , axis=1) for i in range(16)] , axis=0)\n",
    "    img_over *= 255\n",
    "    img_over = img_over.astype(np.int32)\n",
    "    img_over = Image.fromarray(np.uint8(img_over))\n",
    "    img_over.save('./logs/log_predict_risksuicide/sprite_simple_256.png')\n",
    "\n",
    "    df = pd.DataFrame({'label':sprite_label})\n",
    "    df.to_csv('./logs/log_predict_risksuicide/label_simple_256.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"makedata\":\n",
    "    def preprocess(img):\n",
    "        if len(img.shape) == 3 and img.shape[0]==3:\n",
    "            img = img.transpose(1,2,0)\n",
    "        \n",
    "        img = skimage.transform.resize(img, RESOLUTION, mode=\"constant\")\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "\n",
    "    def convert_action_agent2engine(agent_action):\n",
    "        assert type(agent_action) == type(int()) or type(agent_action) == type(np.int64()), print(\"type(agent_action)=\",type(agent_action))\n",
    "        ans = []\n",
    "        for i in range(6):\n",
    "            ans.append(agent_action%2)\n",
    "            agent_action = int(agent_action / 2)\n",
    "        return ans\n",
    "    \n",
    "    game = GameInstance(game=DoomGame(), config_file_path=CONFIG_FILE_PATH,name='test', n_adv=5, rewards=REWARDS)\n",
    "    IMG_BUFF = []\n",
    "    DAMAGE_BUFF = []\n",
    "    ATTACK_BUFF = []\n",
    "    pre_health = 100.0\n",
    "    for i in range(10):\n",
    "        game.new_episode(0)\n",
    "        while not game.is_episode_finished():\n",
    "            s = preprocess(game.get_screen_buff())\n",
    "            h = game.get_health()\n",
    "            engine_action =convert_action_agent2engine( np.random.randint(32) * 2)\n",
    "            game.make_action(engine_action, 4)\n",
    "            IMG_BUFF.append(s)\n",
    "            DAMAGE_BUFF.append(pre_health - game.get_health())\n",
    "            pre_health = game.get_health()\n",
    "\n",
    "            if (game.is_player_dead()):\n",
    "                game.respawn_player()\n",
    "                pre_health = 100.0\n",
    "                \n",
    "    DAMAGE_BUFF = np.array(DAMAGE_BUFF)\n",
    "    IMG_BUFF= np.array(IMG_BUFF)\n",
    "    \n",
    "    idx_negative_sample = np.where(DAMAGE_BUFF <= 0)[0]\n",
    "    n_negative_sample = idx_negative_sample.shape[0]\n",
    "    idx_positive_sample = np.where(DAMAGE_BUFF > 0)[0]\n",
    "    \n",
    "    deleted_idx = np.random.choice(idx_negative_sample, int(n_negative_sample/2))\n",
    "    IMG_BUFF = np.delete(IMG_BUFF, deleted_idx, axis=0)\n",
    "    DAMAGE_BUFF = np.delete(DAMAGE_BUFF, deleted_idx, axis=0)\n",
    "    \n",
    "    GROUP = \"4\"\n",
    "    with h5py.File('data_risksuicide01.hdf5', \"r+\") as f:\n",
    "        f.create_group(GROUP+\"/\")\n",
    "\n",
    "\n",
    "        f.create_dataset(GROUP+\"/states\", data=IMG_BUFF)\n",
    "        f.create_dataset(GROUP+\"/damages\", data=DAMAGE_BUFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"save_weights\":\n",
    "    config = tf.ConfigProto(gpu_options = tf.GPUOptions(visible_device_list=\"0\"))\n",
    "    config.log_device_placement = False\n",
    "    config.allow_soft_placement = True\n",
    "    SESS = tf.Session(config=config)\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        network = NetworkLocal(\"test\")\n",
    "    \n",
    "    network.load_model(MODEL_DIR+\"model.ckpt\")\n",
    "    weights = SESS.run(network.weights_params)\n",
    "    for w,name in zip(weights, [\"conv1_kernel.npy\", \"conv1_bias.npy\", \"conv2_kernel.npy\", \"conv2_bias.npy\"]):\n",
    "        np.save(\"./weights_suicide/\"+name, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
