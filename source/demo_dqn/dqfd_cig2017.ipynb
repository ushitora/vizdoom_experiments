{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage.color, skimage.transform\n",
    "from random import sample, randint, random\n",
    "from tqdm import tqdm\n",
    "import transition\n",
    "import tensorflow as tf\n",
    "import replay_memory\n",
    "import transition\n",
    "import h5py\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_PATH = \"./demonstration/demodata_cig2017_v0-2.h5\"\n",
    "CONFIG_FILE_PATH = \"./config/custom_config.cfg\"\n",
    "LOG_DIR = \"./logs_v01\"\n",
    "\n",
    "RESOLUTION = (120,180,3)\n",
    "\n",
    "N_ADV = 10\n",
    "\n",
    "FREQ_COPY = 10\n",
    "FREQ_TEST = 50\n",
    "\n",
    "N_PRESTEPS = 5\n",
    "N_STEPS = 200\n",
    "TOTAL_STEPS = N_STEPS\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "LEARNING_RATE = 0.5\n",
    "FRAME_REPEAT = 10\n",
    "BATCH_SIZE = 64\n",
    "LAMBDA1 = 1.0\n",
    "LAMBDA2 = 1.0\n",
    "LAMBDA3 = 10e-5\n",
    "L_MIN = 0.8\n",
    "\n",
    "N_ACTION = 6\n",
    "\n",
    "BOTS_NUM = 20\n",
    "\n",
    "N_FOLDER = 2\n",
    "\n",
    "REWARDS = {'living':-0.01, 'health_loss':-1, 'medkit':50, 'ammo':0.0, 'frag':500, 'dist':3e-2, 'suicide':-500} \n",
    "\n",
    "CAPACITY = 10000\n",
    "\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.0\n",
    "LINEAR_EPS_START = 0.1\n",
    "LINEAR_EPS_END = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self,name):\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(CONFIG_FILE_PATH)\n",
    "        self.game.set_window_visible(False)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "#         self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        self.game.set_screen_format(ScreenFormat.CRCGCB)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "        \n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen = RewardGenerater(health,ammo,frag,pos_x,pos_y)\n",
    "        \n",
    "        self.replay_buff = replay_memory.ReplayMemory(CAPACITY,data_name=\"demodata_cig2017.npy\")\n",
    "        self.network = Network()\n",
    "        self.agent = Agent(self.network,self.replay_buff,self.reward_gen)\n",
    "        \n",
    "        self.local_step = 0\n",
    "        \n",
    "        self.finished = False\n",
    "        \n",
    "        self.name = name\n",
    "\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.game.new_episode()\n",
    "        for i in range(BOTS_NUM):\n",
    "            self.game.send_game_command(\"addbot\")\n",
    "        \n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION,mode='constant')\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def get_reward(self):\n",
    "        health = self.game.get_game_variable(GameVariable.HEALTH)\n",
    "        ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO)\n",
    "        frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pos_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        pos_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        \n",
    "        r,r_detail = self.reward_gen.get_reward(health,ammo,frag,pos_x,pos_y)\n",
    "    \n",
    "        return r, r_detail\n",
    "    \n",
    "    def load_demonstration(self,isnpy=False):\n",
    "        if isnpy:\n",
    "            self.replay_buff.load_data(dir_path)\n",
    "        else:\n",
    "            hdf5file = h5py.File(DEMO_PATH,\"r\")\n",
    "            folder = \"demodata_\"+str(0)\n",
    "            state1 = hdf5file[folder+\"/state1\"].value\n",
    "            state2 = hdf5file[folder+\"/state2\"].value\n",
    "            actions = hdf5file[folder+\"/actions\"].value\n",
    "            isterminals = hdf5file[folder+\"/isterminals\"].value\n",
    "            health = hdf5file[folder+\"/healths\"].value\n",
    "            ammo = hdf5file[folder+\"/ammos\"].value\n",
    "            posx = hdf5file[folder+\"/posxs\"].value\n",
    "            posy = hdf5file[folder+\"/posys\"].value\n",
    "            death = hdf5file[folder+\"/deaths\"].value\n",
    "            frag = hdf5file[folder+\"/frags\"].value\n",
    "            \n",
    "            for i in range(1,N_FOLDER):\n",
    "                folder = \"demodata_\" +str(i)\n",
    "                state1 = np.concatenate((state1,hdf5file[folder+\"/state1\"].value),axis=0)\n",
    "                state2 = np.concatenate((state2,hdf5file[folder+\"/state2\"].value),axis=0)\n",
    "                actions = np.concatenate((actions,hdf5file[folder+\"/actions\"].value),axis=0)\n",
    "                isterminals = np.concatenate((isterminals,hdf5file[folder+\"/isterminals\"].value),axis=0)\n",
    "                health = np.concatenate((health,hdf5file[folder+\"/healths\"].value),axis=0)\n",
    "                ammo = np.concatenate((ammo,hdf5file[folder+\"/ammos\"].value),axis=0)\n",
    "                posx = np.concatenate((posx,hdf5file[folder+\"/posxs\"].value),axis=0)\n",
    "                posy = np.concatenate((posy,hdf5file[folder+\"/posys\"].value),axis=0)\n",
    "                death = np.concatenate((death,hdf5file[folder+\"/deaths\"].value),axis=0)\n",
    "                frag = np.concatenate((frag,hdf5file[folder+\"/frags\"].value),axis=0)\n",
    "\n",
    "            n_transit, n_step, _ = actions.shape\n",
    "            \n",
    "            print(\"SIZE of DEMO:\",actions.shape)\n",
    "\n",
    "            transit = np.empty((n_step,),dtype=object)\n",
    "\n",
    "            is_dead = False\n",
    "            is_finished = False\n",
    "\n",
    "            pre_health = 100\n",
    "            pre_ammo = 15\n",
    "            pre_frag = 0\n",
    "            pre_death = 0\n",
    "            pre_posx = 0.0\n",
    "            pre_posy = 0.0\n",
    "\n",
    "\n",
    "            for i in range(n_transit):\n",
    "\n",
    "                if i % 2 == 0:\n",
    "                    pre_posx = posx[i][0]\n",
    "                    pre_posy = posy[i][0]\n",
    "\n",
    "                for j in range(n_step):\n",
    "                    if not is_finished:\n",
    "                        if is_dead :\n",
    "                            pre_posx = posx[i][j]\n",
    "                            pre_posy = posy[i][j]\n",
    "                            is_dead = False\n",
    "\n",
    "                        m_frag = frag[i][j] - pre_frag\n",
    "                        m_death = death[i][j] - pre_death\n",
    "                        m_health = health[i][j] - pre_health\n",
    "                        m_ammo = ammo[i][j] - pre_ammo\n",
    "                        m_posx = posx[i][j] - pre_posx\n",
    "                        m_posy = posy[i][j] - pre_posy\n",
    "\n",
    "                        if m_death >= 1:\n",
    "                            is_dead = True \n",
    "\n",
    "                        if isterminals[i][j] == True:\n",
    "                            is_finished = True\n",
    "                            \n",
    "                        r_d = self.reward_gen.calc_reward(m_frag,m_death,m_health,m_ammo,m_posx,m_posy)\n",
    "                        r = sum(r_d.values())\n",
    "                        transit[j] = transition.Transition(state1[i][j],actions[i][j],state2[i][j],r,isterminals[i][j],True)\n",
    "\n",
    "                        pre_frag = frag[i][j]\n",
    "                        pre_death = death[i][j]\n",
    "                        pre_health = health[i][j]\n",
    "                        pre_ammo = ammo[i][j]\n",
    "                    else:\n",
    "                        transit[j] = transition.Transition(None,None,None,None,True,True)\n",
    "                        \n",
    "                is_finished = False\n",
    "                \n",
    "                self.replay_buff.store(np.copy(transit))\n",
    "                \n",
    "    def test_score(self):        \n",
    "\n",
    "        pre_frag = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        pre_death = self.game.get_game_variable(GameVariable.DEATHCOUNT)\n",
    "\n",
    "        self.start_episode()\n",
    "        \n",
    "        total_reward = 0\n",
    "        total_reward_detail = {'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "        \n",
    "        total_steps = 0\n",
    "        \n",
    "        posx = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "        posy = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "        self.reward_gen.new_episode(100,15,posx,posy)\n",
    "\n",
    "        while not self.game.is_episode_finished():\n",
    "            if self.game.is_player_dead():\n",
    "                self.game.respawn_player()\n",
    "                \n",
    "                posx = self.game.get_game_variable(GameVariable.POSITION_X)\n",
    "                posy = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
    "                self.reward_gen.respawn_pos(100,15,posx,posy)\n",
    "                \n",
    "            if total_steps%2 == 0:\n",
    "                self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                  self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "            if self.game.is_episode_finished():\n",
    "                break\n",
    "\n",
    "            action = self.agent.act_greedy(self.preprocess(self.game.get_state().screen_buffer))\n",
    "\n",
    "            self.game.make_action(action, 5)\n",
    "\n",
    "            reward, r_d  = self.get_reward()\n",
    "            total_reward += reward\n",
    "            for k,v in r_d.items():\n",
    "                total_reward_detail[k] += v\n",
    "            \n",
    "            total_steps += 1\n",
    "            \n",
    "\n",
    "        frag_count = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        death_count = self.game.get_game_variable(GameVariable.DEATHCOUNT)\n",
    "\n",
    "        return total_steps,total_reward, total_reward_detail,frag_count-pre_frag, death_count-pre_death\n",
    "                \n",
    "    def pre_learning(self):\n",
    "        \n",
    "        global frames\n",
    "        for i in tqdm(range(N_PRESTEPS)):\n",
    "            self.agent.perform_pre_learning_step()\n",
    "            frames += 1\n",
    "    \n",
    "    def run(self):\n",
    "#         global frames,runout\n",
    "        global frames\n",
    "    \n",
    "        self.start_episode()\n",
    "        \n",
    "        train_episode = 0\n",
    "        step = 0\n",
    "        transitions = np.empty((N_ADV,),dtype=object)\n",
    "        for step in tqdm(range(N_STEPS)):\n",
    "\n",
    "            if not self.game.is_episode_finished():\n",
    "                \n",
    "                if step%N_ADV==0 and not step==0:\n",
    "                    self.replay_buff.store(np.copy(transitions))\n",
    "                    self.reward_gen.update_origin(self.game.get_game_variable(GameVariable.POSITION_X),\\\n",
    "                                                  self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "                s1 = self.preprocess(self.game.get_state().screen_buffer)\n",
    "                action = self.agent.act_eps_greedy(s1)\n",
    "                action_idx = action.index(1)\n",
    "                self.game.make_action(action)\n",
    "                reward,_ = self.get_reward()\n",
    "                isterminal = self.game.is_episode_finished()\n",
    "                s2 = self.preprocess(self.game.get_state().screen_buffer) if not isterminal else None\n",
    "                \n",
    "                transitions[step%10]=transition.Transition(s1,action_idx,s2,reward,isterminal,False)\n",
    "                \n",
    "                if self.game.is_player_dead():\n",
    "                    self.game.respawn_player()\n",
    "                    self.reward_gen.respawn_pos(self.game.get_game_variable(GameVariable.HEALTH),                                                 self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO),                                                 self.game.get_game_variable(GameVariable.POSITION_X),                                                self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "            else:\n",
    "                train_episode += 1\n",
    "                self.start_episode()\n",
    "                self.reward_gen.new_episode(health = self.game.get_game_variable(GameVariable.HEALTH),                                            ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO),                                            posx = self.game.get_game_variable(GameVariable.POSITION_X),                                            posy = self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "            self.local_step += 1   \n",
    "            frames += 1\n",
    "            step += 1\n",
    "            self.agent.batch_learn()\n",
    "            if frames%FREQ_COPY:\n",
    "                self.network.copy_params()\n",
    "                \n",
    "            if frames % FREQ_TEST == 0:\n",
    "                steps_test,r,r_d,f,d = self.test_score()\n",
    "                print(\"\\t TEST at \", frames)\n",
    "                print(\"\\t SPEND STEPS:\",steps_test)\n",
    "                print(\"\\tFRAG:\",f,\"DEATH:\",d,\"REWARD:\",r)\n",
    "                print(\"\\t\",r_d)\n",
    "                self.agent.q_network.write_score(step=frames,reward=r,frag=f,death=d)\n",
    "                self.start_episode()\n",
    "                self.reward_gen.new_episode(health = self.game.get_game_variable(GameVariable.HEALTH), \\\n",
    "                                            ammo = self.game.get_game_variable(GameVariable.SELECTED_WEAPON_AMMO), \\\n",
    "                                            posx = self.game.get_game_variable(GameVariable.POSITION_X), \\\n",
    "                                            posy = self.game.get_game_variable(GameVariable.POSITION_Y))\n",
    "\n",
    "        print(self.name,\" finished\")\n",
    "        self.finished = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardGenerater(object):\n",
    "    def __init__(self,health,ammo,frag,pos_x,pos_y):\n",
    "\n",
    "        # Reward\n",
    "        self.rewards = REWARDS\n",
    "        self.dist_unit = 6.0\n",
    "        \n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y\n",
    "        \n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "        self.pre_frag = frag\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self.total_reward_detail = {'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "\n",
    "    \n",
    "    def get_reward(self,health,ammo,frag,pos_x,pos_y):\n",
    "        \n",
    "        if abs(health) > 10000:\n",
    "            health = 100.0\n",
    "\n",
    "        if self.origin_x == 0 and self.origin_y == 0:\n",
    "            self.origin_x = pos_x\n",
    "            self.origin_y = pos_y\n",
    "        \n",
    "        self.reward_detail = self.calc_reward(frag-self.pre_frag,0.0, \\\n",
    "                                              health-self.pre_health,\\\n",
    "                                              ammo-self.pre_ammo, \\\n",
    "                                              pos_x-self.origin_x, \\\n",
    "                                              pos_y-self.origin_y)\n",
    "        self.reward = sum(self.reward_detail.values())\n",
    "\n",
    "        for k,v in self.reward_detail.items():\n",
    "            self.total_reward_detail[k] += v\n",
    "        self.total_reward = sum(self.total_reward_detail.values())\n",
    "\n",
    "        self.pre_frag = frag\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "                    \n",
    "        return (self.reward, self.reward_detail)\n",
    "    \n",
    "    def calc_reward(self,m_frag,m_death,m_health,m_ammo,m_posx,m_posy):\n",
    "\n",
    "        ret_detail = {}\n",
    "\n",
    "        ret_detail['living'] = self.rewards['living']\n",
    "\n",
    "        if m_frag >= 0:\n",
    "            ret_detail['frag'] = (m_frag)*self.rewards['frag']\n",
    "            ret_detail['suicide'] = 0.0\n",
    "        else:\n",
    "            ret_detail['suicide'] = (m_frag*-1)*(self.rewards['suicide'])\n",
    "            ret_detail['frag'] = 0.0\n",
    "        \n",
    "        ret_detail['dist'] = int((math.sqrt((m_posx)**2 + (m_posy)**2))/self.dist_unit) * (self.rewards['dist'] * self.dist_unit)\n",
    "        \n",
    "        if m_health > 0:\n",
    "            ret_detail['medkit'] = self.rewards['medkit']\n",
    "            ret_detail['health_loss'] = 0.0\n",
    "        else:\n",
    "            ret_detail['medkit'] = 0.0\n",
    "            ret_detail['health_loss'] = (m_health)*self.rewards['health_loss'] * (-1)\n",
    "\n",
    "        ret_detail['ammo'] = (m_ammo)*self.rewards['ammo'] if m_ammo>0 else 0.0\n",
    "        \n",
    "        return ret_detail \n",
    "    \n",
    "    def respawn_pos(self,health,ammo,posx, posy):\n",
    "        self.origin_x = posx\n",
    "        self.origin_y = posy\n",
    "        self.pre_health = health\n",
    "        self.pre_ammo = ammo\n",
    "\n",
    "    def new_episode(self,health,ammo,posx,posy):\n",
    "        self.respawn_pos(health,ammo,posx,posy)\n",
    "        self.pre_frag = 0\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.total_reward_detail={'living':0.0, 'health_loss':0.0, 'medkit':0.0, 'ammo':0.0, 'frag':0.0, 'dist':0.0, 'suicide': 0.0}\n",
    "    \n",
    "    def update_origin(self,pos_x, pos_y):\n",
    "        self.origin_x = pos_x\n",
    "        self.origin_y = pos_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self,q_network,replay_buff,reward_gen):\n",
    "        self.q_network = q_network\n",
    "        self.replay_buff = replay_buff\n",
    "        \n",
    "        self.reward_repeat = 0\n",
    "\n",
    "        self.record_score = 0\n",
    "\n",
    "    def preprocess(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.transpose(1,2,0)\n",
    "\n",
    "        img = skimage.transform.resize(img, RESOLUTION)\n",
    "        img = img.astype(np.float32)\n",
    "        return img\n",
    "\n",
    "    def reset_reward_generator(self,reward_gen):\n",
    "        self.reward_gen = reward_gen\n",
    "\n",
    "    def margin(self, a_demo):\n",
    "        ret = np.ones((BATCH_SIZE,N_ACTION),dtype=np.float32) * L_MIN\n",
    "        ret[:,a_demo] = 0.0\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def batch_learn(self):\n",
    "\n",
    "        tree_idx, batch, is_weight = self.replay_buff.sample(BATCH_SIZE)\n",
    "\n",
    "        s1 = np.zeros((N_ADV,BATCH_SIZE)+RESOLUTION,dtype=np.float32)\n",
    "        s2 = np.zeros((N_ADV,BATCH_SIZE)+RESOLUTION,dtype=np.float32)\n",
    "        actions = np.zeros((N_ADV,BATCH_SIZE,),dtype=np.int8)\n",
    "        rewards = np.zeros((N_ADV,BATCH_SIZE,),dtype=np.float32)\n",
    "        isterminals = np.zeros((N_ADV,BATCH_SIZE,),dtype=np.int8)\n",
    "        isdemos = np.zeros((N_ADV,BATCH_SIZE,),dtype=np.int8)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            for j in range(N_ADV):\n",
    "                isterminals[j][i] = batch[i][j].isterminal\n",
    "                isdemos[j][i] = batch[i][j].isdemo\n",
    "                if isterminals[j][i] == False:\n",
    "                    s1[j][i] = batch[i][j].s1\n",
    "                    s2[j][i] = batch[i][j].s2\n",
    "                    actions[j][i] = batch[i][j].action\n",
    "                    rewards[j][i] = batch[i][j].reward\n",
    "                else:\n",
    "                    if type(batch[i][j].s1) != type(None):\n",
    "                        s1[j][i] = batch[i][j].s1\n",
    "                        actions[j][i] = batch[i][j].action\n",
    "                        rewards[j][i] = batch[i][j].reward\n",
    "\n",
    "        q2 = self.q_network.get_q_target_values(s2[0][:])\n",
    "        target_q = self.q_network.get_q_values(s1[0][:])\n",
    "\n",
    "        target_q_nsteps = np.empty((BATCH_SIZE, N_ACTION),dtype=np.float32)\n",
    "        q_demo = np.empty((BATCH_SIZE, N_ACTION),dtype=np.float32)\n",
    "        target_q_nsteps[:,:] = target_q\n",
    "        q_demo[:,:] = target_q\n",
    "\n",
    "        td_error = rewards[0] + DISCOUNT * (1-isterminals[0]) * q2\n",
    "\n",
    "        target_q[range(target_q.shape[0]),actions[0]] = td_error\n",
    "\n",
    "        target_q_nsteps[range(target_q_nsteps.shape[0]),actions[0]] = \\\n",
    "                        DISCOUNT*(1-isterminals[N_ADV-1]) * self.q_network.get_q_target_values(s2[N_ADV-1])\n",
    "        for i in range(N_ADV-2,-1,-1):\n",
    "            target_q_nsteps[range(target_q_nsteps.shape[0]),actions[0]] = \\\n",
    "                        (1-isterminals[i])*(rewards[i] + DISCOUNT*target_q_nsteps[range(target_q_nsteps.shape[0]),actions[0]]) + \\\n",
    "                        (isterminals[i])*(self.q_network.get_q_target_values(s2[i]))\n",
    "\n",
    "        loss_class = (isdemos[0])*(np.amax((q_demo + self.margin(actions[0])), axis=1) - q_demo[np.arange(q_demo.shape[0]),actions[0]])\n",
    "\n",
    "        self.q_network.learn(s1[0], target_q, target_q_nsteps, loss_class, is_weight)\n",
    "\n",
    "    def perform_pre_learning_step(self):\n",
    "\n",
    "        self.batch_learn()\n",
    "\n",
    "        if frames % FREQ_COPY == 0:\n",
    "            self.q_network.copy_params()\n",
    "\n",
    "    def act_eps_greedy(self,s1):\n",
    "        \n",
    "        global frames\n",
    "            \n",
    "        if frames<TOTAL_STEPS*LINEAR_EPS_START:\n",
    "            eps = EPS_START\n",
    "        elif frames>=TOTAL_STEPS*LINEAR_EPS_START and frames<TOTAL_STEPS*LINEAR_EPS_END:\n",
    "            eps = EPS_START + frames*(EPS_END-EPS_START)/(TOTAL_STEPS)\n",
    "        else:\n",
    "            eps = EPS_END\n",
    "         \n",
    "        ret_action = np.zeros((N_ACTION,))\n",
    "        if random() <= eps:\n",
    "            s1 = np.array(([s1]))\n",
    "            a_idx = self.q_network.get_best_action(s1)\n",
    "        else:\n",
    "            a_idx = randint(0,N_ACTION-1)\n",
    "        \n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "    \n",
    "    def act_greedy(self,s1):\n",
    "        ret_action = np.zeros((N_ACTION,))\n",
    "        s1 = np.array(([s1]))\n",
    "        a_idx = self.q_network.get_best_action(s1)        \n",
    "        ret_action[a_idx] = 1\n",
    "        return ret_action.tolist()\n",
    "\n",
    "    def restore_model(self, model_path):\n",
    "        self.q_network.restore_model(model_path)\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        self.q_network.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSetting:\n",
    "    \n",
    "    def conv1(pre_layer):\n",
    "        num_outputs = 32\n",
    "        kernel_size = [6,6]\n",
    "        stride = [3,3]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,\\\n",
    "                                        num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,\\\n",
    "                                        biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool1(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    \n",
    "    def conv2(pre_layer):\n",
    "        num_outputs = 64\n",
    "        kernel_size = [3,3]\n",
    "        stride = [2,2]\n",
    "        padding = 'SAME'\n",
    "        activation = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.conv2d(pre_layer,kernel_size=kernel_size,num_outputs=num_outputs,\\\n",
    "                                        stride=stride,padding=padding,activation_fn=activation,\\\n",
    "                                        weights_initializer=weights_init,biases_initializer=bias_init)\n",
    "    \n",
    "    def maxpool2(pre_layer):\n",
    "        return tf.nn.max_pool(pre_layer,[1,3,3,1],[1,2,2,1],'SAME')\n",
    "        \n",
    "    def reshape(pre_layer):\n",
    "        return tf.contrib.layers.flatten(pre_layer)\n",
    "        \n",
    "    def fc1(pre_layer):\n",
    "        num_outputs = 512\n",
    "        activation_fn = tf.nn.relu\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n",
    "    \n",
    "    def q_value(pre_layer):\n",
    "        num_outputs = N_ACTION\n",
    "        activation_fn = None\n",
    "        weights_init = tf.contrib.layers.xavier_initializer()\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        return tf.contrib.layers.fully_connected(pre_layer,num_outputs=num_outputs,activation_fn=activation_fn,\\\n",
    "                                                 weights_initializer=weights_init, biases_initializer=bias_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.s1_ = tf.placeholder(tf.float32, [None] + [RESOLUTION[0],RESOLUTION[1], RESOLUTION[2]], name=\"State\")\n",
    "        self.a_ = tf.placeholder(tf.int32, [None], name=\"Action\")\n",
    "        self.target_q_one_ = tf.placeholder(tf.float32, [None, N_ACTION], name=\"TargetQ_one\")\n",
    "        self.target_q_n_ = tf.placeholder(tf.float32, [None, N_ACTION], name=\"TargetQ_n\")\n",
    "        self.loss_class_ = tf.placeholder(tf.float32,[None], name=\"Loss_Classification\")\n",
    "\n",
    "        self.is_weight_ = tf.placeholder(tf.float32,[None],name=\"IS_weight\")\n",
    "        \n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            with tf.variable_scope(\"learning\"):\n",
    "                self.model = self._model(True)\n",
    "            with tf.variable_scope(\"target\"):\n",
    "                self.model_t = self._model(False)\n",
    "\n",
    "            self._graph()\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self._summary()\n",
    "    \n",
    "    def _model(self,isLearning):\n",
    "        if isLearning:\n",
    "            self.conv1 = NetworkSetting.conv1(self.s1_)\n",
    "            maxpool1 = NetworkSetting.maxpool1(self.conv1)\n",
    "        else:\n",
    "            conv1 = NetworkSetting.conv1(self.s1_)\n",
    "            maxpool1 = NetworkSetting.maxpool1(conv1)\n",
    "        if isLearning:\n",
    "            self.conv2 = NetworkSetting.conv2(maxpool1)\n",
    "            maxpool2 = NetworkSetting.maxpool2(self.conv2)\n",
    "        else:\n",
    "            conv2 = NetworkSetting.conv2(maxpool1)\n",
    "            maxpool2 = NetworkSetting.maxpool2(conv2)\n",
    "        reshape = NetworkSetting.reshape(maxpool2)\n",
    "        fc1 = NetworkSetting.reshape(reshape)\n",
    "        return NetworkSetting.q_value(fc1)\n",
    "    \n",
    "    def _graph(self):\n",
    "\n",
    "        self.best_action = tf.argmax(self.model,1)\n",
    "        \n",
    "        self.weights_params_learning = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"learning\")\n",
    "        self.weights_params_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target\")\n",
    "\n",
    "        self.loss_one = tf.losses.mean_squared_error(self.model,self.target_q_one_)\n",
    "        self.loss_n = tf.scalar_mul(tf.constant(LAMBDA1), tf.losses.mean_squared_error(self.model, self.target_q_n_))\n",
    "        self.loss_class = tf.scalar_mul(tf.constant(LAMBDA2),tf.reduce_mean(self.loss_class_))\n",
    "        self.loss_l2 = tf.scalar_mul(tf.constant(LAMBDA3),tf.add_n(self.get_l2_loss()))        \n",
    "\n",
    "        self.loss = tf.add_n([self.loss_one,self.loss_n,self.loss_class,self.loss_l2])\n",
    "\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE)\n",
    "\n",
    "        self.train_step = self.optimizer.minimize(tf.multiply(self.is_weight_,self.loss))\n",
    "        \n",
    "        self.copy_learn2target = [t_p.assign(l_p) for l_p,t_p in zip(self.weights_params_learning,self.weights_params_target)]\n",
    "    \n",
    "    def _summary(self):\n",
    "        \n",
    "        self.reward_ = tf.placeholder(tf.float32)\n",
    "        self.frag_ = tf.placeholder(tf.int64)\n",
    "        self.death_ = tf.placeholder(tf.int64)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        summary_lo = tf.summary.scalar('loss_one',self.loss_one)\n",
    "        summary_ln = tf.summary.scalar('loss_n', self.loss_n)\n",
    "        summary_ll = tf.summary.scalar('loss_l2', self.loss_l2)\n",
    "        summary_lc = tf.summary.scalar('loss_class',self.loss_class)\n",
    "\n",
    "        self.merged_scalar = tf.summary.merge([summary_lo,summary_ln,summary_ll,summary_lc])\n",
    "\n",
    "        state_shape = self.s1_.get_shape()\n",
    "        conv1_shape = self.conv1.get_shape()\n",
    "        conv2_shape = self.conv2.get_shape()\n",
    "        summary_state  = tf.summary.image('state',tf.reshape(self.s1_,[-1, state_shape[1], state_shape[2], state_shape[3]]),1)\n",
    "        summary_conv1 = tf.summary.image('conv1',tf.reshape(self.conv1,[-1, conv1_shape[1], conv1_shape[2], 1]),1)\n",
    "        summary_conv2 = tf.summary.image('conv2',tf.reshape(self.conv2,[-1, conv2_shape[1], conv2_shape[2], 1]),1)\n",
    "\n",
    "        self.merged_image = tf.summary.merge([summary_state,summary_conv1,summary_conv2])\n",
    "        \n",
    "        summary_reward = tf.summary.scalar('reward',self.reward_)\n",
    "        summary_frag = tf.summary.scalar('frag',self.frag_)\n",
    "        summary_death = tf.summary.scalar('death',self.death_)\n",
    "        \n",
    "        self.merged_testscore = tf.summary.merge([summary_reward,summary_frag,summary_death])\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(LOG_DIR,SESS.graph)\n",
    "\n",
    "\n",
    "    def learn(self, s1, target, target_n, loss_class,is_weight):\n",
    "\n",
    "        global frames\n",
    "        \n",
    "        l, _ = SESS.run([self.loss, self.train_step],\\\n",
    "         feed_dict={self.s1_:s1, self.target_q_one_:target, self.target_q_n_:target_n, self.loss_class_:loss_class,\\\n",
    "         self.is_weight_:is_weight})\n",
    "        \n",
    "        self.write_loss_img(frames,s1,target,target_n,loss_class)\n",
    "\n",
    "    def get_q_values(self, state):\n",
    "        return SESS.run(self.model, feed_dict={self.s1_:state})\n",
    "\n",
    "    def get_q_target_values(self,state):\n",
    "        best_actions = self.get_best_actions(state)\n",
    "        q =  SESS.run(self.model_t, feed_dict={self.s1_:state})\n",
    "        ret = []\n",
    "        for i in range(len(q)):\n",
    "            ret.append(q[i][best_actions[i]])\n",
    "        ret = np.array(ret)\n",
    "        return ret\n",
    "\n",
    "    def get_best_actions(self,state):\n",
    "        s1 = state.reshape([-1,RESOLUTION[0],RESOLUTION[1],RESOLUTION[2]])\n",
    "        return SESS.run(self.best_action, feed_dict={self.s1_:s1})\n",
    "\n",
    "    def get_best_action(self,state):\n",
    "        return SESS.run(self.best_action, feed_dict={self.s1_:state})[0]\n",
    "\n",
    "    def write_loss_img(self,step,s1,target,target_n,loss_class):\n",
    "        if step%100==0:\n",
    "            feeddict = {self.s1_:s1,self.target_q_one_:target,self.target_q_n_:target_n,self.loss_class_:loss_class}\n",
    "            m_l,m_i = SESS.run([self.merged_scalar,self.merged_image],feeddict)\n",
    "            self.writer.add_summary(m_l,step)\n",
    "            self.writer.add_summary(m_i,step)\n",
    "            \n",
    "    def write_score(self,step,reward,frag,death):\n",
    "        feeddict={self.reward_:reward,self.frag_:frag,self.death_:death}\n",
    "        m_r = SESS.run(self.merged_testscore,feeddict)\n",
    "        self.writer.add_summary(m_r,step)\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        self.saver.save(SESS, model_path)\n",
    "\n",
    "    def restore_model(self,model_path):\n",
    "        self.saver.restore(SESS,model_path)\n",
    "\n",
    "    def copy_params(self):\n",
    "\n",
    "        self.copyop = [tf.assign(target, origin) for origin,target in zip(self.weights_params_learning,self.weights_params_target) ]\n",
    "        SESS.run(self.copyop)\n",
    "\n",
    "    def get_l2_loss(self):\n",
    "        return [tf.nn.l2_loss(w) for w in self.weights_params_learning]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(0,), dtype=int32, device=/device:GPU:0)\n",
      "Tensor(\"Shape_1:0\", shape=(0,), dtype=int32, device=/device:GPU:0)\n",
      "Tensor(\"Shape_2:0\", shape=(0,), dtype=int32, device=/device:GPU:0)\n",
      "Tensor(\"Shape_3:0\", shape=(0,), dtype=int32, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE of DEMO: (200, 10, 1)\n",
      "---PRE TRAING PHASE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.21it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TRAINING PHASE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 45/200 [00:22<01:17,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t TEST at  50\n",
      "\t SPEND STEPS: 145\n",
      "\tFRAG: 0.0 DEATH: 4.0 REWARD: -394.12999999999977\n",
      "\t {'dist': 31.31999999999999, 'medkit': 0.0, 'health_loss': -424.0, 'ammo': 0.0, 'suicide': 0.0, 'frag': 0.0, 'living': -1.450000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 95/200 [00:48<00:53,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t TEST at  100\n",
      "\t SPEND STEPS: 211\n",
      "\tFRAG: -1.0 DEATH: 3.0 REWARD: -718.3700000000006\n",
      "\t {'dist': 79.74000000000002, 'medkit': 0.0, 'health_loss': -296.0, 'ammo': 0.0, 'suicide': -500.0, 'frag': 0.0, 'living': -2.109999999999999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 145/200 [01:15<00:28,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t TEST at  150\n",
      "\t SPEND STEPS: 280\n",
      "\tFRAG: 0.0 DEATH: 2.0 REWARD: -222.99999999999952\n",
      "\t {'dist': 28.799999999999997, 'medkit': 100.0, 'health_loss': -349.0, 'ammo': 0.0, 'suicide': 0.0, 'frag': 0.0, 'living': -2.7999999999999843}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 195/200 [01:43<00:02,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t TEST at  200\n",
      "\t SPEND STEPS: 281\n",
      "\tFRAG: 0.0 DEATH: 2.0 REWARD: -94.83000000000047\n",
      "\t {'dist': 37.97999999999998, 'medkit': 0.0, 'health_loss': -130.0, 'ammo': 0.0, 'suicide': 0.0, 'frag': 0.0, 'living': -2.809999999999984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:45<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_1  finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frames = 0\n",
    "SESS = tf.Session()\n",
    "\n",
    "env = Environment(\"learning_1\")\n",
    "env.load_demonstration()\n",
    "\n",
    "SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"---PRE TRAING PHASE---\")\n",
    "env.pre_learning()\n",
    "    \n",
    "print(\"---TRAINING PHASE---\")\n",
    "env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
